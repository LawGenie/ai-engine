"""
ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëˆ„ë½ëœ ëª¨ë“ˆ, í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬, ì—ëŸ¬ ì²˜ë¦¬, ìºì‹±, ë³‘ë ¬ ì²˜ë¦¬ ë“±ì„ í…ŒìŠ¤íŠ¸
"""

import asyncio
import os
import sys
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

async def test_improvements():
    """ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ìš”ê±´ íŒŒíŠ¸ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # 1. í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
        from app.services.requirements.env_manager import env_manager
        
        api_status = env_manager.get_api_status_summary()
        print(f"âœ… í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {api_status['available_api_keys']}/{api_status['total_api_keys']}")
        
        # 2. ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        from app.services.requirements.error_handler import error_handler, WorkflowError, ErrorSeverity
        
        # í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ìƒì„±
        test_error = WorkflowError("í…ŒìŠ¤íŠ¸ ì—ëŸ¬", ErrorSeverity.MEDIUM)
        error_result = error_handler.handle_error(test_error, {'test': True})
        print(f"âœ… ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {error_result['status']}")
        
        # 3. Data.gov API ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ Data.gov API ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸")
        from app.services.requirements.data_gov_api import DataGovAPIService
        
        data_gov_service = DataGovAPIService()
        print(f"âœ… Data.gov API ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 4. í–¥ìƒëœ ìºì‹± ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ í–¥ìƒëœ ìºì‹± ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸")
        from app.services.requirements.enhanced_cache_service import enhanced_cache
        
        # ìºì‹œ í…ŒìŠ¤íŠ¸
        test_key = "test_key"
        test_value = {"test": "data", "timestamp": datetime.now().isoformat()}
        
        await enhanced_cache.set(test_key, test_value, ttl=60)
        cached_value = await enhanced_cache.get(test_key)
        
        if cached_value == test_value:
            print(f"âœ… ìºì‹± ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        else:
            print(f"âŒ ìºì‹± ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        # 5. ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\n5ï¸âƒ£ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        from app.services.requirements.parallel_processor import parallel_processor, ProcessingTask, ProcessingMode
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‘ì—…ë“¤
        async def test_task(task_id: str, delay: float = 0.1):
            await asyncio.sleep(delay)
            return f"Task {task_id} completed"
        
        tasks = [
            ProcessingTask(id=f"task_{i}", func=test_task, args=(f"task_{i}", 0.1))
            for i in range(5)
        ]
        
        results = await parallel_processor.process_parallel(tasks, ProcessingMode.PARALLEL)
        successful_tasks = len([r for r in results if r.success])
        
        print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {successful_tasks}/{len(tasks)}ê°œ ì‘ì—… ì„±ê³µ")
        
        # 6. í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        print("\n6ï¸âƒ£ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
        from workflows.unified_workflow import unified_workflow
        
        workflow_status = unified_workflow.get_workflow_status()
        print(f"âœ… í†µí•© ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ì›Œí¬í”Œë¡œìš° íƒ€ì…: {workflow_status['workflow_type']}")
        print(f"   ë…¸ë“œ ìˆ˜: {workflow_status['nodes_count']}")
        
        # 7. ìš”êµ¬ì‚¬í•­ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ë²„ì „)
        print("\n7ï¸âƒ£ ìš”êµ¬ì‚¬í•­ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        try:
            result = await unified_workflow.analyze_requirements(
                hs_code="3304",
                product_name="Test Product",
                product_description="Test Description"
            )
            
            if result.get('status') == 'completed':
                print(f"âœ… ìš”êµ¬ì‚¬í•­ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                print(f"   ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time_ms', 0)}ms")
            else:
                print(f"âš ï¸ ìš”êµ¬ì‚¬í•­ ë¶„ì„ í…ŒìŠ¤íŠ¸ ë¶€ë¶„ ì„±ê³µ: {result.get('status', 'unknown')}")
                
        except Exception as e:
            print(f"âš ï¸ ìš”êµ¬ì‚¬í•­ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # 8. ë©”íŠ¸ë¦­ ìš”ì•½
        print("\n8ï¸âƒ£ ë©”íŠ¸ë¦­ ìš”ì•½")
        cache_metrics = enhanced_cache.get_metrics()
        parallel_metrics = parallel_processor.get_metrics()
        error_summary = error_handler.get_error_summary()
        
        print(f"ğŸ“Š ìºì‹œ ë©”íŠ¸ë¦­:")
        print(f"   íˆíŠ¸ìœ¨: {cache_metrics.get('hit_rate', 0)}%")
        print(f"   ì´ ìš”ì²­: {cache_metrics.get('total_requests', 0)}")
        
        print(f"ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ ë©”íŠ¸ë¦­:")
        print(f"   ì„±ê³µë¥ : {parallel_metrics.get('success_rate', 0)}%")
        print(f"   í‰ê·  ì²˜ë¦¬ì‹œê°„: {parallel_metrics.get('average_processing_time', 0)}ì´ˆ")
        
        print(f"ğŸ“Š ì—ëŸ¬ ìš”ì•½:")
        print(f"   ì´ ì—ëŸ¬: {error_summary.get('total_errors', 0)}")
        
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ LawGenie ìš”ê±´ íŒŒíŠ¸ ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().isoformat()}")
    print("=" * 60)
    
    await test_improvements()
    
    print("\n" + "=" * 60)
    print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().isoformat()}")
    print("í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())
