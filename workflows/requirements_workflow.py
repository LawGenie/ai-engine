"""
ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì›Œí¬í”Œë¡œìš°
HSì½”ë“œ ê¸°ë°˜ ë¯¸êµ­ ìˆ˜ì…ìš”ê±´ ë¶„ì„ì„ ìœ„í•œ í†µí•© ì›Œí¬í”Œë¡œìš°
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any, List, Optional
from app.services.requirements.requirements_cache_service import RequirementsCacheService

@dataclass
class RequirementsState:
    hs_code: str
    product_name: str
    product_description: str
    agency_mapping: Optional[Any] = None
    recommended_agencies: List[str] = None
    search_results: Dict[str, Any] = None
    raw_documents: List[Dict[str, Any]] = None
    llm_summary: Optional[Any] = None
    final_result: Dict[str, Any] = None
    processing_time_ms: int = 0

class RequirementsWorkflow:
    def __init__(self):
        # í†µí•© ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
        from workflows.unified_workflow import unified_workflow
        self.unified_workflow = unified_workflow
        
        # ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
        from app.services.requirements.hs_code_agency_mapping_service import HsCodeAgencyMappingService
        from app.services.requirements.search_service import SearchService
        from app.services.requirements.llm_summary_service import LlmSummaryService
        
        self.agency_mapping_service = HsCodeAgencyMappingService()
        self.search_service = SearchService()
        self.llm_summary_service = LlmSummaryService()
        self.cache_service = RequirementsCacheService()
    
    async def analyze_requirements(
        self, 
        hs_code: str, 
        product_name: str, 
        product_description: str = "",
        force_refresh: bool = False,
        is_new_product: bool = False
    ) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹¤í–‰ (í†µí•© ì›Œí¬í”Œë¡œìš° ì‚¬ìš©)"""
        
        print(f"ğŸš€ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹œì‘ - HSì½”ë“œ: {hs_code}, ê°•ì œê°±ì‹ : {force_refresh}, ì‹ ê·œìƒí’ˆ: {is_new_product}")
        
        # ìºì‹œ í™•ì¸ (ê°•ì œ ê°±ì‹ ì´ ì•„ë‹ˆê³  ì‹ ê·œ ìƒí’ˆì´ ì•„ë‹Œ ê²½ìš°)
        if not force_refresh and not is_new_product:
            cached_result = await self.cache_service.get_cached_analysis(hs_code, product_name)
            if cached_result:
                print(f"âœ… ìºì‹œì—ì„œ ë°˜í™˜")
                # ìºì‹œëœ ë°ì´í„°ë¥¼ RequirementsResponse í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted_result = self._format_cached_response(
                    hs_code, product_name, cached_result, force_refresh, is_new_product
                )
                return formatted_result
        
        # í†µí•© ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
        try:
            result = await self.unified_workflow.analyze_requirements(
                hs_code=hs_code,
                product_name=product_name,
                product_description=product_description,
                force_refresh=force_refresh,
                is_new_product=is_new_product
            )
            
            # ìºì‹œì— ì €ì¥ (ì‹ ê·œ ìƒí’ˆì´ê±°ë‚˜ ê°•ì œ ê°±ì‹ ì¸ ê²½ìš°)
            if is_new_product or force_refresh:
                await self.cache_service.save_analysis_to_cache(hs_code, product_name, result)
                print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œì— ì €ì¥")
            
            print(f"âœ… ë¶„ì„ ì™„ë£Œ")
            return result
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "status": "failed"}
    
    def _build_queries(self, hs_code: str, product_name: str, agencies: List[str]) -> Dict[str, List[str]]:
        queries = {}
        for agency in agencies:
            agency_queries = [
                f"{agency} import requirements {product_name}",
                f"{agency} regulations HS {hs_code}"
            ]
            queries[agency] = agency_queries[:3]
        return queries
    
    def _extract_documents(self, search_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        documents = []
        for agency, search_result in search_results.items():
            for result in search_result.results:
                documents.append({
                    "title": result.get("title", "Unknown"),
                    "content": result.get("content", ""),
                    "url": result.get("url", ""),
                    "agency": agency
                })
        return documents
    
    def _integrate_results(self, state: RequirementsState) -> Dict[str, Any]:
        try:
            result = {
                "hs_code": state.hs_code or "UNKNOWN",
                "product_name": state.product_name or "Unknown Product",
                "processing_time_ms": state.processing_time_ms or 0,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
                "cache_hit": False,
                "force_refresh": False,
                "is_new_product": False
            }
        except Exception as e:
            print(f"âŒ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ğŸ‘‡ ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ í•„ìˆ˜ í•„ë“œë“¤ì„ í¬í•¨í•œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
            result = {
                "hs_code": "UNKNOWN",
                "product_name": "Unknown Product", 
                "processing_time_ms": 0,
                "timestamp": datetime.now().isoformat(),
                "status": "failed",
                "cache_hit": False,
                "force_refresh": False,
                "is_new_product": False,
                "error": str(e)
            }
        
        try:
            if state.recommended_agencies:
                result["recommended_agencies"] = state.recommended_agencies
            
            if state.search_results:
                result["search_results"] = {
                    agency: {
                        "results_count": len(search_result.results),
                        "source": search_result.source,
                        "cost": search_result.cost
                    }
                    for agency, search_result in state.search_results.items()
                }
            
            if state.llm_summary:
                result["llm_summary"] = {
                    "critical_requirements": state.llm_summary.critical_requirements,
                    "required_documents": state.llm_summary.required_documents,
                    "compliance_steps": state.llm_summary.compliance_steps,
                    "estimated_costs": state.llm_summary.estimated_costs,
                    "timeline": state.llm_summary.timeline,
                    "confidence_score": state.llm_summary.confidence_score
                }
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ í•„ìˆ˜ í•„ë“œë“¤ì€ ìœ ì§€
        
        # ğŸ¯ ëª¨ë“  ë‹¨ê³„ì˜ ìƒì„¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìµœì¢… ê²°ê³¼ì— í¬í•¨
        result["comprehensive_metadata"] = {
            "analysis_workflow_steps": {
                "total_steps_completed": 4,  # ê¸°ê´€ë§¤í•‘, ê²€ìƒ‰, ìš”ì•½, í†µí•©
                "processing_stages": [
                    "keyword_extraction",
                    "search_agency_documents", 
                    "hybrid_api_call",
                    "document_scraping",
                    "result_consolidation",
                    "llm_summarization"
                ],
                "workflow_performance": {
                    "total_processing_time_ms": state.processing_time_ms,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "cache_hit": result.get("cache_hit", False),
                    "force_refresh": result.get("force_refresh", False),
                    "is_new_product": result.get("is_new_product", False)
                }
            },
            "data_collection_summary": {
                "agencies_analyzed": len(state.recommended_agencies) if state.recommended_agencies else 0,
                "search_results_count": len(state.search_results) if state.search_results else 0,
                "total_urls_found": sum(len(sr.results) for sr in state.search_results.values()) if state.search_results else 0,
                    "raw_documents_count": 0,  # raw_documentsëŠ” í†µí•© ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ë¨
                "llm_summary_quality": "successful" if state.llm_summary else "failed",
                "metadata_completeness_score": self._calculate_metadata_completeness(result, state)
            },
            "technical_details": {
                "search_provider": state.search_results[list(state.search_results.keys())[0]].source if state.search_results else "unknown",
                "llm_model_used": "default",  # ì‹¤ì œ ì‚¬ìš©ëœ ëª¨ë¸ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
                "data_sources": ["tavily_search", "government_apis", "web_scraping", "precedents_db"],
                "api_endpoints_called": list(self.search_service.free_api_endpoints.keys()) if hasattr(self.search_service, 'free_api_endpoints') else []
            }
        }
        
        return result
    
    def _calculate_metadata_completeness(self, result: Dict[str, Any], state) -> float:
        """ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚° (0.0-1.0)"""
        try:
            score = 0.0
            total_checks = 8
            
            # ê¸°ë³¸ ì •ë³´ í™•ì¸
            if result.get("hs_code"): score += 0.125
            if result.get("product_name"): score += 0.125
            if result.get("recommended_agencies"): score += 0.125
            if result.get("search_results"): score += 0.125
            if result.get("llm_summary"): score += 0.125
            if result.get("processing_time_ms", 0) > 0: score += 0.125
            if result.get("timestamp"): score += 0.125
            
            # í’ë¶€í•œ ë°ì´í„° í™•ì¸
            if result.get("search_results"):
                total_urls = sum(len(sr.results) for sr in result["search_results"].values() if hasattr(sr, 'results'))
                if total_urls > 0: score += 0.125  # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
            
            return min(score, 1.0)
        except Exception:
            return 0.0
    
    def _format_cached_response(self, hs_code: str, product_name: str, cached_result: Dict[str, Any], force_refresh: bool, is_new_product: bool) -> Dict[str, Any]:
        """ìºì‹œëœ ë°ì´í„°ë¥¼ RequirementsResponse í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ê¸°ë³¸ í•„ë“œ ì„¤ì •
            response = {
                "hs_code": hs_code,
                "product_name": product_name,
                "status": "completed",
                "timestamp": datetime.now().isoformat(),
                "processing_time_ms": 0,  # ìºì‹œì—ì„œëŠ” ì²˜ë¦¬ ì‹œê°„ ì—†ìŒ
                "cache_hit": True,
                "force_refresh": force_refresh,
                "is_new_product": is_new_product
            }
            
            # ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if isinstance(cached_result, dict):
                for key, value in cached_result.items():
                    if key not in ["hs_code", "product_name", "status", "timestamp", "processing_time_ms"]:
                        response[key] = value
            
            return response
            
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì‘ë‹µ í¬ë§· ë³€í™˜ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            return {
                "hs_code": hs_code,
                "product_name": product_name,
                "status": "completed",
                "timestamp": datetime.now().isoformat(),
                "processing_time_ms": 0,
                "cache_hit": True,
                "force_refresh": force_refresh,
                "is_new_product": is_new_product
            }