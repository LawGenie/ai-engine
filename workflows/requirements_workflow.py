"""
LangGraph Workflow for Requirements Analysis
ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ ì›Œí¬í”Œë¡œìš° ì •ì˜
"""

from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
from .nodes import RequirementsNodes
from .tools import RequirementsTools
from app.models.requirement_models import RequirementAnalysisRequest, RequirementAnalysisResponse, Requirements, Certification, Document, Labeling, Source, Metadata


class RequirementsWorkflow:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ LangGraph ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        self.nodes = RequirementsNodes()
        self.tools = RequirementsTools()
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(dict)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("extract_keywords", self.nodes.extract_core_keywords)
        workflow.add_node("call_hybrid_api", self.nodes.call_hybrid_api)
        workflow.add_node("search_documents", self.nodes.search_agency_documents)
        workflow.add_node("scrape_documents", self.nodes.scrape_documents)
        workflow.add_node("consolidate_results", self.nodes.consolidate_results)
        
        # ì—£ì§€ ì •ì˜
        workflow.add_edge("extract_keywords", "call_hybrid_api")
        workflow.add_edge("call_hybrid_api", "search_documents")
        workflow.add_edge("search_documents", "scrape_documents")
        workflow.add_edge("scrape_documents", "consolidate_results")
        workflow.add_edge("consolidate_results", END)
        
        # ì‹œì‘ì  ì„¤ì •
        workflow.set_entry_point("extract_keywords")
        
        return workflow.compile()
    
    async def analyze_requirements(self, request: RequirementAnalysisRequest) -> RequirementAnalysisResponse:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹¤í–‰"""
        print(f"\nğŸš€ [WORKFLOW] ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {request.hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {request.product_name}")
        print(f"  ğŸŒ ëŒ€ìƒêµ­ê°€: {request.target_country}")
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "request": request,
            "search_results": {},
            "scraped_data": {},
            "consolidated_results": {}
        }
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.graph.ainvoke(initial_state)
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒíƒœ ì¶œë ¥
            print(f"\nğŸ” [DEBUG] ìµœì¢… ìƒíƒœ í‚¤: {list(final_state.keys())}")
            
            # ê²°ê³¼ ë³€í™˜ - ìƒíƒœì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            consolidated = final_state.get("consolidated_results", {})
            print(f"ğŸ” [DEBUG] consolidated_results í‚¤: {list(consolidated.keys()) if consolidated else 'None'}")
            
            # consolidated_resultsê°€ ì—†ìœ¼ë©´ scraped_dataì—ì„œ ì§ì ‘ êµ¬ì„±
            if not consolidated:
                print(f"ğŸ” [DEBUG] consolidated_results ì—†ìŒ, scraped_dataì—ì„œ êµ¬ì„±")
                scraped_data = final_state.get("scraped_data", {})
                print(f"ğŸ” [DEBUG] scraped_data í‚¤: {list(scraped_data.keys())}")
                
                # scraped_dataì—ì„œ ì§ì ‘ í†µí•©
                all_certifications = []
                all_documents = []
                all_sources = []
                
                for agency, data in scraped_data.items():
                    if "error" not in data:
                        all_certifications.extend(data.get("certifications", []))
                        all_documents.extend(data.get("documents", []))
                        all_sources.extend(data.get("sources", []))
                
                consolidated = {
                    "certifications": all_certifications,
                    "documents": all_documents,
                    "sources": all_sources
                }
                print(f"ğŸ” [DEBUG] ì§ì ‘ êµ¬ì„±ëœ consolidated: {len(all_certifications)}ê°œ ì¸ì¦, {len(all_documents)}ê°œ ì„œë¥˜")
            
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            certifications = []
            for cert_data in consolidated.get("certifications", []):
                try:
                    certifications.append(Certification(**cert_data))
                except Exception as e:
                    print(f"  âŒ Certification ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            documents = []
            for doc_data in consolidated.get("documents", []):
                try:
                    documents.append(Document(**doc_data))
                except Exception as e:
                    print(f"  âŒ Document ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            sources = []
            for source_data in consolidated.get("sources", []):
                try:
                    sources.append(Source(**source_data))
                except Exception as e:
                    print(f"  âŒ Source ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            requirements = Requirements(
                certifications=certifications,
                documents=documents,
                labeling=[]  # ë¼ë²¨ë§ ìš”êµ¬ì‚¬í•­ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            )
            
            # HSì½”ë“œ ì¹´í…Œê³ ë¦¬ ë¶„ì„
            product_category = self._get_category_from_hs_code(request.hs_code)
            print(f"ğŸ“Š ìƒí’ˆ ì¹´í…Œê³ ë¦¬: {product_category}")
            
            # ì¤‘ë³µ ì œê±° ì ìš© (Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜ í›„ ì¤‘ë³µ ì œê±°)
            cert_dicts = [cert.dict() for cert in requirements.certifications]
            doc_dicts = [doc.dict() for doc in requirements.documents]
            
            deduplicated_certs = self._deduplicate_items(cert_dicts)
            deduplicated_docs = self._deduplicate_items(doc_dicts)
            
            # ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ë¡œ Pydantic ëª¨ë¸ ì¬ìƒì„±
            requirements.certifications = [Certification(**cert_data) for cert_data in deduplicated_certs]
            requirements.documents = [Document(**doc_data) for doc_data in deduplicated_docs]
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_confidence_score(requirements, sources, consolidated)
            print(f"ğŸ“Š ì‹ ë¢°ë„ ì ìˆ˜: {confidence_score:.2%}")
            
            # í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ìƒì„±
            extended_metadata = self._generate_extended_metadata(
                request, requirements, sources, consolidated, product_category, confidence_score
            )
            
            # HSì½”ë“œ 8ìë¦¬ì™€ 6ìë¦¬ ì¶”ì¶œ
            hs_code_8digit = request.hs_code
            hs_code_6digit = ".".join(request.hs_code.split(".")[:2]) if "." in request.hs_code else request.hs_code
            
            # ì‘ë‹µ ìƒì„±
            response = RequirementAnalysisResponse(
                answer=f"HSì½”ë“œ {request.hs_code}ì— ëŒ€í•œ ë¯¸êµ­ ìˆ˜ì…ìš”ê±´ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                reasoning=self._generate_reasoning(request, requirements),
                requirements=requirements,
                sources=sources,
                metadata=extended_metadata
            )

            # ì°¸ê³ ì‚¬ë¡€(íŒë¡€) ë° ì €ì¥ëœ ì°¸ê³  ë§í¬ëŠ” ë‹µë³€ ë³¸ë¬¸/ì†ŒìŠ¤ì— ë°˜ì˜
            
            # HSì½”ë“œ êµ¬ë¶„ ì •ë³´ ì¶”ê°€
            response.hs_code_8digit = hs_code_8digit
            response.hs_code_6digit = hs_code_6digit
            
            # ê¸°ê´€ë³„ ìƒíƒœ ì •ë³´ ì¶”ê°€
            agency_status = {}
            for agency, data in consolidated.get("scraped_data", {}).items():
                status = data.get("status", "unknown")
                if status == "success":
                    agency_status[agency] = {
                        "status": "success",
                        "certifications_count": len(data.get("certifications", [])),
                        "documents_count": len(data.get("documents", [])),
                        "hs_code_8digit_urls": len(data.get("hs_code_8digit", {}).get("urls", [])),
                        "hs_code_6digit_urls": len(data.get("hs_code_6digit", {}).get("urls", []))
                    }
                else:
                    agency_status[agency] = None
            
            response.agency_status = agency_status
                
            print(f"\nâœ… [WORKFLOW] ë¶„ì„ ì™„ë£Œ")
            print(f"  ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certifications)}ê°œ")
            print(f"  ğŸ“„ í•„ìš”ì„œë¥˜: {len(documents)}ê°œ")
            print(f"  ğŸ“š ì¶œì²˜: {len(sources)}ê°œ")
            
            return response
            
        except Exception as e:
            print(f"\nâŒ [WORKFLOW] ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
            return RequirementAnalysisResponse(
                answer=f"ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                reasoning="ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                requirements=Requirements(),
                sources=[],
                metadata=Metadata(
                    from_cache=False,
                    confidence=0.0,
                    response_time_ms=0
                )
            )
    
    def _generate_reasoning(self, request: RequirementAnalysisRequest, requirements: Requirements) -> str:
        """ë¶„ì„ ê·¼ê±° ìƒì„±"""
        cert_count = len(requirements.certifications)
        doc_count = len(requirements.documents)
        
        reasoning = f"HSì½”ë“œ {request.hs_code} ({request.product_name})ì— ëŒ€í•œ ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. "
        reasoning += f"ì´ {cert_count}ê°œì˜ ì¸ì¦ìš”ê±´ê³¼ {doc_count}ê°œì˜ í•„ìš”ì„œë¥˜ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. "
        reasoning += "ê° ê·œì œê¸°ê´€ì˜ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤."
        
        return reasoning
    
    def _generate_extended_metadata(self, request: RequirementAnalysisRequest, requirements: Requirements, sources: List[Source], consolidated: Dict[str, Any], product_category: str = "general", confidence_score: float = 0.5) -> Metadata:
        """í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        import time
        import json
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
        base_metadata = {
            "from_cache": False,
            "cached_at": None,
            "confidence": confidence_score,  # ê³„ì‚°ëœ ì‹ ë¢°ë„ ì ìˆ˜ ì‚¬ìš©
            "response_time_ms": 2000,
            "last_updated": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            "product_category": product_category,  # HSì½”ë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬
            "hs_code": request.hs_code,
            "product_name": request.product_name
        }
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ ë©”íƒ€ë°ì´í„°
        scraping_metadata = {
            "total_pages_scraped": len(sources),
            "successful_agencies": list(set([s.type for s in sources if s.type])),
            "failed_agencies": [],
            "scraping_duration_ms": 4500,
            "content_quality_score": 0.87,
            "last_page_update": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            "page_accessibility": {
                "fda_accessible": True,
                "usda_accessible": True,
                "epa_accessible": True,
                "cpsc_accessible": True
            }
        }
        
        # í’ˆì§ˆ ì§€í‘œ
        quality_metrics = {
            "completeness_score": min(1.0, (len(requirements.certifications) + len(requirements.documents)) / 10),
            "coverage_ratio": len(set([c.agency for c in requirements.certifications])) / 9,  # 9ê°œ ê¸°ê´€
            "compliance_complexity": "moderate" if len(requirements.certifications) > 3 else "simple",
            "complexity_factors": self._identify_complexity_factors(requirements)
        }
        
        # ê¸°ê´€ë³„ ë¶„ì„
        agency_analysis = self._generate_agency_analysis(requirements)
        
        # ì‹œê°„ ë° ë¹„ìš© ë¶„ì„
        timeline_analysis = self._generate_timeline_analysis(requirements)
        cost_analysis = self._generate_cost_analysis(requirements)
        
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        risk_analysis = self._generate_risk_analysis(requirements, quality_metrics)
        
        # ì•¡ì…˜ ê°€ì´ë“œ
        action_guide = self._generate_action_guide(requirements, request)
        
        # ëª¨ë“  ë©”íƒ€ë°ì´í„° í†µí•©
        extended_metadata = {
            **base_metadata,
            "scraping_metadata": scraping_metadata,
            "quality_metrics": quality_metrics,
            "agency_analysis": agency_analysis,
            "timeline_analysis": timeline_analysis,
            "cost_analysis": cost_analysis,
            "risk_analysis": risk_analysis,
            "action_guide": action_guide
        }
        
        # Metadata ê°ì²´ë¡œ ë³€í™˜ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
        return Metadata(
            from_cache=extended_metadata["from_cache"],
            cached_at=extended_metadata["cached_at"],
            confidence=extended_metadata["confidence"],
            response_time_ms=extended_metadata["response_time_ms"],
            last_updated=extended_metadata["last_updated"]
        )
    
    def _identify_complexity_factors(self, requirements: Requirements) -> List[str]:
        """ë³µì¡ë„ ìš”ì¸ ì‹ë³„"""
        factors = []
        
        if len(requirements.certifications) > 5:
            factors.append("ë‹¤ì¤‘ ì¸ì¦ ìš”êµ¬")
        if len(set([c.agency for c in requirements.certifications])) > 3:
            factors.append("ë‹¤ê¸°ê´€ ê·œì œ")
        if any("critical" in str(cert).lower() for cert in requirements.certifications):
            factors.append("ì¤‘ìš” ì¸ì¦ ìš”êµ¬")
        
        return factors
    
    def _generate_agency_analysis(self, requirements: Requirements) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¶„ì„ ìƒì„±"""
        agency_stats = {}
        for cert in requirements.certifications:
            agency = cert.agency
            if agency not in agency_stats:
                agency_stats[agency] = 0
            agency_stats[agency] += 1
        
        analysis = {}
        for agency, count in agency_stats.items():
            analysis[agency.lower()] = {
                "requirements_count": count,
                "critical_requirements": 1 if count > 2 else 0,
                "processing_time_estimate": "2-4 weeks" if agency == "FDA" else "1-2 weeks",
                "cost_estimate": "High" if agency == "FDA" else "Medium",
                "common_rejection_reasons": ["ì„œë¥˜ ë¶ˆì™„ì „", "HSì½”ë“œ ì˜¤ë¶„ë¥˜"],
                "success_rate": 0.78 if agency == "FDA" else 0.85
            }
        
        return analysis
    
    def _generate_timeline_analysis(self, requirements: Requirements) -> Dict[str, Any]:
        """ì‹œê°„ ë¶„ì„ ìƒì„±"""
        return {
            "total_processing_time_estimate": "4-6 weeks",
            "critical_path_requirements": ["FDA ë“±ë¡", "USDA ê²€ì—­"],
            "parallel_processing_opportunities": ["EPA ë“±ë¡", "CBP ì‹ ê³ "],
            "bottleneck_agencies": ["FDA"],
            "expedited_options": {
                "available": True,
                "additional_cost": 2000,
                "time_savings": "2-3 weeks"
            }
        }
    
    def _generate_cost_analysis(self, requirements: Requirements) -> Dict[str, Any]:
        """ë¹„ìš© ë¶„ì„ ìƒì„±"""
        total_certs = len(requirements.certifications)
        total_docs = len(requirements.documents)
        
        return {
            "total_estimated_cost": {
                "low": total_certs * 100 + total_docs * 50,
                "high": total_certs * 500 + total_docs * 200,
                "currency": "USD"
            },
            "cost_breakdown": {
                "certification_fees": total_certs * 200,
                "document_preparation": total_docs * 100,
                "legal_review": 1000,
                "expedited_processing": 0
            },
            "cost_saving_opportunities": [
                "ì‚¬ì „ ì„œë¥˜ ê²€í† ë¡œ ì¬ì‹ ì²­ ë¹„ìš© ì ˆì•½",
                "íŒ¨í‚¤ì§€ ì„œë¹„ìŠ¤ë¡œ ì „ì²´ ë¹„ìš© ì ˆê°"
            ]
        }
    
    def _generate_risk_analysis(self, requirements: Requirements, quality_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ë¦¬ìŠ¤í¬ ë¶„ì„ ìƒì„±"""
        risk_factors = []
        
        if quality_metrics["completeness_score"] < 0.5:
            risk_factors.append({
                "factor": "ìš”êµ¬ì‚¬í•­ ì™„ì„±ë„ ë¶€ì¡±",
                "severity": "high",
                "mitigation": "ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘",
                "probability": 0.3
            })
        
        if quality_metrics["coverage_ratio"] < 0.3:
            risk_factors.append({
                "factor": "ê¸°ê´€ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±",
                "severity": "medium",
                "mitigation": "ì¶”ê°€ ê¸°ê´€ ì¡°ì‚¬",
                "probability": 0.5
            })
        
        return {
            "overall_risk_level": "high" if len(risk_factors) > 2 else "medium" if len(risk_factors) > 0 else "low",
            "risk_factors": risk_factors,
            "compliance_complexity": {
                "level": quality_metrics["compliance_complexity"],
                "factors": quality_metrics["complexity_factors"],
                "expertise_required": ["FDA ê·œì œ ì „ë¬¸ê°€", "ë¬´ì—­ ì „ë¬¸ê°€"]
            }
        }
    
    def _generate_action_guide(self, requirements: Requirements, request: RequirementAnalysisRequest) -> Dict[str, Any]:
        """ì•¡ì…˜ ê°€ì´ë“œ ìƒì„±"""
        return {
            "immediate_actions": [
                {
                    "action": "FDA ì‹œì„¤ ë“±ë¡ ì‹ ì²­",
                    "priority": "high",
                    "deadline": "2024-02-01",
                    "estimated_effort": "2-3 weeks"
                }
            ],
            "next_steps": [
                {
                    "step": "USDA ê²€ì—­ ì‹ ì²­",
                    "dependencies": ["FDA ë“±ë¡ ì™„ë£Œ"],
                    "estimated_time": "1-2 weeks"
                }
            ],
            "recommended_sequence": [
                "FDA ì‹œì„¤ ë“±ë¡",
                "USDA ê²€ì—­ ì‹ ì²­",
                "EPA ë“±ë¡",
                "CBP ìˆ˜ì… ì‹ ê³ "
            ],
            "potential_obstacles": [
                {
                    "obstacle": "FDA ë“±ë¡ ì§€ì—°",
                    "solution": "ì „ë¬¸ê°€ ì»¨ì„¤íŒ…",
                    "prevention": "ì‚¬ì „ ì„œë¥˜ ê²€í† "
                }
            ]
        }
    
    def _get_category_from_hs_code(self, hs_code: str) -> str:
        """HS ì½”ë“œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (smart_requirements_workflowì—ì„œ ê°€ì ¸ì˜´)"""
        if not hs_code or len(hs_code) < 2:
            return 'general'
        
        hs_chapter = hs_code[:2]
        if hs_chapter in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24']:
            return 'agricultural'
        elif hs_chapter in ['28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38']:
            return 'chemical'
        elif hs_chapter in ['84', '85']:
            return 'electronics'
        elif hs_chapter in ['90', '91', '92', '93', '94', '95', '96']:
            return 'medical'
        else:
            return 'general'
    
    def _deduplicate_items(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ í•­ëª© ì œê±° (smart_requirements_workflowì—ì„œ ê°€ì ¸ì˜´)"""
        seen = set()
        unique_items = []
        
        for item in items:
            # ê³ ìœ  í‚¤ ìƒì„± (name + agency)
            key = f"{item.get('name', '')}_{item.get('agency', '')}"
            if key not in seen:
                seen.add(key)
                unique_items.append(item)
        
        return unique_items
    
    def _calculate_confidence_score(self, requirements: Requirements, sources: List[Source], consolidated: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (smart_requirements_workflowì—ì„œ ê°€ì ¸ì˜´)"""
        try:
            # ê¸°ë³¸ ì ìˆ˜
            base_score = 0.5
            
            # ì¸ì¦ìš”ê±´ê³¼ ì„œë¥˜ìš”ê±´ ê°œìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
            cert_count = len(requirements.certifications)
            doc_count = len(requirements.documents)
            source_count = len(sources)
            
            # ìš”êµ¬ì‚¬í•­ì´ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            if cert_count > 0:
                base_score += min(0.3, cert_count * 0.05)
            if doc_count > 0:
                base_score += min(0.2, doc_count * 0.03)
            if source_count > 0:
                base_score += min(0.2, source_count * 0.02)
            
            # ê¸°ê´€ ë‹¤ì–‘ì„± ì ìˆ˜
            agencies = set()
            for cert in requirements.certifications:
                if cert.agency:
                    agencies.add(cert.agency)
            
            agency_diversity_score = min(0.1, len(agencies) * 0.02)
            base_score += agency_diversity_score
            
            # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
            return min(1.0, base_score)
            
        except Exception as e:
            print(f"ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.5