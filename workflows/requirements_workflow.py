"""
ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì›Œí¬í”Œë¡œìš°
HSì½”ë“œ ê¸°ë°˜ ë¯¸êµ­ ìˆ˜ìž…ìš”ê±´ ë¶„ì„ì„ ìœ„í•œ í†µí•© ì›Œí¬í”Œë¡œìš°
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any, List, Optional
from app.services.requirements.requirements_cache_service import RequirementsCacheService

@dataclass
class RequirementsState:
    hs_code: str
    product_name: str
    product_description: str
    agency_mapping: Optional[Any] = None
    recommended_agencies: List[str] = None
    search_results: Dict[str, Any] = None
    raw_documents: List[Dict[str, Any]] = None
    llm_summary: Optional[Any] = None
    final_result: Dict[str, Any] = None
    processing_time_ms: int = 0

class RequirementsWorkflow:
    def __init__(self):
        from app.services.requirements.hs_code_agency_mapping_service import HsCodeAgencyMappingService
        from app.services.requirements.search_service import SearchService
        from app.services.requirements.llm_summary_service import LlmSummaryService
        
        self.agency_mapping_service = HsCodeAgencyMappingService()
        self.search_service = SearchService()
        self.llm_summary_service = LlmSummaryService()
        self.cache_service = RequirementsCacheService()
    
    async def analyze_requirements(
        self, 
        hs_code: str, 
        product_name: str, 
        product_description: str = "",
        force_refresh: bool = False,
        is_new_product: bool = False
    ) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹¤í–‰"""
        
        print(f"ðŸš€ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹œìž‘ - HSì½”ë“œ: {hs_code}, ê°•ì œê°±ì‹ : {force_refresh}, ì‹ ê·œìƒí’ˆ: {is_new_product}")
        
        # ìºì‹œ í™•ì¸ (ê°•ì œ ê°±ì‹ ì´ ì•„ë‹ˆê³  ì‹ ê·œ ìƒí’ˆì´ ì•„ë‹Œ ê²½ìš°)
        if not force_refresh and not is_new_product:
            cached_result = await self.cache_service.get_cached_analysis(hs_code, product_name)
            if cached_result:
                print(f"âœ… ìºì‹œì—ì„œ ë°˜í™˜")
                cached_result["cache_hit"] = True
                cached_result["force_refresh"] = force_refresh
                cached_result["is_new_product"] = is_new_product
                return cached_result
        
        start_time = datetime.now()
        
        state = RequirementsState(
            hs_code=hs_code,
            product_name=product_name,
            product_description=product_description
        )
        
        try:
            # 1ë‹¨ê³„: ê¸°ê´€ ë§¤í•‘
            mapping_result = await self.agency_mapping_service.get_relevant_agencies(
                hs_code=hs_code,
                product_name=product_name,
                product_description=product_description
            )
            state.recommended_agencies = mapping_result.recommended_agencies
            
            # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            search_queries = self._build_queries(hs_code, product_name, state.recommended_agencies)
            search_results = await self.search_service.search_requirements(
                hs_code=hs_code,
                product_name=product_name,
                agencies=state.recommended_agencies,
                search_queries=search_queries
            )
            state.search_results = search_results
            
            # 3ë‹¨ê³„: LLM ìš”ì•½
            raw_documents = self._extract_documents(search_results)
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ê¸°ë³¸ ìš”ì•½ ì‹¤í–‰ (mock ë°ì´í„° í¬í•¨)
            if not raw_documents:
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ê¸°ë³¸ ìš”ì•½ ì •ë³´ ìƒì„±")
                raw_documents = [{
                    "content": f"HSì½”ë“œ {hs_code} ({product_name})ì˜ ê¸°ë³¸ ìˆ˜ìž… ìš”ê±´ ì•ˆë‚´",
                    "source": "ê¸°ë³¸ ì•ˆë‚´",
                    "website": "https://www.cbp.gov/trade/trade-tools"
                }]
            
            summary_result = await self.llm_summary_service.summarize_regulations(
                hs_code=hs_code,
                product_name=product_name,
                raw_documents=raw_documents
            )
            state.llm_summary = summary_result
            
            # 4ë‹¨ê³„: ê²°ê³¼ í†µí•©
            state.processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            state.final_result = self._integrate_results(state)
            
            # ìºì‹œì— ì €ìž¥ (ì‹ ê·œ ìƒí’ˆì´ê±°ë‚˜ ê°•ì œ ê°±ì‹ ì¸ ê²½ìš°)
            if is_new_product or force_refresh:
                await self.cache_service.save_analysis_to_cache(hs_code, product_name, state.final_result)
                print(f"ðŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œì— ì €ìž¥")
            
            print(f"âœ… ë¶„ì„ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {state.processing_time_ms}ms")
            return state.final_result
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "status": "failed"}
    
    def _build_queries(self, hs_code: str, product_name: str, agencies: List[str]) -> Dict[str, List[str]]:
        queries = {}
        for agency in agencies:
            agency_queries = [
                f"{agency} import requirements {product_name}",
                f"{agency} regulations HS {hs_code}"
            ]
            queries[agency] = agency_queries[:3]
        return queries
    
    def _extract_documents(self, search_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        documents = []
        for agency, search_result in search_results.items():
            for result in search_result.results:
                documents.append({
                    "title": result.get("title", "Unknown"),
                    "content": result.get("content", ""),
                    "url": result.get("url", ""),
                    "agency": agency
                })
        return documents
    
    def _integrate_results(self, state: RequirementsState) -> Dict[str, Any]:
        try:
            result = {
                "hs_code": state.hs_code or "UNKNOWN",
                "product_name": state.product_name or "Unknown Product",
                "processing_time_ms": state.processing_time_ms or 0,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
                "cache_hit": False,
                "force_refresh": False,
                "is_new_product": False
            }
        except Exception as e:
            print(f"âŒ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ðŸ‘‡ ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ í•„ìˆ˜ í•„ë“œë“¤ì„ í¬í•¨í•œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
            result = {
                "hs_code": "UNKNOWN",
                "product_name": "Unknown Product", 
                "processing_time_ms": 0,
                "timestamp": datetime.now().isoformat(),
                "status": "failed",
                "cache_hit": False,
                "force_refresh": False,
                "is_new_product": False,
                "error": str(e)
            }
        
        try:
            if state.recommended_agencies:
                result["recommended_agencies"] = state.recommended_agencies
            
            if state.search_results:
                result["search_results"] = {
                    agency: {
                        "results_count": len(search_result.results),
                        "source": search_result.source,
                        "cost": search_result.cost
                    }
                    for agency, search_result in state.search_results.items()
                }
            
            if state.llm_summary:
                result["llm_summary"] = {
                    "critical_requirements": state.llm_summary.critical_requirements,
                    "required_documents": state.llm_summary.required_documents,
                    "compliance_steps": state.llm_summary.compliance_steps,
                    "estimated_costs": state.llm_summary.estimated_costs,
                    "timeline": state.llm_summary.timeline,
                    "confidence_score": state.llm_summary.confidence_score
                }
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ í•„ìˆ˜ í•„ë“œë“¤ì€ ìœ ì§€
        
        # ðŸŽ¯ ëª¨ë“  ë‹¨ê³„ì˜ ìƒì„¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìµœì¢… ê²°ê³¼ì— í¬í•¨
        result["comprehensive_metadata"] = {
            "analysis_workflow_steps": {
                "total_steps_completed": 4,  # ê¸°ê´€ë§¤í•‘, ê²€ìƒ‰, ìš”ì•½, í†µí•©
                "processing_stages": [
                    "keyword_extraction",
                    "search_agency_documents", 
                    "hybrid_api_call",
                    "document_scraping",
                    "result_consolidation",
                    "llm_summarization"
                ],
                "workflow_performance": {
                    "total_processing_time_ms": state.processing_time_ms,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "cache_hit": result.get("cache_hit", False),
                    "force_refresh": result.get("force_refresh", False),
                    "is_new_product": result.get("is_new_product", False)
                }
            },
            "data_collection_summary": {
                "agencies_analyzed": len(state.recommended_agencies) if state.recommended_agencies else 0,
                "search_results_count": len(state.search_results) if state.search_results else 0,
                "total_urls_found": sum(len(sr.results) for sr in state.search_results.values()) if state.search_results else 0,
                    "raw_documents_count": 0,  # raw_documentsëŠ” í†µí•© ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ë¨
                "llm_summary_quality": "successful" if state.llm_summary else "failed",
                "metadata_completeness_score": self._calculate_metadata_completeness(result, state)
            },
            "technical_details": {
                "search_provider": state.search_results[list(state.search_results.keys())[0]].source if state.search_results else "unknown",
                "llm_model_used": "default",  # ì‹¤ì œ ì‚¬ìš©ëœ ëª¨ë¸ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
                "data_sources": ["tavily_search", "government_apis", "web_scraping", "precedents_db"],
                "api_endpoints_called": list(self.search_service.free_api_endpoints.keys()) if hasattr(self.search_service, 'free_api_endpoints') else []
            }
        }
        
        return result
    
    def _calculate_metadata_completeness(self, result: Dict[str, Any], state) -> float:
        """ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚° (0.0-1.0)"""
        try:
            score = 0.0
            total_checks = 8
            
            # ê¸°ë³¸ ì •ë³´ í™•ì¸
            if result.get("hs_code"): score += 0.125
            if result.get("product_name"): score += 0.125
            if result.get("recommended_agencies"): score += 0.125
            if result.get("search_results"): score += 0.125
            if result.get("llm_summary"): score += 0.125
            if result.get("processing_time_ms", 0) > 0: score += 0.125
            if result.get("timestamp"): score += 0.125
            
            # í’ë¶€í•œ ë°ì´í„° í™•ì¸
            if result.get("search_results"):
                total_urls = sum(len(sr.results) for sr in result["search_results"].values() if hasattr(sr, 'results'))
                if total_urls > 0: score += 0.125  # ê²€ìƒ‰ ê²°ê³¼ê°€ ìžˆìœ¼ë©´ ë³´ë„ˆìŠ¤
            
            return min(score, 1.0)
        except Exception:
            return 0.0