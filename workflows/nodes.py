"""
LangGraph Nodes for Requirements Analysis
ê° ë‹¨ê³„ë³„ë¡œ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œë“¤
(Updated: 2025-10-10 - LLM ìš”ì•½ ì¶”ê°€, íƒ€ì… ì—ëŸ¬ ìˆ˜ì •)
(Updated: 2025-10-11 - Phase 2-4 ì „ë¬¸ ì„œë¹„ìŠ¤ ì—°ê²°)
"""

from typing import Dict, Any, List
from .tools import RequirementsTools
from app.services.requirements.keyword_extractor import KeywordExtractor, HfKeywordExtractor, OpenAiKeywordExtractor
from app.services.requirements.tavily_search import TavilySearchService
from app.services.requirements.web_scraper import WebScraper
from app.services.requirements.error_handler import error_handler, WorkflowError, ErrorSeverity, ErrorRecoveryStrategy
from app.models.requirement_models import RequirementAnalysisRequest
from datetime import datetime
import asyncio

# Phase 2-4 ì „ë¬¸ ì„œë¹„ìŠ¤ import
from app.services.requirements.detailed_regulations_service import detailed_regulations_service
from app.services.requirements.testing_procedures_service import testing_procedures_service
from app.services.requirements.penalties_service import penalties_service
from app.services.requirements.validity_service import validity_service
from app.services.requirements.cross_validation_service import CrossValidationService


class RequirementsNodes:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ LangGraph ë…¸ë“œë“¤"""
    
    def __init__(self):
        # RequirementsToolsì—ì„œ í”„ë¡œë°”ì´ë”ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
        self.tools = RequirementsTools()
        self.web_scraper = WebScraper()
        self.keyword_extractor = None
        self.hf_extractor = None
        self.openai_extractor = None
        
        # Phase 2-4 ì „ë¬¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.detailed_regulations = detailed_regulations_service
        self.testing_procedures = testing_procedures_service
        self.penalties = penalties_service
        self.validity = validity_service
        self.cross_validation = CrossValidationService()
        
        print("âœ… RequirementsNodes ì´ˆê¸°í™” ì™„ë£Œ (Phase 2-4 ì„œë¹„ìŠ¤ í¬í•¨)")

    async def extract_core_keywords(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒí’ˆëª…/ì„¤ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±).
        - ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë¶„ì ˆ
        - ë¶ˆìš©ì–´ ì œê±°
        - ê¸¸ì´ 3 ì´ìƒ ë‹¨ì–´ ìš°ì„ , ìƒìœ„ 3ê°œ ë°˜í™˜
        - í•œê¸€ì¼ ê²½ìš° ê°„ë‹¨ ë§¤í•‘ ì‹œë„
        """
        try:
            request = state["request"]
            name = (request.product_name or "").strip()
            desc = (request.product_description or "").strip()
            
            # HF ì¶”ì¶œê¸° ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±
            if self.hf_extractor is None:
                try:
                    self.hf_extractor = HfKeywordExtractor()
                except Exception as e:
                    self.hf_extractor = None
                    print(f"âš ï¸ HF í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            if self.keyword_extractor is None:
                self.keyword_extractor = KeywordExtractor()
            
            if self.openai_extractor is None:
                try:
                    self.openai_extractor = OpenAiKeywordExtractor()
                except Exception as e:
                    self.openai_extractor = None
                    print(f"âš ï¸ OpenAI í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

            core_keywords = []
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„ (ìš°ì„ ìˆœìœ„: OpenAI â†’ HF â†’ íœ´ë¦¬ìŠ¤í‹±)
            try:
                if self.openai_extractor:
                    core_keywords = self.openai_extractor.extract(name, desc, top_k=3)
                    print(f"âœ… OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {core_keywords}")
            except Exception as e:
                print(f"âš ï¸ OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                core_keywords = []
            
            if not core_keywords:
                try:
                    if self.hf_extractor:
                        core_keywords = self.hf_extractor.extract(name, desc, top_k=3)
                        print(f"âœ… HF í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {core_keywords}")
                except Exception as e:
                    print(f"âš ï¸ HF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    core_keywords = []
            
            if not core_keywords:
                try:
                    core_keywords = self.keyword_extractor.extract(name, desc, top_k=3)
                    print(f"âœ… íœ´ë¦¬ìŠ¤í‹± í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {core_keywords}")
                except Exception as e:
                    print(f"âŒ íœ´ë¦¬ìŠ¤í‹± í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    # ìµœì¢… í´ë°±: ìƒí’ˆëª…ì—ì„œ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                    core_keywords = self._extract_fallback_keywords(name, desc)
                    print(f"ğŸ”„ í´ë°± í‚¤ì›Œë“œ ì¶”ì¶œ: {core_keywords}")
            
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ë…¸ë“œ ì „ì²´ ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì²˜ë¦¬
            error_result = error_handler.handle_error(
                WorkflowError(
                    f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}",
                    ErrorSeverity.MEDIUM,
                    ErrorRecoveryStrategy.FALLBACK,
                    {'step': 'keyword_extraction', 'hs_code': request.hs_code}
                ),
                {'step': 'keyword_extraction', 'state': state}
            )
            
            if error_result['continue_workflow']:
                core_keywords = error_result.get('fallback_data', {}).get('keywords', ['default'])
                print(f"ğŸ”„ ì—ëŸ¬ ë³µêµ¬ í›„ í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {core_keywords}")
            else:
                raise WorkflowError("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨", ErrorSeverity.HIGH)
        
        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì‹œë„í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
        state["core_keywords"] = core_keywords
        state["keyword_strategies"] = [
            {"strategy": "top1", "keywords": core_keywords[:1]},
            {"strategy": "top2", "keywords": core_keywords[:2]},
            {"strategy": "top3", "keywords": core_keywords[:3]}
        ]
        
        # ğŸ¯ í‚¤ì›Œë“œ ì¶”ì¶œ ë‹¨ê³„ì˜ ìƒì„¸ metadata ìˆ˜ì§‘
        keyword_metadata = {
            "extraction_step": {
                "hs_code": request.hs_code,
                "product_name": request.product_name,
                "product_description": request.product_description,
                "extraction_methods_tried": [
                    {"method": "OpenAI", "success": self.openai_extractor is not None, "keywords_found": core_keywords if self.openai_extractor else []},
                    {"method": "HuggingFace", "success": self.hf_extractor is not None, "keywords_found": core_keywords if self.hf_extractor else []},
                    {"method": "Heuristic", "success": True, "keywords_found": core_keywords}
                ],
                "final_keywords": core_keywords,
                "keyword_count": len(core_keywords),
                "extraction_timestamp": datetime.now().isoformat(),
                "keyword_sources": {
                    "from_product_name": name,
                    "from_description": desc,
                    "combined_text": f"{name} {desc}".strip()
                }
            }
        }
        state["detailed_metadata"] = state.get("detailed_metadata", {})
        state["detailed_metadata"].update(keyword_metadata)
        
        print(f"\nğŸ” [NODE] í•µì‹¬ í‚¤ì›Œë“œ: {core_keywords}")
        print(f"ğŸ” [NODE] í‚¤ì›Œë“œ ì „ëµ: {[s['strategy'] for s in state['keyword_strategies']]}")
        print(f"ğŸ” [METADATA] í‚¤ì›Œë“œ ì¶”ì¶œ ìƒì„¸ ì •ë³´ ì €ì¥ë¨")
        state["next_action"] = "call_hybrid_api"
        return state
    
    def _extract_fallback_keywords(self, product_name: str, product_description: str) -> List[str]:
        """í´ë°± í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹±)"""
        text = f"{product_name} {product_description}".lower()
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_mapping = {
            'vitamin': ['vitamin', 'supplement', 'health'],
            'serum': ['serum', 'skincare', 'beauty'],
            'cream': ['cream', 'moisturizer', 'skincare'],
            'food': ['food', 'nutrition', 'diet'],
            'cosmetic': ['cosmetic', 'beauty', 'makeup'],
            'electronic': ['electronic', 'device', 'technology'],
            'toy': ['toy', 'children', 'play'],
            'clothing': ['clothing', 'garment', 'textile']
        }
        
        # ë§¤í•‘ëœ í‚¤ì›Œë“œ ì°¾ê¸°
        for key, keywords in keyword_mapping.items():
            if key in text:
                return keywords[:3]
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = text.split()
        keywords = []
        for word in words:
            if len(word) > 3 and word.isalpha():
                keywords.append(word)
                if len(keywords) >= 3:
                    break
        
        return keywords if keywords else ['product', 'import', 'requirement']
    
    async def search_agency_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        request = state["request"]
        hs_code = request.hs_code
        product_name = request.product_name
        keywords = state.get("core_keywords") or []
        keyword_strategies = state.get("keyword_strategies", [])
        
        # í‚¤ì›Œë“œ ì „ëµì„ ë‹¨ê³„ì ìœ¼ë¡œ ì‹œë„ (top1 â†’ top2 â†’ top3)
        query_terms = []
        for strategy in keyword_strategies:
            if strategy["keywords"]:
                query_terms.append(" ".join(strategy["keywords"]))
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒí’ˆëª… ì‚¬ìš©
        if not query_terms:
            query_terms = [product_name]
        
        query_term = query_terms[0]  # ì²« ë²ˆì§¸ ì „ëµ ì‚¬ìš©
        
        print(f"\nğŸ” [NODE] ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {product_name}")
        
        # ê¸°ë³¸ URL í´ë°± (TavilySearch ì‹¤íŒ¨ ì‹œ ì‚¬ìš©) - 9ê°œ ê¸°ê´€ ëª¨ë‘
        default_urls = {
            "FDA": "https://www.fda.gov",
            "FCC": "https://www.fcc.gov",
            "CBP": "https://www.cbp.gov",
            "USDA": "https://www.usda.gov",
            "EPA": "https://www.epa.gov",
            "CPSC": "https://www.cpsc.gov",
            "KCS": "https://www.customs.go.kr",
            "MFDS": "https://www.mfds.go.kr",
            "MOTIE": "https://www.motie.go.kr"
        }
        
        # HSì½”ë“œ 8ìë¦¬ì™€ 6ìë¦¬ ì¶”ì¶œ
        hs_code_8digit = hs_code
        hs_code_6digit = ".".join(hs_code.split(".")[:2]) if "." in hs_code else hs_code
        
        print(f"  ğŸ“‹ 8ìë¦¬ HSì½”ë“œ: {hs_code_8digit}")
        print(f"  ğŸ“‹ 6ìë¦¬ HSì½”ë“œ: {hs_code_6digit}")
        
        # íƒ€ê²Ÿ ê¸°ê´€ ê²°ì • (AI ë§¤í•‘ ë˜ëŠ” í•˜ë“œì½”ë”© ë˜ëŠ” ì±•í„° ê¸°ë°˜ ì¶”ë¡ )
        target_agencies_data = await self.tools._get_target_agencies_for_hs_code(hs_code, product_name)
        target_agencies = target_agencies_data.get("primary_agencies", [])
        
        # íƒ€ê²Ÿ ê¸°ê´€ì´ ì—†ìœ¼ë©´ ìµœì†Œí•œ FDAëŠ” í¬í•¨
        if not target_agencies:
            target_agencies = ["FDA"]
            print(f"  âš ï¸ íƒ€ê²Ÿ ê¸°ê´€ ì—†ìŒ - ê¸°ë³¸ê°’ FDA ì‚¬ìš©")
        
        print(f"  ğŸ¯ íƒ€ê²Ÿ ê¸°ê´€: {', '.join(target_agencies)} ({target_agencies_data.get('source', 'unknown')})")
        print(f"  ğŸ’° Tavily ê²€ìƒ‰ ìµœì í™”: {len(target_agencies)}ê°œ ê¸°ê´€ë§Œ ê²€ìƒ‰")
        
        # ê° ê¸°ê´€ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ (8ìë¦¬ì™€ 6ìë¦¬ ëª¨ë‘) - íƒ€ê²Ÿ ê¸°ê´€ë§Œ!
        search_queries = {}
        
        # ê¸°ê´€ë³„ ì‚¬ì´íŠ¸ ë„ë©”ì¸ ë§¤í•‘
        agency_domains = {
            "FDA": "fda.gov",
            "FCC": "fcc.gov",
            "CBP": "cbp.gov",
            "USDA": "usda.gov",
            "EPA": "epa.gov",
            "CPSC": "cpsc.gov",
            "KCS": "customs.go.kr",
            "MFDS": "mfds.go.kr",
            "MOTIE": "motie.go.kr"
        }
        
        # íƒ€ê²Ÿ ê¸°ê´€ë§Œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        for agency in target_agencies:
            domain = agency_domains.get(agency, f"{agency.lower()}.gov")
            
            # 8ìë¦¬ HSì½”ë“œ ê²€ìƒ‰
            if agency == "FDA":
                search_queries[f"{agency}_8digit"] = f"site:{domain} import requirements {query_term} HS {hs_code_8digit}"
            elif agency == "FCC":
                search_queries[f"{agency}_8digit"] = f"site:{domain} device authorization requirements {query_term} HS {hs_code_8digit}"
            elif agency == "CBP":
                search_queries[f"{agency}_8digit"] = f"site:{domain} import documentation requirements HS {hs_code_8digit} {query_term}"
            elif agency == "USDA":
                search_queries[f"{agency}_8digit"] = f"site:{domain} agricultural import requirements {query_term} HS {hs_code_8digit}"
            elif agency == "EPA":
                search_queries[f"{agency}_8digit"] = f"site:{domain} environmental regulations {query_term} HS {hs_code_8digit}"
            elif agency == "CPSC":
                search_queries[f"{agency}_8digit"] = f"site:{domain} consumer product safety {query_term} HS {hs_code_8digit}"
            elif agency == "KCS":
                search_queries[f"{agency}_8digit"] = f"site:{domain} Korea customs import requirements {query_term} HS {hs_code_8digit}"
            elif agency == "MFDS":
                search_queries[f"{agency}_8digit"] = f"site:{domain} food drug safety import {query_term} HS {hs_code_8digit}"
            elif agency == "MOTIE":
                search_queries[f"{agency}_8digit"] = f"site:{domain} trade policy import requirements {query_term} HS {hs_code_8digit}"
        
        # 6ìë¦¬ HSì½”ë“œ ê²€ìƒ‰ (ìœ ì‚¬) - íƒ€ê²Ÿ ê¸°ê´€ë§Œ!
        for agency in target_agencies:
            domain = agency_domains.get(agency, f"{agency.lower()}.gov")
            
            # 6ìë¦¬ HSì½”ë“œ ê²€ìƒ‰
            if agency == "FDA":
                search_queries[f"{agency}_6digit"] = f"site:{domain} import requirements {query_term} HS {hs_code_6digit}"
            elif agency == "FCC":
                search_queries[f"{agency}_6digit"] = f"site:{domain} device authorization requirements {query_term} HS {hs_code_6digit}"
            elif agency == "CBP":
                search_queries[f"{agency}_6digit"] = f"site:{domain} import documentation requirements HS {hs_code_6digit} {query_term}"
            elif agency == "USDA":
                search_queries[f"{agency}_6digit"] = f"site:{domain} agricultural import requirements {query_term} HS {hs_code_6digit}"
            elif agency == "EPA":
                search_queries[f"{agency}_6digit"] = f"site:{domain} environmental regulations {query_term} HS {hs_code_6digit}"
            elif agency == "CPSC":
                search_queries[f"{agency}_6digit"] = f"site:{domain} consumer product safety {query_term} HS {hs_code_6digit}"
            elif agency == "KCS":
                search_queries[f"{agency}_6digit"] = f"site:{domain} Korea customs import requirements {query_term} HS {hs_code_6digit}"
            elif agency == "MFDS":
                search_queries[f"{agency}_6digit"] = f"site:{domain} food drug safety import {query_term} HS {hs_code_6digit}"
            elif agency == "MOTIE":
                search_queries[f"{agency}_6digit"] = f"site:{domain} trade policy import requirements {query_term} HS {hs_code_6digit}"
        
        search_results = {}
        
        for agency, query in search_queries.items():
            print(f"\n  ğŸ“¡ {agency} ê²€ìƒ‰ ì¤‘...")
            print(f"    ì¿¼ë¦¬: {query}")
            
            # í”„ë¡œë°”ì´ë”ë¥¼ í†µí•œ ê²€ìƒ‰ ì‹œë„ (ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘)
            results = await self.tools.search_provider.search(query, max_results=15)  # ê²€ìƒ‰ ê²°ê³¼ë¥¼ 15ê°œë¡œ í™•ì¥
            print(f"    ğŸ“Š {self.tools.search_provider.provider_name} ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
            
            # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            chosen_urls = []
            
            if not results and self.tools.search_provider.provider_name == "disabled":
                print(f"    ğŸ”‡ ê²€ìƒ‰ ë¹„í™œì„±í™” ëª¨ë“œ: '{query}' ìŠ¤í‚µë¨")
                agency_name = agency.split("_")[0]
                default_url = default_urls.get(agency_name)
                if default_url:
                    chosen_urls = [default_url]
            elif not results:
                print(f"    ğŸ’¡ íŒ: TAVILY_API_KEYë¥¼ ì„¤ì •í•˜ë©´ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                agency_name = agency.split("_")[0]
                default_url = default_urls.get(agency_name)
                if default_url:
                    chosen_urls = [default_url]
                print(f"    ğŸ”„ {agency} TavilySearch ì‹¤íŒ¨, ê¸°ë³¸ URL ì‚¬ìš©: {default_url}")
            else:
                # ê²€ìƒ‰ ì„±ê³µ - ì—¬ëŸ¬ ë§í¬ ìˆ˜ì§‘ (ìµœëŒ€ 10ê°œ)
                for i, result in enumerate(results, 1):
                    title = result.get('title', 'No title')
                    url = result.get('url', 'No URL')
                    print(f"      {i}. {title}")
                    print(f"         URL: {url}")
                
                # site: ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í–ˆìœ¼ë¯€ë¡œ ëª¨ë“  ê²°ê³¼ê°€ ê³µì‹ ì‚¬ì´íŠ¸ (ìµœëŒ€ 10ê°œ ì„ íƒ)
                chosen_urls = [result.get("url") for result in results[:10] if result.get("url")]
                print(f"    âœ… {agency} ê³µì‹ ì‚¬ì´íŠ¸ ê²°ê³¼ {len(chosen_urls)}ê°œ ì„ íƒ")
            
            search_results[agency] = {
                "urls": chosen_urls,  # ì—¬ëŸ¬ URL ì €ì¥
                "all_results": results,
                "query": query,
                "is_fallback": not results,  # í´ë°± ì‚¬ìš© ì—¬ë¶€ í‘œì‹œ
                "hs_code_type": "8digit" if "8digit" in agency else "6digit",
                "agency": agency.split("_")[0]  # FDA_8digit -> FDA
            }
        
        # ìš”ì•½ ì¹´ìš´íŠ¸: í•˜ë‚˜ ì´ìƒì˜ URL ë³´ìœ í•œ í•­ëª© ìˆ˜
        found_count = sum(1 for v in search_results.values() if v.get("urls"))
        print(f"\nğŸ“‹ [NODE] ê²€ìƒ‰ ì™„ë£Œ - {found_count}ê°œ URL ì„¸íŠ¸ ë°œê²¬")
        
        # ğŸ¯ ê¸°ê´€ë³„ ê²€ìƒ‰ ë‹¨ê³„ì˜ ìƒì„¸ metadata ìˆ˜ì§‘
        search_metadata = {
            "search_step": {
                "hs_code_8digit": hs_code_8digit,
                "hs_code_6digit": hs_code_6digit,
                "query_term": query_term,
                "search_strategies": search_queries,
                "search_provider": self.tools.search_provider.provider_name if hasattr(self.tools, 'search_provider') else "unknown",
                "total_urls_found": found_count,
                "search_results_per_agency": {
                    agency: {
                        "url_count": len(search_data["urls"]),
                        "query": search_data["query"],
                        "hs_code_type": search_data["hs_code_type"],
                        "is_fallback": search_data["is_fallback"],
                        "search_timestamp": datetime.now().isoformat()
                    } for agency, search_data in search_results.items()
                },
                "default_urls_used": default_urls,
                "search_performance": {
                    "total_queries_executed": len(search_queries),
                    "successful_searches": len([sr for sr in search_results.values() if not sr.get("is_fallback", False)]),
                    "fallback_searches": len([sr for sr in search_results.values() if sr.get("is_fallback", False)])
                }
            }
        }
        state["detailed_metadata"] = state.get("detailed_metadata", {})
        state["detailed_metadata"].update(search_metadata)

        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["search_results"] = search_results
        # ì°¸ê³  ë§í¬ ì €ì¥
        request = state["request"]
        save_meta = self.tools.save_reference_links(request.hs_code, request.product_name, search_results)
        state["references_saved"] = save_meta
        
        print(f"ğŸ” [METADATA] ê¸°ê´€ë³„ ê²€ìƒ‰ ìƒì„¸ ì •ë³´ ì €ì¥ë¨ - ì´ {found_count}ê°œ URL ë°œê²¬")
        state["next_action"] = "scrape_documents"
        return state

    async def call_hybrid_api(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ API í˜¸ì¶œ ë…¸ë“œ (Data.gov/USDA/EPA + ì›¹ ê²€ìƒ‰ í†µí•© + Phase 2-4)."""
        request = state["request"]
        hs_code = request.hs_code
        product_name = request.product_name
        product_description = request.product_description or ""
        keywords = state.get("core_keywords") or []
        query_term = (keywords[0] if keywords else product_name) or ""
        print(f"\nğŸ“¡ [NODE] í•˜ì´ë¸Œë¦¬ë“œ API í˜¸ì¶œ ì‹œì‘: {hs_code} / {product_name}")
        try:
            # Phase 2-4 í¬í•¨ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            hybrid_start_time = datetime.now()
            hybrid = await self.tools.search_requirements_hybrid(hs_code, query_term, product_description)
            hybrid_end_time = datetime.now()
            
            # ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë‹¨ê³„ì˜ ìƒì„¸ metadata ìˆ˜ì§‘
            hybrid_metadata = {
                "hybrid_api_step": {
                    "hs_code": hs_code,
                    "query_term": query_term,
                    "product_description": product_description[:100] if product_description else "",  # ì²˜ìŒ 100ìë§Œ
                    "api_parameters": {
                        "hs_code": hs_code,
                        "query": query_term,
                        "description_length": len(product_description) if product_description else 0,
                        "keywords_length": len(keywords)
                    },
                    "api_response": {
                        "success": not hybrid.get("error"),
                        "response_time_ms": int((hybrid_end_time - hybrid_start_time).total_seconds() * 1000),
                        "data_keys": list(hybrid.keys()) if isinstance(hybrid, dict) else [],
                        "error_message": hybrid.get("error") if hybrid.get("error") else None
                    },
                    "timestamp": hybrid_end_time.isoformat()
                }
            }
            state["detailed_metadata"] = state.get("detailed_metadata", {})
            state["detailed_metadata"].update(hybrid_metadata)
            
            state["hybrid_result"] = hybrid
            print(f"ğŸ“¡ [METADATA] í•˜ì´ë¸Œë¦¬ë“œ API ê²€ìƒ‰ ìƒì„¸ ì •ë³´ ì €ì¥ë¨ - ì‘ë‹µì‹œê°„: {(hybrid_end_time - hybrid_start_time).total_seconds()*1000:.0f}ms")
            state["next_action"] = "scrape_documents"
        except Exception as e:
            print(f"  âŒ í•˜ì´ë¸Œë¦¬ë“œ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            hybrid_metadata = {
                "hybrid_api_step": {
                    "hs_code": hs_code,
                    "query_term": query_term,
                    "product_description": product_description[:100] if product_description else "",
                    "api_parameters": {
                        "hs_code": hs_code,
                        "query": query_term,
                        "description_length": len(product_description) if product_description else 0,
                        "keywords_length": len(keywords)
                    },
                    "api_response": {
                        "success": False,
                        "response_time_ms": None,
                        "data_keys": [],
                        "error_message": str(e)
                    },
                    "timestamp": datetime.now().isoformat()
                }
            }
            state["detailed_metadata"] = state.get("detailed_metadata", {})
            state["detailed_metadata"].update(hybrid_metadata)
            
            state["hybrid_result"] = {"error": str(e)}
            print(f"ğŸ“¡ [METADATA] í•˜ì´ë¸Œë¦¬ë“œ API ì˜¤ë¥˜ ì •ë³´ ì €ì¥ë¨: {e}")
            state["next_action"] = "scrape_documents"
        return state
    
    async def scrape_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ë…¸ë“œ"""
        search_results = state["search_results"]
        request = state["request"]
        hs_code = request.hs_code
        
        print(f"\nğŸ” [NODE] ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        scraped_data = {}
        
        # ê¸°ê´€ë³„ë¡œ 8ìë¦¬ì™€ 6ìë¦¬ ê²°ê³¼ í†µí•©
        agency_results = {}
        
        for agency_key, search_data in search_results.items():
            agency_name = search_data["agency"]
            hs_code_type = search_data["hs_code_type"]
            
            if agency_name not in agency_results:
                agency_results[agency_name] = {
                    "8digit": {"urls": [], "results": []},
                    "6digit": {"urls": [], "results": []}
                }
            
            agency_results[agency_name][hs_code_type]["urls"] = search_data["urls"]
        
        # ê° ê¸°ê´€ë³„ë¡œ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰
        for agency_name, agency_data in agency_results.items():
            print(f"\n  ğŸ“„ {agency_name} ìŠ¤í¬ë˜í•‘ ì¤‘...")
            
            # 8ìë¦¬ì™€ 6ìë¦¬ URL ëª¨ë‘ ìˆ˜ì§‘
            all_urls = agency_data["8digit"]["urls"] + agency_data["6digit"]["urls"]
            
            if not all_urls:
                print(f"    âŒ {agency_name}: ìŠ¤í¬ë˜í•‘í•  URL ì—†ìŒ")
                # URLì´ ì—†ì–´ë„ Noneìœ¼ë¡œ ê²°ê³¼ ì €ì¥
                scraped_data[agency_name] = {
                    "certifications": [],
                    "documents": [],
                    "labeling": [],
                    "sources": [],
                    "status": "no_urls_found",
                    "hs_code_8digit": {"urls": agency_data["8digit"]["urls"], "results": []},
                    "hs_code_6digit": {"urls": agency_data["6digit"]["urls"], "results": []}
                }
                continue
            
            print(f"    ğŸ“‹ 8ìë¦¬ URL: {len(agency_data['8digit']['urls'])}ê°œ")
            print(f"    ğŸ“‹ 6ìë¦¬ URL: {len(agency_data['6digit']['urls'])}ê°œ")
            print(f"    ğŸ“‹ ì´ URL: {len(all_urls)}ê°œ")
            
            try:
                # 9ê°œ ê¸°ê´€ ëª¨ë‘ ì²˜ë¦¬
                if agency_name == "FDA":
                    result = await self.web_scraper.scrape_fda_requirements(hs_code, all_urls[0])
                elif agency_name == "FCC":
                    result = await self.web_scraper.scrape_fcc_requirements(hs_code, all_urls[0])
                elif agency_name == "CBP":
                    result = await self.web_scraper.scrape_cbp_requirements(hs_code, all_urls[0])
                elif agency_name == "USDA":
                    result = await self.web_scraper.scrape_usda_requirements(hs_code, all_urls[0])
                elif agency_name == "EPA":
                    result = await self.web_scraper.scrape_epa_requirements(hs_code, all_urls[0])
                elif agency_name == "CPSC":
                    result = await self.web_scraper.scrape_cpsc_requirements(hs_code, all_urls[0])
                elif agency_name == "KCS":
                    result = await self.web_scraper.scrape_kcs_requirements(hs_code, all_urls[0])
                elif agency_name == "MFDS":
                    result = await self.web_scraper.scrape_mfds_requirements(hs_code, all_urls[0])
                elif agency_name == "MOTIE":
                    result = await self.web_scraper.scrape_motie_requirements(hs_code, all_urls[0])
                else:
                    print(f"    âŒ {agency_name}: ì§€ì›ë˜ì§€ ì•ŠëŠ” ê¸°ê´€")
                    continue
                
                # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                certs = result.get("certifications", [])
                docs = result.get("documents", [])
                
                print(f"    âœ… {agency_name} ìŠ¤í¬ë˜í•‘ ì„±ê³µ:")
                print(f"      ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ")
                for cert in certs:
                    print(f"        â€¢ {cert.get('name', 'Unknown')} ({cert.get('agency', 'Unknown')})")
                
                print(f"      ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ")
                for doc in docs:
                    print(f"        â€¢ {doc.get('name', 'Unknown')}")
                
                # HSì½”ë“œ êµ¬ë¶„ ì •ë³´ ì¶”ê°€
                # ì•ˆì „í•˜ê²Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (íƒ€ì… ì—ëŸ¬ ë°©ì§€)
                certs_list = result.get("certifications", [])
                docs_list = result.get("documents", [])
                if not isinstance(certs_list, list):
                    certs_list = []
                if not isinstance(docs_list, list):
                    docs_list = []
                
                result["hs_code_8digit"] = {
                    "urls": agency_data["8digit"]["urls"],
                    "results": certs_list + docs_list
                }
                result["hs_code_6digit"] = {
                    "urls": agency_data["6digit"]["urls"],
                    "results": []
                }
                result["status"] = "success"
                
                scraped_data[agency_name] = result
                
            except Exception as e:
                print(f"    âŒ {agency_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                scraped_data[agency_name] = {
                    "certifications": [],
                    "documents": [],
                    "labeling": [],
                    "sources": [],
                    "status": "scraping_failed",
                    "error": str(e),
                    "hs_code_8digit": {"urls": agency_data["8digit"]["urls"], "results": []},
                    "hs_code_6digit": {"urls": agency_data["6digit"]["urls"], "results": []}
                }
        
        print(f"\nğŸ“‹ [NODE] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ - {len(scraped_data)}ê°œ ê¸°ê´€ ì²˜ë¦¬")
        
        # ğŸ¯ ì›¹ ìŠ¤í¬ë˜í•‘ ë‹¨ê³„ì˜ ìƒì„¸ metadata ìˆ˜ì§‘
        scraping_metadata = {
            "scraping_step": {
                "hs_code": hs_code,
                "total_agencies_scraped": len(scraped_data),
                "scraping_performance": {
                    "successful_scraping": len([data for data in scraped_data.values() if data.get("status") == "success"]),
                    "failed_scraping": len([data for data in scraped_data.values() if data.get("status") in ["scraping_failed", "no_urls_found"]]) + len([data for data in scraped_data.values() if data.get("error")]),
                    "total_certifications_found": sum(len(data.get("certifications", [])) for data in scraped_data.values()),
                    "total_documents_found": sum(len(data.get("documents", [])) for data in scraped_data.values()),
                    "total_sources_collected": sum(len(data.get("sources", [])) for data in scraped_data.values())
                },
                "scraped_agencies_details": {
                    agency: {
                        "status": data.get("status", "unknown"),
                        "certifications_count": len(data.get("certifications", [])),
                        "documents_count": len(data.get("documents", [])),
                        "sources_count": len(data.get("sources", [])),
                        "has_raw_page_data": "raw_page_data" in data,
                        "hs_code_8digit_urls": len(data.get("hs_code_8digit", {}).get("urls", [])),
                        "hs_code_6digit_urls": len(data.get("hs_code_6digit", {}).get("urls", [])),
                        "error_message": data.get("error") if data.get("error") else None,
                        "scraping_timestamp": datetime.now().isoformat()
                    } for agency, data in scraped_data.items()
                },
                "scraping_statistics": {
                    "8digit_hs_code_urls": sum(len(data.get("hs_code_8digit", {}).get("urls", [])) for data in scraped_data.values()),
                    "6digit_hs_code_urls": sum(len(data.get("hs_code_6digit", {}).get("urls", [])) for data in scraped_data.values()),
                }
            }
        }
        state["detailed_metadata"] = state.get("detailed_metadata", {})
        state["detailed_metadata"].update(scraping_metadata)
        
        print(f"ğŸ“‹ [METADATA] ì›¹ ìŠ¤í¬ë˜í•‘ ìƒì„¸ ì •ë³´ ì €ì¥ë¨ - ì¸ì¦ ìš”ê±´: {scraping_metadata['scraping_step']['scraping_performance']['total_certifications_found']}ê°œ, ì„œë¥˜: {scraping_metadata['scraping_step']['scraping_performance']['total_documents_found']}ê°œ")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["scraped_data"] = scraped_data
        state["next_action"] = "consolidate_results"
        return state
    
    async def consolidate_results(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
        scraped_data = state["scraped_data"]
        
        print(f"\nğŸ” [NODE] ê²°ê³¼ í†µí•© ì‹œì‘")
        
        all_certifications = []
        all_documents = []
        all_sources = []
        
        for agency, data in scraped_data.items():
            status = data.get("status", "unknown")
            
            if status == "no_urls_found":
                print(f"  âŒ {agency}: URL ì—†ìŒ (None)")
                continue
            elif status == "scraping_failed":
                print(f"  âŒ {agency}: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (None)")
                continue
            elif "error" in data:
                print(f"  âŒ {agency}: ì˜¤ë¥˜ë¡œ ì¸í•´ ì œì™¸ (None)")
                continue
                
            print(f"  ğŸ“Š {agency} ë°ì´í„° í†µí•©:")
            
            # ì¸ì¦ìš”ê±´ í†µí•©
            certs = data.get("certifications", [])
            all_certifications.extend(certs)
            print(f"    ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ ì¶”ê°€")
            
            # í•„ìš”ì„œë¥˜ í†µí•©
            docs = data.get("documents", [])
            all_documents.extend(docs)
            print(f"    ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ ì¶”ê°€")
            
            # ì¶œì²˜ í†µí•©
            sources = data.get("sources", [])
            all_sources.extend(sources)
            print(f"    ğŸ“š ì¶œì²˜: {len(sources)}ê°œ ì¶”ê°€")
        
        print(f"\nğŸ“‹ [NODE] í†µí•© ì™„ë£Œ:")
        print(f"  ğŸ“‹ ì´ ì¸ì¦ìš”ê±´: {len(all_certifications)}ê°œ")
        print(f"  ğŸ“„ ì´ í•„ìš”ì„œë¥˜: {len(all_documents)}ê°œ")
        print(f"  ğŸ“š ì´ ì¶œì²˜: {len(all_sources)}ê°œ")
        
        consolidation_start_time = datetime.now()
        
        # ğŸ†• FAISS DBì—ì„œ íŒë¡€ ìˆ˜ì§‘ (API í˜¸ì¶œ ëŒ€ì‹ !)
        request = state.get("request")
        precedents_list = []
        precedents_fetch_start = datetime.now()
        
        if request:
            try:
                # FAISS DBì—ì„œ íŒë¡€ ê°€ì ¸ì˜¤ê¸°
                from app.services.requirements.precedent_validation_service import get_precedent_validation_service
                precedent_validator = get_precedent_validation_service()
                
                precedents_list = await precedent_validator._get_precedents_from_db(
                    hs_code=request.hs_code,
                    product_name=request.product_name
                )
                
                precedents_fetch_end = datetime.now()
                precedents_fetch_time = (precedents_fetch_end - precedents_fetch_start).total_seconds() * 1000
                print(f"ğŸ“Š FAISS DB íŒë¡€ ìˆ˜ì§‘ ì„±ê³µ: {len(precedents_list)}ê°œ íŒë¡€ í™•ì¸ë¨ ({precedents_fetch_time:.0f}ms)")
                
                cbp = {
                    "hs_code": request.hs_code,
                    "count": len(precedents_list),
                    "precedents": precedents_list,
                    "source": "faiss_db"
                }
                
            except Exception as e:
                cbp = {"error": "precedent_fetch_failed", "error_message": str(e)}
                precedents_fetch_end = datetime.now()
                precedents_fetch_time = (precedents_fetch_end - precedents_fetch_start).total_seconds() * 1000
                print(f"ğŸ“Š FAISS DB íŒë¡€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e} ({precedents_fetch_time:.0f}ms)")

        # í•˜ì´ë¸Œë¦¬ë“œ(API+ì›¹) ê²°ê³¼ë„ í†µí•© (Phase 2-4 í¬í•¨)
        hybrid = state.get("hybrid_result") or {}
        hybrid_certifications = 0
        hybrid_documents = 0
        hybrid_sources = 0
        phase_2_4_counts = {"testing_procedures": 0, "penalties_enforcement": 0, "validity_periods": 0}
        
        if hybrid and not hybrid.get("error"):
            combined = hybrid.get("combined_results", {})
            if combined:
                # ì•ˆì „í•˜ê²Œ intë¡œ ë³€í™˜ (íƒ€ì… ì—ëŸ¬ ë°©ì§€)
                certs = combined.get("certifications", [])
                docs = combined.get("documents", [])
                srcs = combined.get("sources", [])
                
                hybrid_certifications = len(certs) if isinstance(certs, list) else 0
                hybrid_documents = len(docs) if isinstance(docs, list) else 0
                hybrid_sources = len(srcs) if isinstance(srcs, list) else 0
                
                all_certifications.extend(combined.get("certifications", []))
                all_documents.extend(combined.get("documents", []))
                all_sources.extend(combined.get("sources", []))
                
                # Phase 2-4 ê²°ê³¼ í†µí•©
                phase_2_4_counts = {
                    "testing_procedures": len(combined.get('testing_procedures', [])),
                    "penalties_enforcement": len(combined.get('penalties_enforcement', [])),
                    "validity_periods": len(combined.get('validity_periods', []))
                }
                print(f"  ğŸ“Š Phase 2-4 ê²°ê³¼ í†µí•©:")
                print(f"    ğŸ§ª ê²€ì‚¬ ì ˆì°¨: {phase_2_4_counts['testing_procedures']}ê°œ")
                print(f"    âš–ï¸ ì²˜ë²Œ ì •ë³´: {phase_2_4_counts['penalties_enforcement']}ê°œ")
                print(f"    â° ìœ íš¨ê¸°ê°„: {phase_2_4_counts['validity_periods']}ê°œ")

        consolidation_end_time = datetime.now()
        consolidation_time = (consolidation_end_time - consolidation_start_time).total_seconds() * 1000

        # ğŸ¯ ê²°ê³¼ í†µí•© ë‹¨ê³„ì˜ ìƒì„¸ metadata ìˆ˜ì§‘
        consolidation_metadata = {
            "consolidation_step": {
                "hs_code": request.hs_code if request else "unknown",
                "consolidation_performance": {
                    "total_processing_time_ms": consolidation_time,
                    "precedents_fetch_time_ms": precedents_fetch_time if 'precedents_fetch_time' in locals() else None,
                    "consolidation_timestamp": consolidation_end_time.isoformat()
                },
                "final_counts": {
                    "total_certifications": len(all_certifications),
                    "total_documents": len(all_documents),
                    "total_sources": len(all_sources),
                    "total_precedents": len(cbp.get("precedents", [])) if cbp else 0,
                    "hybrid_certifications_added": hybrid_certifications,
                    "hybrid_documents_added": hybrid_documents,
                    "hybrid_sources_added": hybrid_sources
                },
                "phase_2_4_counts": phase_2_4_counts,
                "cbp_precedents_info": {
                    "success": not cbp.get("error") if cbp else False,
                    "precedents_count": len(cbp.get("precedents", [])) if cbp else 0,
                    "error_message": cbp.get("error_message") if cbp and cbp.get("error") else None
                },
                "data_sources_summary": {
                    "web_scraping_results": len(all_certifications) - hybrid_certifications,
                    "hybrid_api_results": hybrid_certifications + hybrid_documents + hybrid_sources,
                    "cbp_precedents": len(cbp.get("precedents", [])) if cbp else 0
                }
            }
        }
        state["detailed_metadata"] = state.get("detailed_metadata", {})
        state["detailed_metadata"].update(consolidation_metadata)

        print(f"ğŸ“‹ [METADATA] ê²°ê³¼ í†µí•© ìƒì„¸ ì •ë³´ ì €ì¥ë¨ - ì´ ì‹œê°„: {consolidation_time:.0f}ms, ìµœì¢… ê²°ê³¼: ì¸ì¦ {len(all_certifications)}ê°œ, ì„œë¥˜ {len(all_documents)}ê°œ")

        # Citations ì¶”ì¶œ (ë°±ì—”ë“œ APIì—ì„œ ì œê³µ)
        citations = []
        if hybrid and not hybrid.get("error"):
            citations = hybrid.get("citations", [])
            print(f"  ğŸ“š Citations ì¶”ì¶œ: {len(citations)}ê°œ")
        
        # LLM ìš”ì•½ ìƒì„±
        llm_summary = None
        try:
            from app.services.requirements.llm_summary_service import LlmSummaryService
            llm_service = LlmSummaryService()
            
            # í†µí•©ëœ ë°ì´í„°ë¥¼ ë¬¸ì„œ í˜•íƒœë¡œ ë³€í™˜
            raw_documents = []
            
            # ì¸ì¦ìš”ê±´ì„ ë¬¸ì„œë¡œ ë³€í™˜
            for cert in all_certifications:
                raw_documents.append({
                    "title": cert.get("name", "Unknown"),
                    "content": cert.get("description", ""),
                    "url": cert.get("source_url", ""),
                    "agency": cert.get("agency", "")
                })
            
            # í•„ìš”ì„œë¥˜ë¥¼ ë¬¸ì„œë¡œ ë³€í™˜
            for doc in all_documents:
                raw_documents.append({
                    "title": doc.get("name", "Unknown"),
                    "content": doc.get("description", ""),
                    "url": doc.get("source_url", ""),
                    "agency": doc.get("agency", "")
                })
            
            # Citationsë„ ì¶”ê°€
            for citation in citations:
                raw_documents.append({
                    "title": citation.get("title", ""),
                    "content": citation.get("snippet", ""),
                    "url": citation.get("url", ""),
                    "agency": citation.get("agency", "")
                })
            
            print(f"  ğŸ¤– LLM ìš”ì•½ ìƒì„± ì¤‘... (ë¬¸ì„œ {len(raw_documents)}ê°œ)")
            
            # summarize_regulations ë©”ì„œë“œ í˜¸ì¶œ
            summary_result = await llm_service.summarize_regulations(
                hs_code=request.hs_code if request else "unknown",
                product_name=request.product_name if request else "unknown",
                raw_documents=raw_documents
            )
            
            # SummaryResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            if summary_result:
                llm_summary = {
                    "critical_requirements": summary_result.critical_requirements,
                    "required_documents": summary_result.required_documents,
                    "compliance_steps": summary_result.compliance_steps,
                    "estimated_costs": summary_result.estimated_costs,
                    "timeline": summary_result.timeline,
                    "risk_factors": summary_result.risk_factors,
                    "recommendations": summary_result.recommendations,
                    "confidence_score": summary_result.confidence_score,
                    "model_used": summary_result.model_used,
                    "tokens_used": summary_result.tokens_used,
                    "cost": summary_result.cost
                }
                print(f"  âœ… LLM ìš”ì•½ ìƒì„± ì™„ë£Œ - ì‹ ë¢°ë„: {summary_result.confidence_score:.2f}")
            
        except Exception as e:
            print(f"  âš ï¸ LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            llm_summary = None
        
        # ========================================
        # ğŸš€ Phase 1-4 ì „ë¬¸ ì„œë¹„ìŠ¤ í˜¸ì¶œ (ë³‘ë ¬ ì‹¤í–‰)
        # ========================================
        print(f"\nğŸš€ [PHASE 1-4] ì „ë¬¸ ë¶„ì„ ì„œë¹„ìŠ¤ ì‹¤í–‰ ì‹œì‘")
        phase_start = datetime.now()
        
        phase_1_result = None  # ì„¸ë¶€ ê·œì •
        phase_2_result = None  # ê²€ì‚¬ ì ˆì°¨
        phase_3_result = None  # ì²˜ë²Œ ë²Œê¸ˆ
        phase_4_result = None  # ìœ íš¨ê¸°ê°„
        detailed_regs_result = None  # Phase 1 ê²°ê³¼
        cross_validation_result = None  # êµì°¨ ê²€ì¦
        
        try:
            # ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„± (íŒë¡€ ê²°ê³¼ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬)
            tasks = []
            
            if request:
                # 1ë‹¨ê³„: ì„¸ë¶€ ê·œì • ì¶”ì¶œ (ë†ì•½ ì”ë¥˜ëŸ‰, í™”í•™ì„±ë¶„ ì œí•œ ë“±)
                task1 = asyncio.create_task(
                    self.detailed_regulations.analyze(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        product_description=request.product_description or ""
                    )
                )
                tasks.append(("detailed_regulations", task1))
                
                # 2ë‹¨ê³„: ê²€ì‚¬ ì ˆì°¨ ë¶„ì„
                task2 = asyncio.create_task(
                    self.testing_procedures.analyze(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        product_description=request.product_description or ""
                    )
                )
                tasks.append(("testing_procedures", task2))
                
                # 3ë‹¨ê³„: ì²˜ë²Œ ë²Œê¸ˆ ë¶„ì„
                task3 = asyncio.create_task(
                    self.penalties.analyze(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        product_description=request.product_description or ""
                    )
                )
                tasks.append(("penalties", task3))
                
                # 4ë‹¨ê³„: ìœ íš¨ê¸°ê°„ ë¶„ì„
                task4 = asyncio.create_task(
                    self.validity.analyze(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        product_description=request.product_description or ""
                    )
                )
                tasks.append(("validity", task4))
            
            # ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            if tasks:
                print(f"  ğŸ”„ {len(tasks)}ê°œ ë¶„ì„ íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰ ì¤‘...")
                completed_tasks = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
                
                # ê²°ê³¼ í• ë‹¹
                for i, (name, _) in enumerate(tasks):
                    result = completed_tasks[i]
                    if isinstance(result, Exception):
                        print(f"  âŒ {name} ë¶„ì„ ì‹¤íŒ¨: {result}")
                    else:
                        print(f"  âœ… {name} ë¶„ì„ ì™„ë£Œ - {len(result.get('sources', []))}ê°œ ì¶œì²˜")
                        if name == "detailed_regulations":
                            detailed_regs_result = result
                        elif name == "testing_procedures":
                            phase_2_result = result
                        elif name == "penalties":
                            phase_3_result = result
                        elif name == "validity":
                            phase_4_result = result
            
            # êµì°¨ ê²€ì¦ (ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘ í›„ ì‹¤í–‰)
            if llm_summary and request:
                print(f"  ğŸ” êµì°¨ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
                try:
                    cross_validation_result = await self.cross_validation.validate_requirements(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        llm_summary=llm_summary,
                        phase_results={
                            "detailed_regulations": detailed_regs_result,
                            "testing_procedures": phase_2_result,
                            "penalties": phase_3_result,
                            "validity": phase_4_result
                        }
                    )
                    print(f"  âœ… êµì°¨ ê²€ì¦ ì™„ë£Œ - ê²€ì¦ì ìˆ˜: {cross_validation_result.validation_score:.2f}, ì¶©ëŒ: {len(cross_validation_result.conflicts_found)}ê±´")
                except Exception as e:
                    print(f"  âš ï¸ êµì°¨ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    cross_validation_result = None
            
            # ğŸ’¾ íŒë¡€ ê²€ì¦ ì „ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©)
            if request:
                try:
                    import json
                    from pathlib import Path
                    
                    # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ì§ë ¬í™”
                    def safe_serialize(obj):
                        """ê°ì²´ë¥¼ ì•ˆì „í•˜ê²Œ dictë¡œ ë³€í™˜"""
                        if obj is None:
                            return None
                        elif hasattr(obj, '__dict__'):
                            return {k: safe_serialize(v) for k, v in obj.__dict__.items()}
                        elif isinstance(obj, dict):
                            return {k: safe_serialize(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [safe_serialize(item) for item in obj]
                        elif isinstance(obj, (str, int, float, bool)):
                            return obj
                        else:
                            return str(obj)
                    
                    intermediate_data = {
                        "timestamp": datetime.now().isoformat(),
                        "hs_code": request.hs_code,
                        "product_name": request.product_name,
                        "llm_summary": safe_serialize(llm_summary),
                        "phase_1_detailed_regulations": safe_serialize(detailed_regs_result),
                        "phase_2_testing_procedures": safe_serialize(phase_2_result),
                        "phase_3_penalties": safe_serialize(phase_3_result),
                        "phase_4_validity": safe_serialize(phase_4_result),
                        "cross_validation": safe_serialize(cross_validation_result),
                        "certifications": safe_serialize(all_certifications),
                        "documents": safe_serialize(all_documents),
                        "sources": safe_serialize(all_sources)
                    }
                    
                    # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                    safe_filename = request.product_name.replace(" ", "_").replace("/", "_")[:50]
                    output_dir = Path("requirements_intermediate")
                    output_dir.mkdir(exist_ok=True)
                    
                    output_file = output_dir / f"intermediate_{safe_filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(intermediate_data, f, indent=2, ensure_ascii=False, default=str)
                    
                    print(f"  ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
                    
                except Exception as e:
                    print(f"  âš ï¸ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            
            # ğŸ†• íŒë¡€ ê¸°ë°˜ ê²€ì¦ (FAISS DB ì‚¬ìš©) - êµì°¨ ê²€ì¦ê³¼ ê°™ì€ ìœ„ì¹˜ì—ì„œ ì‹¤í–‰
            precedent_validation_result = None
            if request and precedents_list:
                print(f"  ğŸ” íŒë¡€ ê¸°ë°˜ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
                try:
                    from app.services.requirements.precedent_validation_service import get_precedent_validation_service
                    precedent_validator = get_precedent_validation_service()
                    
                    precedent_validation_result = await precedent_validator.validate_requirements(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        our_requirements={
                            "certifications": all_certifications,
                            "documents": all_documents
                        },
                        precedents=precedents_list
                    )
                    
                    print(f"  âœ… íŒë¡€ ê²€ì¦ ì™„ë£Œ - ì ìˆ˜: {precedent_validation_result.validation_score:.2f}, íŒì •: {precedent_validation_result.verdict['status']}")
                    print(f"    ğŸ“Š ì¼ì¹˜: {len(precedent_validation_result.matched_requirements)}ê°œ, ëˆ„ë½: {len(precedent_validation_result.missing_requirements)}ê°œ, Red Flags: {len(precedent_validation_result.red_flags)}ê°œ")
                    
                except Exception as e:
                    print(f"  âš ï¸ íŒë¡€ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                    precedent_validation_result = None
            
            # ğŸš€ LLM ìš”ì•½ì— Phase 1-4 ê²°ê³¼ í¬í•¨í•˜ì—¬ ì¬ìƒì„±
            if request and (detailed_regs_result or phase_2_result or phase_3_result or phase_4_result):
                print(f"  ğŸ”„ Phase 1-4 ê²°ê³¼ë¥¼ í¬í•¨í•œ LLM ìš”ì•½ ì¬ìƒì„±...")
                try:
                    # Phase 1-4 ê²°ê³¼ë¥¼ raw_documentsì— ì¶”ê°€
                    phase_documents = []
                    
                    if detailed_regs_result:
                        phase_documents.append({
                            "title": f"Phase 1: ì„¸ë¶€ ê·œì • ({request.hs_code})",
                            "content": f"ìƒì„¸ ê·œì • ë¶„ì„ ê²°ê³¼: {detailed_regs_result.get('summary', '')}",
                            "url": "phase_1_result",
                            "source": "detailed_regulations_service"
                        })
                    
                    if phase_2_result:
                        phase_documents.append({
                            "title": f"Phase 2: ê²€ì‚¬ ì ˆì°¨ ({request.hs_code})",
                            "content": f"ê²€ì‚¬ ì ˆì°¨ ë¶„ì„ ê²°ê³¼: {phase_2_result.get('summary', '')}",
                            "url": "phase_2_result",
                            "source": "testing_procedures_service"
                        })
                    
                    if phase_3_result:
                        phase_documents.append({
                            "title": f"Phase 3: ì²˜ë²Œ ì •ë³´ ({request.hs_code})",
                            "content": f"ì²˜ë²Œ ì •ë³´ ë¶„ì„ ê²°ê³¼: {phase_3_result.get('summary', '')}",
                            "url": "phase_3_result",
                            "source": "penalties_service"
                        })
                    
                    if phase_4_result:
                        phase_documents.append({
                            "title": f"Phase 4: ìœ íš¨ê¸°ê°„ ({request.hs_code})",
                            "content": f"ìœ íš¨ê¸°ê°„ ë¶„ì„ ê²°ê³¼: {phase_4_result.get('summary', '')}",
                            "url": "phase_4_result",
                            "source": "validity_service"
                        })
                    
                    # ê¸°ì¡´ ë¬¸ì„œì™€ Phase ê²°ê³¼ í•©ì¹˜ê¸°
                    enhanced_documents = raw_documents + phase_documents
                    
                    # LLM ìš”ì•½ ì¬ìƒì„± (raw_summaryë¡œ ë°›ì•„ì„œ í™•ì¥ í•„ë“œ í¬í•¨)
                    enhanced_summary_raw = await llm_service._call_gpt_summary(
                        hs_code=request.hs_code,
                        product_name=request.product_name,
                        document_texts=llm_service._extract_document_texts(enhanced_documents)
                    )
                    
                    if enhanced_summary_raw:
                        # ê¸°ì¡´ ìš”ì•½ì„ Phase 1-4 ê²°ê³¼ë¡œ í™•ì¥ (ëª¨ë“  GPT í•„ë“œ í¬í•¨)
                        llm_summary = {
                            # ê¸°ë³¸ í•„ë“œ
                            "critical_requirements": enhanced_summary_raw.get("critical_requirements", []),
                            "required_documents": enhanced_summary_raw.get("required_documents", []),
                            "compliance_steps": enhanced_summary_raw.get("compliance_steps", []),
                            "estimated_costs": enhanced_summary_raw.get("estimated_costs", {}),
                            "timeline": enhanced_summary_raw.get("timeline", "ì •ë³´ ì—†ìŒ"),
                            "risk_factors": enhanced_summary_raw.get("risk_factors", []),
                            "recommendations": enhanced_summary_raw.get("recommendations", []),
                            "confidence_score": enhanced_summary_raw.get("confidence_score", 0.0),
                            "model_used": "gpt-4o-mini",
                            "tokens_used": enhanced_summary_raw.get("tokens_used", 0),
                            "cost": enhanced_summary_raw.get("cost", 0.0),
                            # í™•ì¥ í•„ë“œ (ìƒˆë¡œ ì¶”ê°€ëœ ê²ƒë“¤)
                            "execution_checklist": enhanced_summary_raw.get("execution_checklist"),
                            "cost_breakdown": enhanced_summary_raw.get("cost_breakdown"),
                            "risk_matrix": enhanced_summary_raw.get("risk_matrix"),
                            "compliance_score": enhanced_summary_raw.get("compliance_score"),
                            "market_access": enhanced_summary_raw.get("market_access"),
                            # Phase 1-4 ê²°ê³¼ ì¶”ê°€
                            "phase_1_detailed_regulations": detailed_regs_result,
                            "phase_2_testing_procedures": phase_2_result,
                            "phase_3_penalties": phase_3_result,
                            "phase_4_validity": phase_4_result,
                            "cross_validation": cross_validation_result
                        }
                        print(f"  âœ… Phase 1-4 í¬í•¨ LLM ìš”ì•½ ì¬ìƒì„± ì™„ë£Œ (í™•ì¥ í•„ë“œ í¬í•¨)")
                    
                except Exception as e:
                    print(f"  âš ï¸ Phase 1-4 í¬í•¨ LLM ìš”ì•½ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ìš”ì•½ ìœ ì§€
            
        except Exception as e:
            print(f"  âŒ Phase 2-4 ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        phase_end = datetime.now()
        phase_duration = (phase_end - phase_start).total_seconds() * 1000
        print(f"âœ… [PHASE 2-4] ì „ë¬¸ ë¶„ì„ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {phase_duration:.0f}ms")
        
        # Phase 2-4 ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        phase_metadata = {
            "phase_2_4_analysis": {
                "processing_time_ms": phase_duration,
                "detailed_regulations": {
                    "success": detailed_regs_result is not None,
                    "agencies": detailed_regs_result.get("agencies", []) if detailed_regs_result else [],
                    "sources_count": len(detailed_regs_result.get("sources", [])) if detailed_regs_result else 0
                },
                "testing_procedures": {
                    "success": phase_2_result is not None,
                    "inspection_cycle": phase_2_result.get("inspection_cycle") if phase_2_result else "unknown",
                    "estimated_cost": phase_2_result.get("estimates", {}).get("estimated_cost_band") if phase_2_result else "unknown"
                },
                "penalties": {
                    "success": phase_3_result is not None,
                    "fine_range": phase_3_result.get("fine_range", {}) if phase_3_result else {}
                },
                "validity": {
                    "success": phase_4_result is not None,
                    "validity_period": phase_4_result.get("validity") if phase_4_result else "unknown"
                },
                "cross_validation": {
                    "success": cross_validation_result is not None,
                    "validation_score": cross_validation_result.validation_score if cross_validation_result else 0.0,
                    "conflicts_found": len(cross_validation_result.conflicts_found) if cross_validation_result else 0
                }
            }
        }
        state["detailed_metadata"] = state.get("detailed_metadata", {})
        state["detailed_metadata"].update(phase_metadata)
        
        # Phase 1-4 ê²°ê³¼ ë””ë²„ê¹… ë¡œê·¸
        print(f"  ğŸ” [DEBUG] Phase ê²°ê³¼ ìƒíƒœ:")
        print(f"    ğŸ“‹ Phase 1 (detailed_regulations): {'âœ…' if detailed_regs_result else 'âŒ'}")
        print(f"    ğŸ§ª Phase 2 (testing_procedures): {'âœ…' if phase_2_result else 'âŒ'}")
        print(f"    âš–ï¸ Phase 3 (penalties): {'âœ…' if phase_3_result else 'âŒ'}")
        print(f"    â° Phase 4 (validity): {'âœ…' if phase_4_result else 'âŒ'}")
        print(f"    ğŸ” êµì°¨ ê²€ì¦ (cross_validation): {'âœ…' if cross_validation_result else 'âŒ'}")
        
        # ğŸ¯ í†µí•© ì‹ ë¢°ë„ ê³„ì‚° (íŒë¡€ ê²€ì¦ + êµì°¨ ê²€ì¦ + ì¶œì²˜ ì‹ ë¢°ë„)
        overall_confidence = None
        if precedent_validation_result or cross_validation_result:
            print(f"  ğŸ“Š í†µí•© ì‹ ë¢°ë„ ê³„ì‚° ì¤‘...")
            try:
                # ì¶œì²˜ ì‹ ë¢°ë„ ê³„ì‚°
                official_sources_count = len([s for s in all_sources if '.gov' in str(s.get('url', ''))])
                source_reliability_score = official_sources_count / len(all_sources) if all_sources else 0.5
                
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                precedent_score = precedent_validation_result.validation_score if precedent_validation_result else 0.5
                cross_score = cross_validation_result.validation_score if cross_validation_result else 0.5
                
                overall_score = (precedent_score * 0.4) + (cross_score * 0.3) + (source_reliability_score * 0.3)
                
                # ëª¨ë“  Red Flags ìˆ˜ì§‘
                all_red_flags = []
                if precedent_validation_result:
                    all_red_flags.extend(precedent_validation_result.red_flags)
                if cross_validation_result:
                    for conflict in cross_validation_result.conflicts_found:
                        all_red_flags.append({
                            "type": "regulation_conflict",
                            "severity": conflict.severity,
                            "description": conflict.conflict_description,
                            "agencies": conflict.conflicting_agencies
                        })
                
                # ìµœì¢… íŒì •
                if overall_score >= 0.85 and len(all_red_flags) == 0:
                    verdict_status = "RELIABLE"
                    confidence_level = "HIGH"
                elif overall_score >= 0.7:
                    verdict_status = "NEEDS_REVIEW"
                    confidence_level = "MEDIUM"
                else:
                    verdict_status = "UNRELIABLE"
                    confidence_level = "LOW"
                
                overall_confidence = {
                    "overall_score": overall_score,
                    "confidence_level": confidence_level,
                    "breakdown": {
                        "precedent_validation": {"score": precedent_score, "weight": 0.4},
                        "cross_validation": {"score": cross_score, "weight": 0.3},
                        "source_reliability": {"score": source_reliability_score, "weight": 0.3}
                    },
                    "red_flags": all_red_flags,
                    "red_flags_count": len(all_red_flags),
                    "verdict": {
                        "status": verdict_status,
                        "confidence": confidence_level,
                        "reason": f"íŒë¡€ ê²€ì¦ {precedent_score:.0%}, êµì°¨ ê²€ì¦ {cross_score:.0%}, ì¶œì²˜ ì‹ ë¢°ë„ {source_reliability_score:.0%}",
                        "action": "ìˆ˜ì… ì§„í–‰ ê°€ëŠ¥" if verdict_status == "RELIABLE" else 
                                 "ì¶”ê°€ í™•ì¸ í•„ìš”" if verdict_status == "NEEDS_REVIEW" else 
                                 "ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥"
                    }
                }
                
                print(f"  âœ… í†µí•© ì‹ ë¢°ë„: {overall_score:.2f} ({confidence_level}) - {verdict_status}")
                
            except Exception as e:
                print(f"  âš ï¸ í†µí•© ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                overall_confidence = None
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€ + citations + llm_summary + Phase 1-4 ê²°ê³¼ + íŒë¡€ ê²€ì¦ ì¶”ê°€)
        state["consolidated_results"] = {
            "certifications": all_certifications,
            "documents": all_documents,
            "sources": all_sources,
            "llm_summary": llm_summary,
            "precedents": cbp.get("precedents", []) if cbp else [],
            "citations": citations,
            # Phase 1-4 ì „ë¬¸ ë¶„ì„ ê²°ê³¼ ì¶”ê°€ (Phase 1ë„ í¬í•¨!)
            "detailed_regulations": detailed_regs_result,  # Phase 1
            "testing_procedures": phase_2_result,         # Phase 2
            "penalties": phase_3_result,                  # Phase 3
            "validity": phase_4_result,                   # Phase 4
            "cross_validation": cross_validation_result,  # êµì°¨ ê²€ì¦
            # ğŸ†• íŒë¡€ ê¸°ë°˜ ê²€ì¦ ê²°ê³¼
            "precedent_validation": {
                "validation_score": precedent_validation_result.validation_score if precedent_validation_result else None,
                "precedents_analyzed": precedent_validation_result.precedents_analyzed if precedent_validation_result else 0,
                "precedents_source": precedent_validation_result.precedents_source if precedent_validation_result else "none",
                "matched_requirements": precedent_validation_result.matched_requirements if precedent_validation_result else [],
                "missing_requirements": precedent_validation_result.missing_requirements if precedent_validation_result else [],
                "extra_requirements": precedent_validation_result.extra_requirements if precedent_validation_result else [],
                "red_flags": precedent_validation_result.red_flags if precedent_validation_result else [],
                "verdict": precedent_validation_result.verdict if precedent_validation_result else {}
            } if precedent_validation_result else None,
            # ğŸ†• í†µí•© ì‹ ë¢°ë„
            "overall_confidence": overall_confidence,
            # ğŸ†• ê²€ì¦ ìš”ì•½ (Frontendìš© ê°„ë‹¨ ë²„ì „)
            "verification_summary": {
                "verdict": overall_confidence['verdict']['status'] if overall_confidence else "UNKNOWN",
                "confidence_score": overall_confidence['overall_score'] if overall_confidence else 0.5,
                "confidence_level": overall_confidence['confidence_level'] if overall_confidence else "MEDIUM",
                "red_flags_count": len(overall_confidence['red_flags']) if overall_confidence else 0,
                "action_recommendation": overall_confidence['verdict']['action'] if overall_confidence else "ë¶„ì„ ê²°ê³¼ í™•ì¸ í•„ìš”"
            } if overall_confidence else None
        }
        state["precedents_meta"] = cbp
        state["next_action"] = "complete"
        
        # ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ (íŒë¡€ ê²€ì¦ í¬í•¨)
        if request:
            try:
                import json
                from pathlib import Path
                
                # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ì§ë ¬í™”
                def safe_serialize(obj):
                    """ê°ì²´ë¥¼ ì•ˆì „í•˜ê²Œ dictë¡œ ë³€í™˜"""
                    if obj is None:
                        return None
                    elif hasattr(obj, '__dict__'):
                        return {k: safe_serialize(v) for k, v in obj.__dict__.items()}
                    elif isinstance(obj, dict):
                        return {k: safe_serialize(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [safe_serialize(item) for item in obj]
                    elif isinstance(obj, (str, int, float, bool)):
                        return obj
                    else:
                        return str(obj)
                
                # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                safe_filename = request.product_name.replace(" ", "_").replace("/", "_")[:50]
                output_dir = Path("requirements_final")
                output_dir.mkdir(exist_ok=True)
                
                output_file = output_dir / f"final_{safe_filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                
                # ì•ˆì „í•˜ê²Œ ì§ë ¬í™”
                final_data = safe_serialize(state["consolidated_results"])
                
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(final_data, f, indent=2, ensure_ascii=False, default=str)
                
                print(f"  ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
                
            except Exception as e:
                print(f"  âš ï¸ ìµœì¢… ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                import traceback
                traceback.print_exc()
        
        return state
