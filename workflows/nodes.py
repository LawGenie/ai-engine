"""
LangGraph Nodes for Requirements Analysis
ê° ë‹¨ê³„ë³„ë¡œ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œë“¤
"""

from typing import Dict, Any, List
from app.services.requirements.tavily_search import TavilySearchService
from app.services.requirements.web_scraper import WebScraper
from app.models.requirement_models import RequirementAnalysisRequest


class RequirementsNodes:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ LangGraph ë…¸ë“œë“¤"""
    
    def __init__(self):
        self.search_service = TavilySearchService()
        self.web_scraper = WebScraper()
    
    async def search_agency_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        request = state["request"]
        hs_code = request.hs_code
        product_name = request.product_name
        
        print(f"\nğŸ” [NODE] ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {product_name}")
        
        # ê¸°ë³¸ URL í´ë°± (TavilySearch ì‹¤íŒ¨ ì‹œ ì‚¬ìš©) - ë” ê°„ë‹¨í•œ URL ì‚¬ìš©
        default_urls = {
            "FDA": "https://www.fda.gov",
            "FCC": "https://www.fcc.gov",
            "CBP": "https://www.cbp.gov"
        }
        
        # ê° ê¸°ê´€ë³„ ê²€ìƒ‰ ì¿¼ë¦¬
        search_queries = {
            "FDA": f"FDA import requirements {product_name} HS {hs_code}",
            "FCC": f"FCC device authorization requirements {product_name} HS {hs_code}",
            "CBP": f"CBP import documentation requirements HS {hs_code} {product_name}",
        }
        
        search_results = {}
        
        for agency, query in search_queries.items():
            print(f"\n  ğŸ“¡ {agency} ê²€ìƒ‰ ì¤‘...")
            print(f"    ì¿¼ë¦¬: {query}")
            
            # TavilySearch ì‹œë„
            results = await self.search_service.search(query, max_results=5)
            print(f"    ğŸ“Š TavilySearch ê²°ê³¼: {len(results)}ê°œ")
            
            if not results:
                print(f"    ğŸ’¡ íŒ: TAVILY_API_KEYë¥¼ ì„¤ì •í•˜ë©´ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            chosen_url = None
            
            if results:
                # ê° ê²°ê³¼ ìƒì„¸ ì¶œë ¥
                for i, result in enumerate(results, 1):
                    title = result.get('title', 'No title')
                    url = result.get('url', 'No URL')
                    print(f"      {i}. {title}")
                    print(f"         URL: {url}")
                
                # ê³µì‹ ë„ë©”ì¸ ìš°ì„  ì„ íƒ
                preferred_domains = {
                    "FDA": ["fda.gov"],
                    "FCC": ["fcc.gov"],
                    "CBP": ["cbp.gov"],
                }.get(agency, [])
                
                for result in results:
                    url = result.get("url", "")
                    if any(domain in url for domain in preferred_domains):
                        chosen_url = url
                        print(f"    âœ… {agency} ê³µì‹ ë„ë©”ì¸ ì„ íƒ: {url}")
                        break
                
                if not chosen_url:
                    chosen_url = results[0].get("url")
                    print(f"    ğŸ”„ {agency} ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©: {chosen_url}")
            else:
                # TavilySearch ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ URL ì‚¬ìš©
                chosen_url = default_urls.get(agency)
                print(f"    ğŸ”„ {agency} TavilySearch ì‹¤íŒ¨, ê¸°ë³¸ URL ì‚¬ìš©: {chosen_url}")
            
            search_results[agency] = {
                "url": chosen_url,
                "all_results": results,
                "query": query,
                "is_fallback": not results  # í´ë°± ì‚¬ìš© ì—¬ë¶€ í‘œì‹œ
            }
        
        print(f"\nğŸ“‹ [NODE] ê²€ìƒ‰ ì™„ë£Œ - {len([r for r in search_results.values() if r['url']])}ê°œ URL ë°œê²¬")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["search_results"] = search_results
        state["next_action"] = "scrape_documents"
        return state
    
    async def scrape_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ë…¸ë“œ"""
        search_results = state["search_results"]
        request = state["request"]
        hs_code = request.hs_code
        
        print(f"\nğŸ” [NODE] ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        scraped_data = {}
        
        for agency, search_data in search_results.items():
            if not search_data["url"]:
                print(f"  âŒ {agency}: ìŠ¤í¬ë˜í•‘í•  URL ì—†ìŒ")
                continue
                
            print(f"\n  ğŸ“„ {agency} ìŠ¤í¬ë˜í•‘ ì¤‘...")
            print(f"    URL: {search_data['url']}")
            
            try:
                if agency == "FDA":
                    result = await self.web_scraper.scrape_fda_requirements(hs_code, search_data["url"])
                elif agency == "FCC":
                    result = await self.web_scraper.scrape_fcc_requirements(hs_code, search_data["url"])
                elif agency == "CBP":
                    result = await self.web_scraper.scrape_cbp_requirements(hs_code, search_data["url"])
                else:
                    continue
                
                # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                certs = result.get("certifications", [])
                docs = result.get("documents", [])
                
                print(f"    âœ… {agency} ìŠ¤í¬ë˜í•‘ ì„±ê³µ:")
                print(f"      ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ")
                for cert in certs:
                    print(f"        â€¢ {cert.get('name', 'Unknown')} ({cert.get('agency', 'Unknown')})")
                
                print(f"      ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ")
                for doc in docs:
                    print(f"        â€¢ {doc.get('name', 'Unknown')}")
                
                scraped_data[agency] = result
                
            except Exception as e:
                print(f"    âŒ {agency} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                scraped_data[agency] = {
                    "agency": agency,
                    "error": str(e),
                    "certifications": [],
                    "documents": []
                }
        
        print(f"\nğŸ“‹ [NODE] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ - {len(scraped_data)}ê°œ ê¸°ê´€ ì²˜ë¦¬")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["scraped_data"] = scraped_data
        state["next_action"] = "consolidate_results"
        return state
    
    async def consolidate_results(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
        scraped_data = state["scraped_data"]
        
        print(f"\nğŸ” [NODE] ê²°ê³¼ í†µí•© ì‹œì‘")
        
        all_certifications = []
        all_documents = []
        all_sources = []
        
        for agency, data in scraped_data.items():
            if "error" in data:
                print(f"  âŒ {agency}: ì˜¤ë¥˜ë¡œ ì¸í•´ ì œì™¸")
                continue
                
            print(f"  ğŸ“Š {agency} ë°ì´í„° í†µí•©:")
            
            # ì¸ì¦ìš”ê±´ í†µí•©
            certs = data.get("certifications", [])
            all_certifications.extend(certs)
            print(f"    ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ ì¶”ê°€")
            
            # í•„ìš”ì„œë¥˜ í†µí•©
            docs = data.get("documents", [])
            all_documents.extend(docs)
            print(f"    ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ ì¶”ê°€")
            
            # ì¶œì²˜ í†µí•©
            sources = data.get("sources", [])
            all_sources.extend(sources)
            print(f"    ğŸ“š ì¶œì²˜: {len(sources)}ê°œ ì¶”ê°€")
        
        print(f"\nğŸ“‹ [NODE] í†µí•© ì™„ë£Œ:")
        print(f"  ğŸ“‹ ì´ ì¸ì¦ìš”ê±´: {len(all_certifications)}ê°œ")
        print(f"  ğŸ“„ ì´ í•„ìš”ì„œë¥˜: {len(all_documents)}ê°œ")
        print(f"  ğŸ“š ì´ ì¶œì²˜: {len(all_sources)}ê°œ")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["consolidated_results"] = {
            "certifications": all_certifications,
            "documents": all_documents,
            "sources": all_sources
        }
        state["next_action"] = "complete"
        return state
