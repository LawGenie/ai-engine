"""
LangGraph Nodes for Requirements Analysis
ê° ë‹¨ê³„ë³„ë¡œ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œë“¤
"""

from typing import Dict, Any, List
from .tools import RequirementsTools
from app.services.requirements.keyword_extractor import KeywordExtractor, HfKeywordExtractor, OpenAiKeywordExtractor
from app.services.requirements.tavily_search import TavilySearchService
from app.services.requirements.web_scraper import WebScraper
from app.models.requirement_models import RequirementAnalysisRequest


class RequirementsNodes:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ LangGraph ë…¸ë“œë“¤"""
    
    def __init__(self):
        # RequirementsToolsì—ì„œ í”„ë¡œë°”ì´ë”ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
        self.tools = RequirementsTools()
        self.web_scraper = WebScraper()
        self.keyword_extractor = None
        self.hf_extractor = None
        self.openai_extractor = None

    async def extract_core_keywords(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒí’ˆëª…/ì„¤ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±).
        - ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë¶„ì ˆ
        - ë¶ˆìš©ì–´ ì œê±°
        - ê¸¸ì´ 3 ì´ìƒ ë‹¨ì–´ ìš°ì„ , ìƒìœ„ 3ê°œ ë°˜í™˜
        - í•œê¸€ì¼ ê²½ìš° ê°„ë‹¨ ë§¤í•‘ ì‹œë„
        """
        request = state["request"]
        name = (request.product_name or "").strip()
        desc = (request.product_description or "").strip()
        # HF ì¶”ì¶œê¸° ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±
        if self.hf_extractor is None:
            try:
                self.hf_extractor = HfKeywordExtractor()
            except Exception:
                self.hf_extractor = None
        if self.keyword_extractor is None:
            self.keyword_extractor = KeywordExtractor()
        if self.openai_extractor is None:
            try:
                self.openai_extractor = OpenAiKeywordExtractor()
            except Exception:
                self.openai_extractor = None

        core_keywords = []
        try:
            # OpenAI ìš°ì„ (í”Œë˜ê·¸ í™œì„± ì‹œ) â†’ HF â†’ íœ´ë¦¬ìŠ¤í‹±
            if self.openai_extractor:
                core_keywords = self.openai_extractor.extract(name, desc, top_k=3)
        except Exception:
            core_keywords = []
        try:
            if self.hf_extractor:
                core_keywords = self.hf_extractor.extract(name, desc, top_k=3)
        except Exception:
            core_keywords = []
        if not core_keywords:
            core_keywords = self.keyword_extractor.extract(name, desc, top_k=3)
        
        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì‹œë„í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
        state["core_keywords"] = core_keywords
        state["keyword_strategies"] = [
            {"strategy": "top1", "keywords": core_keywords[:1]},
            {"strategy": "top2", "keywords": core_keywords[:2]},
            {"strategy": "top3", "keywords": core_keywords[:3]}
        ]
        
        print(f"\nğŸ” [NODE] í•µì‹¬ í‚¤ì›Œë“œ: {core_keywords}")
        print(f"ğŸ” [NODE] í‚¤ì›Œë“œ ì „ëµ: {[s['strategy'] for s in state['keyword_strategies']]}")
        state["next_action"] = "call_hybrid_api"
        return state
    
    async def search_agency_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        request = state["request"]
        hs_code = request.hs_code
        product_name = request.product_name
        keywords = state.get("core_keywords") or []
        keyword_strategies = state.get("keyword_strategies", [])
        
        # í‚¤ì›Œë“œ ì „ëµì„ ë‹¨ê³„ì ìœ¼ë¡œ ì‹œë„ (top1 â†’ top2 â†’ top3)
        query_terms = []
        for strategy in keyword_strategies:
            if strategy["keywords"]:
                query_terms.append(" ".join(strategy["keywords"]))
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒí’ˆëª… ì‚¬ìš©
        if not query_terms:
            query_terms = [product_name]
        
        query_term = query_terms[0]  # ì²« ë²ˆì§¸ ì „ëµ ì‚¬ìš©
        
        print(f"\nğŸ” [NODE] ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {product_name}")
        
        # ê¸°ë³¸ URL í´ë°± (TavilySearch ì‹¤íŒ¨ ì‹œ ì‚¬ìš©) - 9ê°œ ê¸°ê´€ ëª¨ë‘
        default_urls = {
            "FDA": "https://www.fda.gov",
            "FCC": "https://www.fcc.gov",
            "CBP": "https://www.cbp.gov",
            "USDA": "https://www.usda.gov",
            "EPA": "https://www.epa.gov",
            "CPSC": "https://www.cpsc.gov",
            "KCS": "https://www.customs.go.kr",
            "MFDS": "https://www.mfds.go.kr",
            "MOTIE": "https://www.motie.go.kr"
        }
        
        # HSì½”ë“œ 8ìë¦¬ì™€ 6ìë¦¬ ì¶”ì¶œ
        hs_code_8digit = hs_code
        hs_code_6digit = ".".join(hs_code.split(".")[:2]) if "." in hs_code else hs_code
        
        print(f"  ğŸ“‹ 8ìë¦¬ HSì½”ë“œ: {hs_code_8digit}")
        print(f"  ğŸ“‹ 6ìë¦¬ HSì½”ë“œ: {hs_code_6digit}")
        
        # ê° ê¸°ê´€ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ (8ìë¦¬ì™€ 6ìë¦¬ ëª¨ë‘)
        search_queries = {}
        
        # 8ìë¦¬ HSì½”ë“œ ê²€ìƒ‰ (ì •í™•)
        search_queries.update({
            f"FDA_8digit": f"site:fda.gov import requirements {query_term} HS {hs_code_8digit}",
            f"FCC_8digit": f"site:fcc.gov device authorization requirements {query_term} HS {hs_code_8digit}",
            f"CBP_8digit": f"site:cbp.gov import documentation requirements HS {hs_code_8digit} {query_term}",
            f"USDA_8digit": f"site:usda.gov agricultural import requirements {query_term} HS {hs_code_8digit}",
            f"EPA_8digit": f"site:epa.gov environmental regulations {query_term} HS {hs_code_8digit}",
            f"CPSC_8digit": f"site:cpsc.gov consumer product safety {query_term} HS {hs_code_8digit}",
            f"KCS_8digit": f"site:customs.go.kr Korea customs import requirements {query_term} HS {hs_code_8digit}",
            f"MFDS_8digit": f"site:mfds.go.kr food drug safety import {query_term} HS {hs_code_8digit}",
            f"MOTIE_8digit": f"site:motie.go.kr trade policy import requirements {query_term} HS {hs_code_8digit}"
        })
        
        # 6ìë¦¬ HSì½”ë“œ ê²€ìƒ‰ (ìœ ì‚¬)
        search_queries.update({
            f"FDA_6digit": f"site:fda.gov import requirements {query_term} HS {hs_code_6digit}",
            f"FCC_6digit": f"site:fcc.gov device authorization requirements {query_term} HS {hs_code_6digit}",
            f"CBP_6digit": f"site:cbp.gov import documentation requirements HS {hs_code_6digit} {query_term}",
            f"USDA_6digit": f"site:usda.gov agricultural import requirements {query_term} HS {hs_code_6digit}",
            f"EPA_6digit": f"site:epa.gov environmental regulations {query_term} HS {hs_code_6digit}",
            f"CPSC_6digit": f"site:cpsc.gov consumer product safety {query_term} HS {hs_code_6digit}",
            f"KCS_6digit": f"site:customs.go.kr Korea customs import requirements {query_term} HS {hs_code_6digit}",
            f"MFDS_6digit": f"site:mfds.go.kr food drug safety import {query_term} HS {hs_code_6digit}",
            f"MOTIE_6digit": f"site:motie.go.kr trade policy import requirements {query_term} HS {hs_code_6digit}"
        })
        
        search_results = {}
        
        for agency, query in search_queries.items():
            print(f"\n  ğŸ“¡ {agency} ê²€ìƒ‰ ì¤‘...")
            print(f"    ì¿¼ë¦¬: {query}")
            
            # í”„ë¡œë°”ì´ë”ë¥¼ í†µí•œ ê²€ìƒ‰ ì‹œë„ (ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘)
            results = await self.tools.search_provider.search(query, max_results=10)
            print(f"    ğŸ“Š {self.tools.search_provider.provider_name} ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
            
            if not results and self.tools.search_provider.provider_name == "disabled":
                print(f"    ğŸ”‡ ê²€ìƒ‰ ë¹„í™œì„±í™” ëª¨ë“œ: '{query}' ìŠ¤í‚µë¨")
            elif not results:
                print(f"    ğŸ’¡ íŒ: TAVILY_API_KEYë¥¼ ì„¤ì •í•˜ë©´ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ì—¬ëŸ¬ ë§í¬ ìˆ˜ì§‘ (ìµœëŒ€ 5ê°œ)
            chosen_urls = []
            
            if results:
                # ê° ê²°ê³¼ ìƒì„¸ ì¶œë ¥
                for i, result in enumerate(results, 1):
                    title = result.get('title', 'No title')
                    url = result.get('url', 'No URL')
                    print(f"      {i}. {title}")
                    print(f"         URL: {url}")
                
                # site: ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í–ˆìœ¼ë¯€ë¡œ ëª¨ë“  ê²°ê³¼ê°€ ê³µì‹ ì‚¬ì´íŠ¸ (ìµœëŒ€ 5ê°œ ì„ íƒ)
                chosen_urls = [result.get("url") for result in results[:5] if result.get("url")]
                print(f"    âœ… {agency} ê³µì‹ ì‚¬ì´íŠ¸ ê²°ê³¼ {len(chosen_urls)}ê°œ ì„ íƒ")
            else:
                # TavilySearch ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ URL ì‚¬ìš©
                agency_name = agency.split("_")[0]  # FDA_8digit -> FDA
                default_url = default_urls.get(agency_name)
                if default_url:
                    chosen_urls = [default_url]
                print(f"    ğŸ”„ {agency} TavilySearch ì‹¤íŒ¨, ê¸°ë³¸ URL ì‚¬ìš©: {default_url}")
            
            search_results[agency] = {
                "urls": chosen_urls,  # ì—¬ëŸ¬ URL ì €ì¥
                "all_results": results,
                "query": query,
                "is_fallback": not results,  # í´ë°± ì‚¬ìš© ì—¬ë¶€ í‘œì‹œ
                "hs_code_type": "8digit" if "8digit" in agency else "6digit",
                "agency": agency.split("_")[0]  # FDA_8digit -> FDA
            }
        
        # ìš”ì•½ ì¹´ìš´íŠ¸: í•˜ë‚˜ ì´ìƒì˜ URL ë³´ìœ í•œ í•­ëª© ìˆ˜
        found_count = sum(1 for v in search_results.values() if v.get("urls"))
        print(f"\nğŸ“‹ [NODE] ê²€ìƒ‰ ì™„ë£Œ - {found_count}ê°œ URL ì„¸íŠ¸ ë°œê²¬")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["search_results"] = search_results
        # ì°¸ê³  ë§í¬ ì €ì¥
        request = state["request"]
        save_meta = self.tools.save_reference_links(request.hs_code, request.product_name, search_results)
        state["references_saved"] = save_meta
        state["next_action"] = "scrape_documents"
        return state

    async def call_hybrid_api(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ API í˜¸ì¶œ ë…¸ë“œ (Data.gov/USDA/EPA + ì›¹ ê²€ìƒ‰ í†µí•© + Phase 2-4)."""
        request = state["request"]
        hs_code = request.hs_code
        product_name = request.product_name
        product_description = request.product_description or ""
        keywords = state.get("core_keywords") or []
        query_term = (keywords[0] if keywords else product_name) or ""
        print(f"\nğŸ“¡ [NODE] í•˜ì´ë¸Œë¦¬ë“œ API í˜¸ì¶œ ì‹œì‘: {hs_code} / {product_name}")
        try:
            # Phase 2-4 í¬í•¨ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            hybrid = await self.tools.search_requirements_hybrid(hs_code, query_term, product_description)
            state["hybrid_result"] = hybrid
            state["next_action"] = "scrape_documents"
        except Exception as e:
            print(f"  âŒ í•˜ì´ë¸Œë¦¬ë“œ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            state["hybrid_result"] = {"error": str(e)}
            state["next_action"] = "scrape_documents"
        return state
    
    async def scrape_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ë…¸ë“œ"""
        search_results = state["search_results"]
        request = state["request"]
        hs_code = request.hs_code
        
        print(f"\nğŸ” [NODE] ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        scraped_data = {}
        
        # ê¸°ê´€ë³„ë¡œ 8ìë¦¬ì™€ 6ìë¦¬ ê²°ê³¼ í†µí•©
        agency_results = {}
        
        for agency_key, search_data in search_results.items():
            agency_name = search_data["agency"]
            hs_code_type = search_data["hs_code_type"]
            
            if agency_name not in agency_results:
                agency_results[agency_name] = {
                    "8digit": {"urls": [], "results": []},
                    "6digit": {"urls": [], "results": []}
                }
            
            agency_results[agency_name][hs_code_type]["urls"] = search_data["urls"]
        
        # ê° ê¸°ê´€ë³„ë¡œ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰
        for agency_name, agency_data in agency_results.items():
            print(f"\n  ğŸ“„ {agency_name} ìŠ¤í¬ë˜í•‘ ì¤‘...")
            
            # 8ìë¦¬ì™€ 6ìë¦¬ URL ëª¨ë‘ ìˆ˜ì§‘
            all_urls = agency_data["8digit"]["urls"] + agency_data["6digit"]["urls"]
            
            if not all_urls:
                print(f"    âŒ {agency_name}: ìŠ¤í¬ë˜í•‘í•  URL ì—†ìŒ")
                # URLì´ ì—†ì–´ë„ Noneìœ¼ë¡œ ê²°ê³¼ ì €ì¥
                scraped_data[agency_name] = {
                    "certifications": [],
                    "documents": [],
                    "labeling": [],
                    "sources": [],
                    "status": "no_urls_found",
                    "hs_code_8digit": {"urls": agency_data["8digit"]["urls"], "results": []},
                    "hs_code_6digit": {"urls": agency_data["6digit"]["urls"], "results": []}
                }
                continue
            
            print(f"    ğŸ“‹ 8ìë¦¬ URL: {len(agency_data['8digit']['urls'])}ê°œ")
            print(f"    ğŸ“‹ 6ìë¦¬ URL: {len(agency_data['6digit']['urls'])}ê°œ")
            print(f"    ğŸ“‹ ì´ URL: {len(all_urls)}ê°œ")
            
            try:
                # 9ê°œ ê¸°ê´€ ëª¨ë‘ ì²˜ë¦¬
                if agency_name == "FDA":
                    result = await self.web_scraper.scrape_fda_requirements(hs_code, all_urls[0])
                elif agency_name == "FCC":
                    result = await self.web_scraper.scrape_fcc_requirements(hs_code, all_urls[0])
                elif agency_name == "CBP":
                    result = await self.web_scraper.scrape_cbp_requirements(hs_code, all_urls[0])
                elif agency_name == "USDA":
                    result = await self.web_scraper.scrape_usda_requirements(hs_code, all_urls[0])
                elif agency_name == "EPA":
                    result = await self.web_scraper.scrape_epa_requirements(hs_code, all_urls[0])
                elif agency_name == "CPSC":
                    result = await self.web_scraper.scrape_cpsc_requirements(hs_code, all_urls[0])
                elif agency_name == "KCS":
                    result = await self.web_scraper.scrape_kcs_requirements(hs_code, all_urls[0])
                elif agency_name == "MFDS":
                    result = await self.web_scraper.scrape_mfds_requirements(hs_code, all_urls[0])
                elif agency_name == "MOTIE":
                    result = await self.web_scraper.scrape_motie_requirements(hs_code, all_urls[0])
                else:
                    print(f"    âŒ {agency_name}: ì§€ì›ë˜ì§€ ì•ŠëŠ” ê¸°ê´€")
                    continue
                
                # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                certs = result.get("certifications", [])
                docs = result.get("documents", [])
                
                print(f"    âœ… {agency_name} ìŠ¤í¬ë˜í•‘ ì„±ê³µ:")
                print(f"      ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ")
                for cert in certs:
                    print(f"        â€¢ {cert.get('name', 'Unknown')} ({cert.get('agency', 'Unknown')})")
                
                print(f"      ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ")
                for doc in docs:
                    print(f"        â€¢ {doc.get('name', 'Unknown')}")
                
                # HSì½”ë“œ êµ¬ë¶„ ì •ë³´ ì¶”ê°€
                result["hs_code_8digit"] = {
                    "urls": agency_data["8digit"]["urls"],
                    "results": result.get("certifications", []) + result.get("documents", [])
                }
                result["hs_code_6digit"] = {
                    "urls": agency_data["6digit"]["urls"],
                    "results": []
                }
                result["status"] = "success"
                
                scraped_data[agency_name] = result
                
            except Exception as e:
                print(f"    âŒ {agency_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                scraped_data[agency_name] = {
                    "certifications": [],
                    "documents": [],
                    "labeling": [],
                    "sources": [],
                    "status": "scraping_failed",
                    "error": str(e),
                    "hs_code_8digit": {"urls": agency_data["8digit"]["urls"], "results": []},
                    "hs_code_6digit": {"urls": agency_data["6digit"]["urls"], "results": []}
                }
        
        print(f"\nğŸ“‹ [NODE] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ - {len(scraped_data)}ê°œ ê¸°ê´€ ì²˜ë¦¬")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["scraped_data"] = scraped_data
        state["next_action"] = "consolidate_results"
        return state
    
    async def consolidate_results(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
        scraped_data = state["scraped_data"]
        
        print(f"\nğŸ” [NODE] ê²°ê³¼ í†µí•© ì‹œì‘")
        
        all_certifications = []
        all_documents = []
        all_sources = []
        
        for agency, data in scraped_data.items():
            status = data.get("status", "unknown")
            
            if status == "no_urls_found":
                print(f"  âŒ {agency}: URL ì—†ìŒ (None)")
                continue
            elif status == "scraping_failed":
                print(f"  âŒ {agency}: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (None)")
                continue
            elif "error" in data:
                print(f"  âŒ {agency}: ì˜¤ë¥˜ë¡œ ì¸í•´ ì œì™¸ (None)")
                continue
                
            print(f"  ğŸ“Š {agency} ë°ì´í„° í†µí•©:")
            
            # ì¸ì¦ìš”ê±´ í†µí•©
            certs = data.get("certifications", [])
            all_certifications.extend(certs)
            print(f"    ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ ì¶”ê°€")
            
            # í•„ìš”ì„œë¥˜ í†µí•©
            docs = data.get("documents", [])
            all_documents.extend(docs)
            print(f"    ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ ì¶”ê°€")
            
            # ì¶œì²˜ í†µí•©
            sources = data.get("sources", [])
            all_sources.extend(sources)
            print(f"    ğŸ“š ì¶œì²˜: {len(sources)}ê°œ ì¶”ê°€")
        
        print(f"\nğŸ“‹ [NODE] í†µí•© ì™„ë£Œ:")
        print(f"  ğŸ“‹ ì´ ì¸ì¦ìš”ê±´: {len(all_certifications)}ê°œ")
        print(f"  ğŸ“„ ì´ í•„ìš”ì„œë¥˜: {len(all_documents)}ê°œ")
        print(f"  ğŸ“š ì´ ì¶œì²˜: {len(all_sources)}ê°œ")
        
        # ì°¸ê³ : CBP íŒë¡€ ìˆ˜ì§‘ ì¶”ê°€
        request = state.get("request")
        cbp = None
        if request:
            try:
                cbp = await self.tools.get_cbp_precedents(request.hs_code)
            except Exception:
                cbp = {"error": "precedent_fetch_failed"}

        # í•˜ì´ë¸Œë¦¬ë“œ(API+ì›¹) ê²°ê³¼ë„ í†µí•© (Phase 2-4 í¬í•¨)
        hybrid = state.get("hybrid_result") or {}
        if hybrid and not hybrid.get("error"):
            combined = hybrid.get("combined_results", {})
            if combined:
                all_certifications.extend(combined.get("certifications", []))
                all_documents.extend(combined.get("documents", []))
                all_sources.extend(combined.get("sources", []))
                
                # Phase 2-4 ê²°ê³¼ í†µí•©
                print(f"  ğŸ“Š Phase 2-4 ê²°ê³¼ í†µí•©:")
                print(f"    ğŸ§ª ê²€ì‚¬ ì ˆì°¨: {len(combined.get('testing_procedures', []))}ê°œ")
                print(f"    âš–ï¸ ì²˜ë²Œ ì •ë³´: {len(combined.get('penalties_enforcement', []))}ê°œ")
                print(f"    â° ìœ íš¨ê¸°ê°„: {len(combined.get('validity_periods', []))}ê°œ")

        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìƒíƒœ ìœ ì§€)
        state["consolidated_results"] = {
            "certifications": all_certifications,
            "documents": all_documents,
            "sources": all_sources,
            "precedents": cbp.get("precedents", []) if cbp else []
        }
        state["precedents_meta"] = cbp
        state["next_action"] = "complete"
        return state
