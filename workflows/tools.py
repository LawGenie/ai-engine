"""
LangGraph Tools for Requirements Analysis
íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë„êµ¬ë“¤
"""

from typing import Dict, Any, List, Optional
import asyncio
import httpx
from pathlib import Path
import json
from pypdf import PdfReader
from io import BytesIO
from datetime import datetime
import importlib.util
import sys
from abc import ABC, abstractmethod
from app.services.requirements.tavily_search import TavilySearchService
from app.services.requirements.web_scraper import WebScraper
from app.services.requirements.data_gov_api import DataGovAPIService


class SearchProvider(ABC):
    """ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì¶”ìƒí™” í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ì‹¤í–‰"""
        pass
    
    @property
    @abstractmethod
    def provider_name(self) -> str:
        """í”„ë¡œë°”ì´ë” ì´ë¦„"""
        pass


class TavilyProvider(SearchProvider):
    """Tavily ê²€ìƒ‰ í”„ë¡œë°”ì´ë”"""
    
    def __init__(self):
        self.service = TavilySearchService()
    
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        try:
            results = await self.service.search(query, **kwargs)
            return results if results else []
        except Exception as e:
            print(f"âŒ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    @property
    def provider_name(self) -> str:
        return "tavily"


class DisabledProvider(SearchProvider):
    """ê²€ìƒ‰ ë¹„í™œì„±í™” í”„ë¡œë°”ì´ë” (Tavily 432 ì—ëŸ¬ ì‹œ ì‚¬ìš©)"""
    
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        print(f"ğŸ”‡ ê²€ìƒ‰ ë¹„í™œì„±í™” ëª¨ë“œ: '{query}' ìŠ¤í‚µë¨")
        return []
    
    @property
    def provider_name(self) -> str:
        return "disabled"


class RequirementsTools:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ LangGraph ë„êµ¬ë“¤"""
    
    def __init__(self, search_provider: Optional[SearchProvider] = None):
        # ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì„¤ì • (ê¸°ë³¸ê°’: Tavily, í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
        import os
        provider_mode = os.getenv("SEARCH_PROVIDER", "tavily").lower()
        
        if provider_mode == "disabled":
            self.search_provider = DisabledProvider()
        else:
            self.search_provider = TavilyProvider()
        
        # ì™¸ë¶€ì—ì„œ ì œê³µëœ í”„ë¡œë°”ì´ë”ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if search_provider:
            self.search_provider = search_provider
            
        # HS ì½”ë“œ ê¸°ë°˜ ê¸°ê´€ ë§¤í•‘
        self.hs_code_agency_mapping = self._build_hs_code_mapping()
            
        # API í‚¤ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            self.web_scraper = WebScraper()
        except Exception as e:
            print(f"âš ï¸ WebScraper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.web_scraper = None
        
        try:
            self.data_gov_api = DataGovAPIService()
        except Exception as e:
            print(f"âš ï¸ DataGovAPIService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.data_gov_api = None
        
        try:
            self.precedent_collector = self._init_cbp_collector()
        except Exception as e:
            print(f"âš ï¸ CBP Collector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.precedent_collector = None
        
        self.references_store_path = Path("reference_links.json")
    def _build_hs_code_mapping(self) -> Dict[str, Dict[str, Any]]:
        """HS ì½”ë“œ ê¸°ë°˜ ì •ë¶€ê¸°ê´€ ë§¤í•‘ êµ¬ì¶•"""
        return {
            # í™”ì¥í’ˆ ë° ë¯¸ìš© ì œí’ˆ (33xx)
                    "3304": {
                        "primary_agencies": ["FDA", "CPSC"],
                        "secondary_agencies": ["FTC"],
                        "search_keywords": ["cosmetic", "skincare", "beauty", "serum", "cream"],
                        "requirements": ["cosmetic registration", "ingredient safety", "labeling compliance", "consumer safety"]
                    },
            "3307": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["DOT"],  # ìš´ì†¡ ê´€ë ¨ (ì•Œì½”ì˜¬ í•¨ìœ )
                "search_keywords": ["perfume", "toilet water", "fragrance", "alcohol"],
                "requirements": ["cosmetic registration", "alcohol content", "shipping requirements"]
            },
            
            # ì‹í’ˆ ë° ê±´ê°•ë³´ì¡°ì‹í’ˆ (21xx, 19xx, 20xx)
            "2106": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["dietary supplement", "ginseng", "extract", "health"],
                "requirements": ["prior notice", "DSHEA compliance", "cGMP", "health claims"]
            },
            "1904": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["rice", "cereal", "prepared food", "instant"],
                "requirements": ["prior notice", "nutritional labeling", "allergen declaration"]
            },
            "1905": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["snack", "cracker", "cookie", "baker"],
                "requirements": ["prior notice", "nutritional labeling", "FALCPA", "inspection"]
            },
            "1902": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["pasta", "noodle", "instant", "ramen"],
                "requirements": ["prior notice", "nutritional labeling", "allergen", "sodium"]
            },
            "2005": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["vegetable", "kimchi", "fermented", "preserved"],
                "requirements": ["prior notice", "HARPC", "acidified foods", "refrigeration"]
            },
            
            # ì „ìì œí’ˆ ë° í†µì‹  (84xx, 85xx)
            "8471": {
                "primary_agencies": ["FCC"],
                "secondary_agencies": ["CPSC"],
                "search_keywords": ["computer", "electronic", "device", "equipment"],
                "requirements": ["device authorization", "EMC", "safety standards"]
            },
            "8517": {
                "primary_agencies": ["FCC"],
                "secondary_agencies": ["CPSC"],
                "search_keywords": ["telephone", "communication", "wireless", "radio"],
                "requirements": ["equipment authorization", "radio frequency", "EMC"]
            },
            
            # ì˜ë¥˜ ë° ì„¬ìœ  (61xx, 62xx)
            "6109": {
                "primary_agencies": ["CPSC"],
                "secondary_agencies": ["FTC"],
                "search_keywords": ["t-shirt", "clothing", "textile", "garment"],
                "requirements": ["flammability", "care labeling", "fiber content"]
            },
            
            # ì¥ë‚œê° ë° ì–´ë¦°ì´ ì œí’ˆ (95xx)
            "9503": {
                "primary_agencies": ["CPSC"],
                "secondary_agencies": ["FDA"],
                "search_keywords": ["toy", "children", "play", "game"],
                "requirements": ["safety standards", "lead content", "small parts", "age grading"]
            }
        }

    def _get_target_agencies_for_hs_code(self, hs_code: str) -> Dict[str, Any]:
        """HS ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ê²Ÿ ê¸°ê´€ ë° ê²€ìƒ‰ ì „ëµ ë°˜í™˜"""
        # HS ì½”ë“œì—ì„œ 4ìë¦¬ ì½”ë“œ ì¶”ì¶œ
        hs_4digit = hs_code.split('.')[0] if '.' in hs_code else hs_code[:4]
        
        # ë§¤í•‘ì—ì„œ í•´ë‹¹ ì½”ë“œ ì°¾ê¸°
        mapping = self.hs_code_agency_mapping.get(hs_4digit, {})
        
        if not mapping:
            # ê¸°ë³¸ ë§¤í•‘ (ëª¨ë“  ê¸°ê´€ ê²€ìƒ‰)
            return {
                "primary_agencies": ["FDA", "USDA", "EPA", "FCC", "CPSC"],
                "secondary_agencies": [],
                "search_keywords": [],
                "requirements": [],
                "confidence": 0.3
            }
        
        return {
            **mapping,
            "confidence": 0.9
        }

    def _extract_keywords_from_product(self, product_name: str, product_description: str = "") -> List[str]:
        """ìƒí’ˆëª…ê³¼ ì„¤ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ìƒí’ˆëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        name_keywords = [
            "vitamin", "serum", "cream", "extract", "ginseng", "rice", "noodle", 
            "kimchi", "snack", "perfume", "cosmetic", "supplement", "food",
            "electronic", "device", "toy", "clothing", "textile"
        ]
        
        product_text = f"{product_name} {product_description}".lower()
        for keyword in name_keywords:
            if keyword in product_text:
                keywords.append(keyword)
        
        # ìƒí’ˆëª…ì—ì„œ ì§ì ‘ ì¶”ì¶œ
        words = product_name.lower().split()
        for word in words:
            if len(word) > 3 and word not in ["premium", "korean", "instant", "pack"]:
                keywords.append(word)
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°

    def _build_hs_code_based_queries(self, product_name: str, hs_code: str, target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """HS ì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = {}
        
        # ì£¼ìš” ê¸°ê´€ë³„ ê²€ìƒ‰ (HS ì½”ë“œ ê¸°ë°˜)
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ê²€ìƒ‰
            queries[f"{agency}_hs_requirements"] = f"site:{agency_lower}.gov import requirements {product_name} HS {hs_code}"
            
            # ì„¸ë¶€ ê·œì • ê²€ìƒ‰ (ê¸°ê´€ë³„ íŠ¹í™”)
            if agency == "FDA":
                if "cosmetic" in target_agencies.get("search_keywords", []):
                    queries[f"{agency}_hs_cosmetic"] = f"site:{agency_lower}.gov cosmetic regulations HS {hs_code} ingredient safety"
                if "food" in target_agencies.get("search_keywords", []):
                    queries[f"{agency}_hs_food"] = f"site:{agency_lower}.gov food import requirements HS {hs_code} prior notice"
                if "supplement" in target_agencies.get("search_keywords", []):
                    queries[f"{agency}_hs_supplement"] = f"site:{agency_lower}.gov dietary supplement requirements HS {hs_code} DSHEA"
            
            elif agency == "USDA":
                queries[f"{agency}_hs_agricultural"] = f"site:{agency_lower}.gov agricultural import requirements HS {hs_code}"
                queries[f"{agency}_hs_organic"] = f"site:{agency_lower}.gov organic certification HS {hs_code}"
            
            elif agency == "EPA":
                queries[f"{agency}_hs_chemical"] = f"site:{agency_lower}.gov chemical regulations HS {hs_code}"
                queries[f"{agency}_hs_environmental"] = f"site:{agency_lower}.gov environmental standards HS {hs_code}"
            
            elif agency == "FCC":
                queries[f"{agency}_hs_device"] = f"site:{agency_lower}.gov device authorization HS {hs_code}"
                queries[f"{agency}_hs_emc"] = f"site:{agency_lower}.gov EMC electromagnetic compatibility HS {hs_code}"
            
            elif agency == "CPSC":
                queries[f"{agency}_hs_safety"] = f"site:{agency_lower}.gov safety standards HS {hs_code}"
                queries[f"{agency}_hs_recall"] = f"site:{agency_lower}.gov recall information HS {hs_code}"
        
        return queries

    def _build_keyword_based_queries(self, product_name: str, keywords: List[str], target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = {}
        
        # ì£¼ìš” ê¸°ê´€ë³„ í‚¤ì›Œë“œ ê²€ìƒ‰
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                # í‚¤ì›Œë“œë³„ íŠ¹í™” ê²€ìƒ‰
                if keyword in ["vitamin", "serum", "cream", "cosmetic"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov cosmetic regulations {keyword} import requirements"
                elif keyword in ["ginseng", "extract", "supplement"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov dietary supplement {keyword} import requirements"
                elif keyword in ["rice", "noodle", "kimchi", "food"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov food import requirements {keyword}"
                elif keyword in ["electronic", "device"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov device authorization {keyword}"
                elif keyword in ["toy", "clothing", "textile"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov safety standards {keyword}"
        
        return queries

    def _build_phase_specific_queries(self, product_name: str, hs_code: str, target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """Phase 2-4 ì „ìš© ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = {}
        
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            # Phase 2: ê²€ì‚¬ ì ˆì°¨ ë° ë°©ë²•
            queries[f"{agency}_phase2_testing"] = f"site:{agency_lower}.gov testing procedures {product_name} HS {hs_code}"
            queries[f"{agency}_phase2_inspection"] = f"site:{agency_lower}.gov inspection methods {product_name} HS {hs_code}"
            queries[f"{agency}_phase2_authorization"] = f"site:{agency_lower}.gov authorization procedures {product_name} HS {hs_code}"
            
            # Phase 3: ì²˜ë²Œ ë° ë²Œê¸ˆ ì •ë³´
            queries[f"{agency}_phase3_penalties"] = f"site:{agency_lower}.gov penalties violations {product_name} HS {hs_code}"
            queries[f"{agency}_phase3_enforcement"] = f"site:{agency_lower}.gov enforcement actions {product_name} HS {hs_code}"
            queries[f"{agency}_phase3_fines"] = f"site:{agency_lower}.gov civil penalties {product_name} HS {hs_code}"
            
            # Phase 4: ìœ íš¨ê¸°ê°„ ë° ê°±ì‹  ì •ë³´
            queries[f"{agency}_phase4_validity"] = f"site:{agency_lower}.gov certificate validity period {product_name} HS {hs_code}"
            queries[f"{agency}_phase4_renewal"] = f"site:{agency_lower}.gov certification renewal {product_name} HS {hs_code}"
            queries[f"{agency}_phase4_duration"] = f"site:{agency_lower}.gov permit duration {product_name} HS {hs_code}"
        
        return queries

    def _init_cbp_collector(self):
        """precedents-analysis/cbp_scraper.pyì˜ CBPDataCollectorë¥¼ ë™ì  ë¡œë“œí•œë‹¤."""
        try:
            base_dir = Path(__file__).resolve().parents[1]  # ai-engine/app
            project_root = base_dir.parent  # ai-engine
            target_path = project_root / "precedents-analysis" / "cbp_scraper.py"
            if not target_path.exists():
                return None
            spec = importlib.util.spec_from_file_location("cbp_scraper", str(target_path))
            if spec is None or spec.loader is None:
                return None
            module = importlib.util.module_from_spec(spec)
            sys.modules["cbp_scraper"] = module
            spec.loader.exec_module(module)
            if hasattr(module, "CBPDataCollector"):
                return module.CBPDataCollector()
        except Exception:
            return None
        return None

        
        # ê¸°ê´€ë³„ ë„ë©”ì¸ ë§¤í•‘
        self.agency_domains = {
            "FDA": "fda.gov",
            "FCC": "fcc.gov", 
            "CBP": "cbp.gov",
            "USDA": "usda.gov",
            "EPA": "epa.gov",
            "CPSC": "cpsc.gov",
            "KCS": "customs.go.kr",  # í•œêµ­ ê´€ì„¸ì²­
            "MFDS": "mfds.go.kr",    # ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜
            "MOTIE": "motie.go.kr"   # ì‚°ì—…í†µìƒìì›ë¶€
        }
    
    async def search_agency_documents(self, agency: str, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (í†µí•©)"""
        print(f"ğŸ”§ [TOOL] {agency} ë¬¸ì„œ ê²€ìƒ‰: {query}")
        
        results = await self.search_provider.search(query, max_results=max_results)
        
        # ê¸°ê´€ë³„ ë„ë©”ì¸ í•„í„°ë§
        agency_domain = self.agency_domains.get(agency, "")
        agency_results = []
        
        for result in results:
            url = result.get("url", "")
            if agency_domain in url:
                agency_results.append(result)
                print(f"  âœ… {agency} ê³µì‹ ë¬¸ì„œ ë°œê²¬: {result.get('title', 'No title')}")
            else:
                print(f"  âŒ {agency} ì™¸ë¶€ ë¬¸ì„œ ì œì™¸: {result.get('title', 'No title')}")
        
        return {
            "agency": agency,
            "query": query,
            "total_results": len(results),
            "agency_results": agency_results,
            "selected_url": agency_results[0]["url"] if agency_results else None,
            "domain": agency_domain
        }
    
    async def search_fda_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """FDA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.search_agency_documents("FDA", query, max_results)
    
    # ë¯¸êµ­ ì •ë¶€ ê¸°ê´€ë“¤
    async def search_fcc_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """FCC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("FCC", query, max_results)
    
    async def search_cbp_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """CBP ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("CBP", query, max_results)
    
    async def search_usda_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """USDA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("USDA", query, max_results)
    
    async def search_epa_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """EPA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("EPA", query, max_results)
    
    async def search_cpsc_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """CPSC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("CPSC", query, max_results)
    
    # í•œêµ­ ì •ë¶€ ê¸°ê´€ë“¤
    async def search_kcs_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """í•œêµ­ ê´€ì„¸ì²­ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("KCS", query, max_results)
    
    async def search_mfds_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("MFDS", query, max_results)
    
    async def search_motie_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ì‚°ì—…í†µìƒìì›ë¶€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("MOTIE", query, max_results)
    
    async def scrape_document(self, agency: str, url: str, hs_code: str) -> Dict[str, Any]:
        """íŠ¹ì • ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ë„êµ¬ (í™•ì¥)"""
        print(f"ğŸ”§ [TOOL] {agency} ë¬¸ì„œ ìŠ¤í¬ë˜í•‘")
        print(f"  URL: {url}")
        print(f"  HSì½”ë“œ: {hs_code}")
        
        try:
            # ê¸°ê´€ë³„ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œ ë§¤í•‘
            scraper_methods = {
                "FDA": "scrape_fda_requirements",
                "FCC": "scrape_fcc_requirements", 
                "CBP": "scrape_cbp_requirements",
                "USDA": "scrape_usda_requirements",
                "EPA": "scrape_epa_requirements",
                "CPSC": "scrape_cpsc_requirements",
                "KCS": "scrape_kcs_requirements",
                "MFDS": "scrape_mfds_requirements",
                "MOTIE": "scrape_motie_requirements"
            }
            
            method_name = scraper_methods.get(agency)
            if not method_name:
                return {"error": f"Unknown agency: {agency}"}
            
            # ë™ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œ í˜¸ì¶œ
            scraper_method = getattr(self.web_scraper, method_name, None)
            if not scraper_method:
                return {"error": f"Scraper method not implemented for {agency}"}
            
            result = await scraper_method(hs_code, url)
            
            # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            certs = result.get("certifications", [])
            docs = result.get("documents", [])
            sources = result.get("sources", [])
            
            print(f"  âœ… {agency} ìŠ¤í¬ë˜í•‘ ì„±ê³µ:")
            print(f"    ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ")
            for i, cert in enumerate(certs, 1):
                print(f"      {i}. {cert.get('name', 'Unknown')} ({cert.get('agency', 'Unknown')})")
                print(f"         ì„¤ëª…: {cert.get('description', 'No description')}")
            
            print(f"    ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ")
            for i, doc in enumerate(docs, 1):
                print(f"      {i}. {doc.get('name', 'Unknown')}")
                print(f"         ì„¤ëª…: {doc.get('description', 'No description')}")
            
            print(f"    ğŸ“š ì¶œì²˜: {len(sources)}ê°œ")
            for i, source in enumerate(sources, 1):
                print(f"      {i}. {source.get('title', 'Unknown')} ({source.get('type', 'Unknown')})")
            
            return result
            
        except Exception as e:
            print(f"  âŒ {agency} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                "agency": agency,
                "error": str(e),
                "certifications": [],
                "documents": [],
                "sources": []
            }

    async def get_cbp_precedents(self, hs_code: str) -> Dict[str, Any]:
        """CBP íŒë¡€/ê²°ì • ì‚¬ë¡€ ì¡°íšŒ ë„êµ¬."""
        try:
            if not self.precedent_collector:
                return {"hs_code": hs_code, "count": 0, "precedents": [], "error": "cbp_collector_not_available"}
            precedents = await self.precedent_collector.get_precedents_by_hs_code(hs_code)
            return {
                "hs_code": hs_code,
                "count": len(precedents),
                "precedents": precedents
            }
        except Exception as e:
            return {"hs_code": hs_code, "count": 0, "precedents": [], "error": str(e)}

    async def summarize_pdf(self, url: str, max_pages: int = 5) -> Dict[str, Any]:
        """PDF ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì•ë¶€ë¶„ì„ ìš”ì•½(ë°œì·Œ)í•œë‹¤."""
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                resp = await client.get(url)
                resp.raise_for_status()
                data = BytesIO(resp.content)
                reader = PdfReader(data)
                num_pages = min(len(reader.pages), max_pages)
                text_chunks: List[str] = []
                for i in range(num_pages):
                    try:
                        text_chunks.append(reader.pages[i].extract_text() or "")
                    except Exception:
                        continue
                combined = "\n".join([t.strip() for t in text_chunks if t and t.strip()])
                preview = (combined[:1200] + "â€¦") if len(combined) > 1200 else combined
                return {
                    "url": url,
                    "pages_read": num_pages,
                    "excerpt": preview,
                    "char_count": len(preview)
                }
        except Exception as e:
            return {"url": url, "error": str(e)}

    def save_reference_links(self, hs_code: str, product_name: str, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ëœ ì°¸ê³  ë§í¬ë“¤ì„ ë¡œì»¬ JSONì— ì €ì¥/ë³‘í•©í•œë‹¤."""
        try:
            existing: Dict[str, Any] = {}
            if self.references_store_path.exists():
                existing = json.loads(self.references_store_path.read_text(encoding="utf-8"))
            key = f"{hs_code}:{product_name}"
            payload = {
                "hs_code": hs_code,
                "product_name": product_name,
                "saved_at": datetime.utcnow().isoformat() + "Z",
                "agencies": {}
            }
            for k, v in search_results.items():
                agency = v.get("agency") or k
                urls = v.get("urls", [])
                payload["agencies"].setdefault(agency, {"urls": []})
                # ë³‘í•©
                payload["agencies"][agency]["urls"] = list({*payload["agencies"][agency]["urls"], *urls})
            existing[key] = payload
            self.references_store_path.write_text(json.dumps(existing, ensure_ascii=False, indent=2), encoding="utf-8")
            return {"saved": True, "reference_key": key, "agencies": list(payload["agencies"].keys())}
        except Exception as e:
            return {"saved": False, "error": str(e)}
    
    async def analyze_requirements(self, requirements_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë„êµ¬ (í™•ì¥)"""
        print(f"ğŸ”§ [TOOL] ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹œì‘")
        
        certifications = requirements_data.get("certifications", [])
        documents = requirements_data.get("documents", [])
        sources = requirements_data.get("sources", [])
        
        # ê¸°ë³¸ ë¶„ì„ í†µê³„
        total_certs = len(certifications)
        total_docs = len(documents)
        total_sources = len(sources)
        
        # ê¸°ê´€ë³„ í†µê³„
        agency_stats = {}
        for cert in certifications:
            agency = cert.get("agency", "Unknown")
            agency_stats[agency] = agency_stats.get(agency, 0) + 1
        
        # ìš°ì„ ìˆœìœ„ ë¶„ì„
        high_priority = [c for c in certifications if c.get("priority") == "high"]
        required_docs = [d for d in documents if d.get("required", False)]
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        completeness_score = min(1.0, (total_certs + total_docs) / 10)  # 0-1 ìŠ¤ì¼€ì¼
        coverage_ratio = len(agency_stats) / len(self.agency_domains)  # ê¸°ê´€ ì»¤ë²„ë¦¬ì§€
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_factors = []
        if total_certs > 5:
            complexity_factors.append("ë‹¤ì¤‘ ì¸ì¦ ìš”êµ¬")
        if len(agency_stats) > 3:
            complexity_factors.append("ë‹¤ê¸°ê´€ ê·œì œ")
        if any("critical" in str(cert).lower() for cert in certifications):
            complexity_factors.append("ì¤‘ìš” ì¸ì¦ ìš”êµ¬")
        
        compliance_complexity = "simple" if len(complexity_factors) == 0 else "moderate" if len(complexity_factors) <= 2 else "complex"
        
        # ë¹„ìš© ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        estimated_cost_low = total_certs * 100 + total_docs * 50  # USD
        estimated_cost_high = total_certs * 500 + total_docs * 200
        
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        risk_factors = []
        if total_certs == 0:
            risk_factors.append("ì¸ì¦ ìš”êµ¬ì‚¬í•­ ë¶ˆëª…í™•")
        if len(required_docs) > 10:
            risk_factors.append("ì„œë¥˜ ìš”êµ¬ì‚¬í•­ ê³¼ë‹¤")
        if coverage_ratio < 0.3:
            risk_factors.append("ê¸°ê´€ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±")
        
        overall_risk_level = "low" if len(risk_factors) == 0 else "medium" if len(risk_factors) <= 2 else "high"
        
        print(f"  ğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"    ğŸ“‹ ì´ ì¸ì¦ìš”ê±´: {total_certs}ê°œ")
        print(f"    ğŸ“„ ì´ í•„ìš”ì„œë¥˜: {total_docs}ê°œ")
        print(f"    ğŸ“š ì´ ì¶œì²˜: {total_sources}ê°œ")
        print(f"    ğŸ¢ ê¸°ê´€ë³„ ì¸ì¦ìš”ê±´:")
        for agency, count in agency_stats.items():
            print(f"      â€¢ {agency}: {count}ê°œ")
        print(f"    âš ï¸ ê³ ìš°ì„ ìˆœìœ„ ì¸ì¦ìš”ê±´: {len(high_priority)}ê°œ")
        print(f"    ğŸ“‹ í•„ìˆ˜ ì„œë¥˜: {len(required_docs)}ê°œ")
        print(f"    ğŸ“ˆ ì™„ì„±ë„ ì ìˆ˜: {completeness_score:.2f}")
        print(f"    ğŸ¯ ê¸°ê´€ ì»¤ë²„ë¦¬ì§€: {coverage_ratio:.2f}")
        print(f"    âš¡ ë³µì¡ë„: {compliance_complexity}")
        print(f"    ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost_low}-${estimated_cost_high}")
        print(f"    âš ï¸ ë¦¬ìŠ¤í¬ ë ˆë²¨: {overall_risk_level}")
        
        return {
            # ê¸°ë³¸ í†µê³„
            "total_certifications": total_certs,
            "total_documents": total_docs,
            "total_sources": total_sources,
            "agency_stats": agency_stats,
            "high_priority_count": len(high_priority),
            "required_docs_count": len(required_docs),
            
            # í’ˆì§ˆ ì§€í‘œ
            "quality_metrics": {
                "completeness_score": completeness_score,
                "coverage_ratio": coverage_ratio,
                "compliance_complexity": compliance_complexity,
                "complexity_factors": complexity_factors
            },
            
            # ë¹„ìš© ë¶„ì„
            "cost_analysis": {
                "estimated_cost_low": estimated_cost_low,
                "estimated_cost_high": estimated_cost_high,
                "currency": "USD"
            },
            
            # ë¦¬ìŠ¤í¬ ë¶„ì„
            "risk_analysis": {
                "overall_risk_level": overall_risk_level,
                "risk_factors": risk_factors
            },
            
            "analysis_complete": True
        }
    
    async def search_requirements_hybrid(self, hs_code: str, product_name: str, product_description: str = "") -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Data.gov API + Tavily Search"""
        print(f"\nğŸš€ [HYBRID] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {product_name}")
        
        results = {
            "hs_code": hs_code,
            "product_name": product_name,
            "search_timestamp": None,
            "api_results": {},
            "web_results": {},
            "combined_results": {},
            "search_methods": []
        }
        
        # 1. Data.gov API ê²€ìƒ‰ (HSì½”ë“œ ì§ì ‘ ê²€ìƒ‰)
        try:
            print(f"\n  ğŸ” 1ë‹¨ê³„: Data.gov API ê²€ìƒ‰")
            api_results = await self.data_gov_api.search_requirements_by_hs_code(hs_code, product_name)
            results["api_results"] = api_results
            results["search_methods"].append("data_gov_api")
            print(f"    âœ… API ê²€ìƒ‰ ì™„ë£Œ: {api_results.get('total_requirements', 0)}ê°œ ìš”êµ¬ì‚¬í•­")
        except Exception as e:
            print(f"    âŒ API ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["api_results"] = {"error": str(e)}
        
        # 2. Tavily Search (HS ì½”ë“œ ê¸°ë°˜ + í‚¤ì›Œë“œ ê¸°ë°˜ ë³µí•© ê²€ìƒ‰)
        try:
            print(f"\n  ğŸ” 2ë‹¨ê³„: HS ì½”ë“œ ê¸°ë°˜ + í‚¤ì›Œë“œ ê¸°ë°˜ ë³µí•© ê²€ìƒ‰")
            
            # HS ì½”ë“œ ê¸°ë°˜ íƒ€ê²Ÿ ê¸°ê´€ ë¶„ì„
            target_agencies = self._get_target_agencies_for_hs_code(hs_code)
            
            # ìƒí’ˆëª…/ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_from_product(product_name, product_description)
            
            # HS ì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            hs_queries = self._build_hs_code_based_queries(product_name, hs_code, target_agencies)
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            keyword_queries = self._build_keyword_based_queries(product_name, keywords, target_agencies)
            
            # Phase 2-4 ì „ìš© ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            phase_queries = self._build_phase_specific_queries(product_name, hs_code, target_agencies)
            
            # ë³µí•© ê²€ìƒ‰ ì¿¼ë¦¬ ë³‘í•©
            web_queries = {**hs_queries, **keyword_queries, **phase_queries}
            
            print(f"  ğŸ¯ íƒ€ê²Ÿ ê¸°ê´€: {', '.join(target_agencies.get('primary_agencies', []))}")
            print(f"  ğŸ“Š ê²€ìƒ‰ ì‹ ë¢°ë„: {target_agencies.get('confidence', 0):.1%}")
            print(f"  ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}")
            print(f"  ğŸ” ì´ ê²€ìƒ‰ ì¿¼ë¦¬: {len(web_queries)}ê°œ (HSì½”ë“œ {len(hs_queries)}ê°œ + í‚¤ì›Œë“œ {len(keyword_queries)}ê°œ + Phase2-4 {len(phase_queries)}ê°œ)")
            
            web_results = {}
            for query_key, query in web_queries.items():
                try:
                    if self.search_provider:
                        search_results = await self.search_provider.search(query, max_results=5)
                    else:
                        print(f"    âš ï¸ ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì—†ìŒ: {query_key} ìŠ¤í‚µë¨")
                        search_results = []
                    # ê²°ê³¼ ë¶„ë¥˜ (HS ì½”ë“œ ê¸°ë°˜ + í‚¤ì›Œë“œ ê¸°ë°˜)
                    category = "basic_requirements"
                    search_type = "hs_code" if "hs_" in query_key else "keyword"
                    
                    # ì¿¼ë¦¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (Phase 1-4)
                    if any(keyword in query_key for keyword in ["cosmetic", "regulations", "standards", "limits", "restrictions", "safety"]):
                        category = "detailed_regulations"
                    elif any(keyword in query_key for keyword in ["testing", "inspection", "procedures", "authorization", "phase2"]):
                        category = "testing_procedures"
                    elif any(keyword in query_key for keyword in ["penalties", "enforcement", "violations", "recall", "phase3"]):
                        category = "penalties_enforcement"
                    elif any(keyword in query_key for keyword in ["validity", "renewal", "duration", "period", "phase4"]):
                        category = "validity_periods"
                    
                    # ê¸°ê´€ ì¶”ì¶œ
                    agency = query_key.split("_")[0].upper()
                    
                    web_results[query_key] = {
                        "query": query,
                        "results": search_results,
                        "urls": [r.get("url") for r in search_results if r.get("url")],
                        "agency": agency,
                        "category": category,
                        "search_type": search_type,
                        "result_count": len(search_results),
                        "target_confidence": target_agencies.get("confidence", 0.5)
                    }
                except Exception as e:
                    web_results[query_key] = {"error": str(e)}
            
            results["web_results"] = web_results
            results["search_methods"].append("tavily_search")
            print(f"    âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(web_results)}ê°œ ì¿¼ë¦¬")
            
        except Exception as e:
            print(f"    âŒ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["web_results"] = {"error": str(e)}
        
        # 3. ê²°ê³¼ í†µí•©
        print(f"\n  ğŸ”„ 3ë‹¨ê³„: ê²°ê³¼ í†µí•©")
        combined_results = self._combine_search_results(results["api_results"], results["web_results"])
        combined_results["target_agencies"] = target_agencies  # íƒ€ê²Ÿ ê¸°ê´€ ì •ë³´ ì¶”ê°€
        combined_results["extracted_keywords"] = keywords  # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
        results["combined_results"] = combined_results
        
        print(f"\nâœ… [HS ì½”ë“œ + í‚¤ì›Œë“œ ë³µí•© ê²€ìƒ‰] ì™„ë£Œ")
        print(f"  ğŸ” ê²€ìƒ‰ ë°©ë²•: {', '.join(results['search_methods'])}")
        print(f"  ğŸ¯ íƒ€ê²Ÿ ê¸°ê´€: {', '.join(target_agencies.get('primary_agencies', []))}")
        print(f"  ğŸ“Š ê²€ìƒ‰ ì‹ ë¢°ë„: {target_agencies.get('confidence', 0):.1%}")
        print(f"  ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}")
        print(f"  ğŸ“‹ ì´ ìš”êµ¬ì‚¬í•­: {combined_results.get('total_requirements', 0)}ê°œ")
        print(f"  ğŸ† ì¸ì¦ìš”ê±´: {combined_results.get('total_certifications', 0)}ê°œ")
        print(f"  ğŸ“„ í•„ìš”ì„œë¥˜: {combined_results.get('total_documents', 0)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì¶œë ¥
        category_stats = combined_results.get('category_stats', {})
        print(f"  ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼:")
        print(f"    ğŸ” ê¸°ë³¸ ìš”êµ¬ì‚¬í•­: {category_stats.get('basic_requirements', 0)}ê°œ")
        print(f"    ğŸ“‹ ì„¸ë¶€ ê·œì •: {category_stats.get('detailed_regulations', 0)}ê°œ")
        print(f"    ğŸ§ª ê²€ì‚¬ ì ˆì°¨: {category_stats.get('testing_procedures', 0)}ê°œ")
        print(f"    âš–ï¸ ì²˜ë²Œ ì •ë³´: {category_stats.get('penalties_enforcement', 0)}ê°œ")
        print(f"    â° ìœ íš¨ê¸°ê°„: {category_stats.get('validity_periods', 0)}ê°œ")
        
        return results
    
    def _extract_requirements_from_web_results(self, web_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        extracted_requirements = {
            "certifications": [],
            "documents": [],
            "sources": [],
            "detailed_regulations": [],
            "testing_procedures": [],
            "penalties_enforcement": [],
            "validity_periods": []
        }
        
        for query_key, result in web_results.items():
            if "error" in result:
                continue
                
            agency = result.get("agency", "Unknown")
            category = result.get("category", "basic_requirements")
            search_results = result.get("results", [])
            
            for search_result in search_results:
                url = search_result.get("url", "")
                title = search_result.get("title", "")
                content = search_result.get("content", "")
                score = search_result.get("score", 0)
                
                # ê³µì‹ ì‚¬ì´íŠ¸ vs ê¸°íƒ€ ì‚¬ì´íŠ¸ êµ¬ë¶„
                is_official = any(domain in url for domain in [".gov", ".fda.gov", ".usda.gov", ".epa.gov", ".fcc.gov", ".cbp.gov", ".cpsc.gov"])
                source_type = "ê³µì‹ ì‚¬ì´íŠ¸" if is_official else "ê¸°íƒ€ ì‚¬ì´íŠ¸"
                
                # ì‹ ë¢°ë„ ê³„ì‚° (ê³µì‹ ì‚¬ì´íŠ¸ëŠ” ë†’ì€ ì ìˆ˜)
                confidence = score * (1.2 if is_official else 0.8)
                
                # ì¹´í…Œê³ ë¦¬ë³„ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
                if category == "basic_requirements":
                    # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­: import, requirements, regulations ë“±ì´ í¬í•¨ëœ ê²½ìš°
                    if any(keyword in content.lower() for keyword in ["import", "requirements", "regulations", "compliance", "standards"]):
                        extracted_requirements["certifications"].append({
                            "name": f"{agency} ìˆ˜ì… ìš”êµ¬ì‚¬í•­ ({title[:50]}...)",
                            "required": True,
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ìˆ˜ì… ìš”êµ¬ì‚¬í•­",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "detailed_regulations":
                    if any(keyword in content.lower() for keyword in ["regulation", "standard", "limit", "restriction"]):
                        extracted_requirements["detailed_regulations"].append({
                            "name": f"{agency} ì„¸ë¶€ ê·œì • ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ì„¸ë¶€ ê·œì •",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "testing_procedures":
                    if any(keyword in content.lower() for keyword in ["test", "inspection", "procedure", "authorization"]):
                        extracted_requirements["testing_procedures"].append({
                            "name": f"{agency} ê²€ì‚¬ ì ˆì°¨ ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ê²€ì‚¬ ì ˆì°¨",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "penalties_enforcement":
                    if any(keyword in content.lower() for keyword in ["penalty", "enforcement", "violation", "fine"]):
                        extracted_requirements["penalties_enforcement"].append({
                            "name": f"{agency} ì²˜ë²Œ ì •ë³´ ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ì²˜ë²Œ ì •ë³´",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "validity_periods":
                    if any(keyword in content.lower() for keyword in ["validity", "renewal", "duration", "period"]):
                        extracted_requirements["validity_periods"].append({
                            "name": f"{agency} ìœ íš¨ê¸°ê°„ ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ìœ íš¨ê¸°ê°„ ì •ë³´",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                extracted_requirements["sources"].append({
                    "title": title,
                    "url": url,
                    "type": source_type,
                    "relevance": "high" if confidence > 0.7 else "medium" if confidence > 0.5 else "low",
                    "agency": agency,
                    "category": category
                })
        
        return extracted_requirements

    def _combine_search_results(self, api_results: Dict[str, Any], web_results: Dict[str, Any]) -> Dict[str, Any]:
        """APIì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
        web_requirements = self._extract_requirements_from_web_results(web_results)
        
        combined = {
            "certifications": [],
            "documents": [],
            "sources": [],
            "detailed_regulations": [],
            "testing_procedures": [],
            "penalties_enforcement": [],
            "validity_periods": [],
            "total_requirements": 0,
            "total_certifications": 0,
            "total_documents": 0,
            "agencies_found": [],
            "category_stats": {
                "basic_requirements": 0,
                "detailed_regulations": 0,
                "testing_procedures": 0,
                "penalties_enforcement": 0,
                "validity_periods": 0
            },
            "search_sources": {
                "api_success": "agencies" in api_results and "error" not in api_results,
                "web_success": len(web_results) > 0 and "error" not in web_results
            }
        }
        
        # API ê²°ê³¼ í†µí•©
        if "agencies" in api_results and "error" not in api_results:
            agencies = api_results.get("agencies", {})
            for agency, data in agencies.items():
                if data.get("status") == "success":
                    combined["certifications"].extend(data.get("certifications", []))
                    combined["documents"].extend(data.get("documents", []))
                    combined["sources"].extend(data.get("sources", []))
                    combined["agencies_found"].append(agency)
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ í†µí•© (ìƒˆë¡œìš´ ì¶”ì¶œ ë¡œì§ ì‚¬ìš©)
        combined["certifications"].extend(web_requirements["certifications"])
        combined["documents"].extend(web_requirements["documents"])
        combined["sources"].extend(web_requirements["sources"])
        combined["detailed_regulations"].extend(web_requirements["detailed_regulations"])
        combined["testing_procedures"].extend(web_requirements["testing_procedures"])
        combined["penalties_enforcement"].extend(web_requirements["penalties_enforcement"])
        combined["validity_periods"].extend(web_requirements["validity_periods"])
        
        # ì›¹ ê²€ìƒ‰ì—ì„œ ì°¾ì€ ê¸°ê´€ë“¤ ì¶”ê°€
        web_agencies = set()
        for source in web_requirements["sources"]:
            agency = source.get("agency", "Unknown")
            if agency != "Unknown":
                web_agencies.add(agency)
        
        for agency in web_agencies:
            if agency not in combined["agencies_found"]:
                combined["agencies_found"].append(agency)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ê³„ì‚°
        combined["category_stats"]["basic_requirements"] = len(web_requirements["certifications"])
        combined["category_stats"]["detailed_regulations"] = len(web_requirements["detailed_regulations"])
        combined["category_stats"]["testing_procedures"] = len(web_requirements["testing_procedures"])
        combined["category_stats"]["penalties_enforcement"] = len(web_requirements["penalties_enforcement"])
        combined["category_stats"]["validity_periods"] = len(web_requirements["validity_periods"])
        
        # í†µê³„ ê³„ì‚°
        combined["total_certifications"] = len(combined["certifications"])
        combined["total_documents"] = len(combined["documents"])
        combined["total_requirements"] = combined["total_certifications"] + combined["total_documents"]
        
        return combined
