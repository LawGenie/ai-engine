"""
LangGraph Tools for Requirements Analysis
íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë„êµ¬ë“¤
"""

from typing import Dict, Any, List, Optional
import asyncio
import httpx
from pathlib import Path
import json
try:
    from pypdf import PdfReader
    HAS_PYPDF = True
except ImportError:
    print("âš ï¸ pypdf íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ PDF ì½ê¸° ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    PdfReader = None
    HAS_PYPDF = False
from io import BytesIO
from datetime import datetime
import importlib.util
import sys
from abc import ABC, abstractmethod
from app.services.requirements.tavily_search import TavilySearchService
from app.services.requirements.web_scraper import WebScraper
from app.services.requirements.data_gov_api import DataGovAPIService
from app.services.requirements.backend_api_service import get_backend_service
from app.services.requirements.hs_code_agency_ai_mapper import get_hs_code_mapper
from app.services.requirements.env_manager import env_manager


class SearchProvider(ABC):
    """ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì¶”ìƒí™” í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ì‹¤í–‰"""
        pass
    
    @property
    @abstractmethod
    def provider_name(self) -> str:
        """í”„ë¡œë°”ì´ë” ì´ë¦„"""
        pass


class TavilyProvider(SearchProvider):
    """Tavily ê²€ìƒ‰ í”„ë¡œë°”ì´ë”"""
    
    def __init__(self):
        self.service = TavilySearchService()
    
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        try:
            results = await self.service.search(query, **kwargs)
            return results if results else []
        except Exception as e:
            print(f"âŒ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    @property
    def provider_name(self) -> str:
        return "tavily"


class DisabledProvider(SearchProvider):
    """ê²€ìƒ‰ ë¹„í™œì„±í™” í”„ë¡œë°”ì´ë” (Tavily 432 ì—ëŸ¬ ì‹œ ì‚¬ìš©)"""
    
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        print(f"ğŸ”‡ ê²€ìƒ‰ ë¹„í™œì„±í™” ëª¨ë“œ: '{query}' ìŠ¤í‚µë¨")
        return []
    
    @property
    def provider_name(self) -> str:
        return "disabled"


class RequirementsTools:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ LangGraph ë„êµ¬ë“¤"""
    
    def __init__(self, search_provider: Optional[SearchProvider] = None):
        # í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ìë¥¼ í†µí•œ ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì„¤ì •
        provider_config = env_manager.get_search_provider_config()
        
        if provider_config['provider'] == "disabled" or not provider_config['is_available']:
            self.search_provider = DisabledProvider()
            print(f"ğŸ”‡ ê²€ìƒ‰ í”„ë¡œë°”ì´ë”: {provider_config['provider']} (API í‚¤ ì—†ìŒ)")
        else:
            self.search_provider = TavilyProvider()
            print(f"âœ… ê²€ìƒ‰ í”„ë¡œë°”ì´ë”: {provider_config['provider']} (API í‚¤ ìˆìŒ)")
        
        # ì™¸ë¶€ì—ì„œ ì œê³µëœ í”„ë¡œë°”ì´ë”ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if search_provider:
            self.search_provider = search_provider
            
        # HS ì½”ë“œ ê¸°ë°˜ ê¸°ê´€ ë§¤í•‘
        self.hs_code_agency_mapping = self._build_hs_code_mapping()
            
        # API í‚¤ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            self.web_scraper = WebScraper()
        except Exception as e:
            print(f"âš ï¸ WebScraper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.web_scraper = None
        
        try:
            self.data_gov_api = DataGovAPIService()
        except Exception as e:
            print(f"âš ï¸ DataGovAPIService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.data_gov_api = None
        
        # ë°±ì—”ë“œ API ì„œë¹„ìŠ¤ (ìƒˆë¡œìš´ í†µí•© ë°©ì‹)
        try:
            self.backend_api = get_backend_service()
        except Exception as e:
            print(f"âš ï¸ BackendAPIService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.backend_api = None
        
        try:
            self.precedent_collector = self._init_cbp_collector()
        except Exception as e:
            print(f"âš ï¸ CBP Collector ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.precedent_collector = None
        
        self.references_store_path = Path("reference_links.json")
        
        # API ìƒíƒœ ë¡œê¹…
        api_status = env_manager.get_api_status_summary()
        print(f"ğŸ“Š API ìƒíƒœ ìš”ì•½: {api_status['available_api_keys']}/{api_status['total_api_keys']}ê°œ í‚¤ ì‚¬ìš© ê°€ëŠ¥")
        if api_status['missing_keys']:
            print(f"âš ï¸ ëˆ„ë½ëœ API í‚¤: {', '.join(api_status['missing_keys'])}")
    
    def get_api_status(self) -> Dict[str, Any]:
        """API í‚¤ ìƒíƒœ ë°˜í™˜"""
        return env_manager.get_api_status_summary()
    
    def validate_dependencies(self) -> Dict[str, bool]:
        """í•„ìˆ˜ ì˜ì¡´ì„± ê²€ì¦"""
        validation = {
            'search_provider': self.search_provider.provider_name != 'disabled',
            'web_scraper': self.web_scraper is not None,
            'data_gov_api': self.data_gov_api is not None,
            'cbp_collector': self.precedent_collector is not None
        }
        return validation
    def _build_hs_code_mapping(self) -> Dict[str, Dict[str, Any]]:
        """HS ì½”ë“œ ê¸°ë°˜ ì •ë¶€ê¸°ê´€ ë§¤í•‘ êµ¬ì¶•"""
        return {
            # í™”ì¥í’ˆ ë° ë¯¸ìš© ì œí’ˆ (33xx)
                    "3304": {
                        "primary_agencies": ["FDA", "CPSC"],
                        "secondary_agencies": ["FTC"],
                        "search_keywords": ["cosmetic", "skincare", "beauty", "serum", "cream"],
                        "requirements": ["cosmetic registration", "ingredient safety", "labeling compliance", "consumer safety"]
                    },
            "3307": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["DOT"],  # ìš´ì†¡ ê´€ë ¨ (ì•Œì½”ì˜¬ í•¨ìœ )
                "search_keywords": ["perfume", "toilet water", "fragrance", "alcohol"],
                "requirements": ["cosmetic registration", "alcohol content", "shipping requirements"]
            },
            
            # ì‹í’ˆ ë° ê±´ê°•ë³´ì¡°ì‹í’ˆ (21xx, 19xx, 20xx)
            "2106": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["dietary supplement", "ginseng", "extract", "health"],
                "requirements": ["prior notice", "DSHEA compliance", "cGMP", "health claims"]
            },
            "1904": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["rice", "cereal", "prepared food", "instant"],
                "requirements": ["prior notice", "nutritional labeling", "allergen declaration"]
            },
            "1905": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["snack", "cracker", "cookie", "baker"],
                "requirements": ["prior notice", "nutritional labeling", "FALCPA", "inspection"]
            },
            "1902": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["pasta", "noodle", "instant", "ramen"],
                "requirements": ["prior notice", "nutritional labeling", "allergen", "sodium"]
            },
            "2005": {
                "primary_agencies": ["FDA"],
                "secondary_agencies": ["USDA"],
                "search_keywords": ["vegetable", "kimchi", "fermented", "preserved"],
                "requirements": ["prior notice", "HARPC", "acidified foods", "refrigeration"]
            },
            
            # ì „ìì œí’ˆ ë° í†µì‹  (84xx, 85xx)
            "8471": {
                "primary_agencies": ["FCC"],
                "secondary_agencies": ["CPSC"],
                "search_keywords": ["computer", "electronic", "device", "equipment"],
                "requirements": ["device authorization", "EMC", "safety standards"]
            },
            "8517": {
                "primary_agencies": ["FCC"],
                "secondary_agencies": ["CPSC"],
                "search_keywords": ["telephone", "communication", "wireless", "radio"],
                "requirements": ["equipment authorization", "radio frequency", "EMC"]
            },
            
            # ì˜ë¥˜ ë° ì„¬ìœ  (61xx, 62xx)
            "6109": {
                "primary_agencies": ["CPSC"],
                "secondary_agencies": ["FTC"],
                "search_keywords": ["t-shirt", "clothing", "textile", "garment"],
                "requirements": ["flammability", "care labeling", "fiber content"]
            },
            
            # ì¥ë‚œê° ë° ì–´ë¦°ì´ ì œí’ˆ (95xx)
            "9503": {
                "primary_agencies": ["CPSC"],
                "secondary_agencies": ["FDA"],
                "search_keywords": ["toy", "children", "play", "game"],
                "requirements": ["safety standards", "lead content", "small parts", "age grading"]
            }
        }

    async def _get_target_agencies_for_hs_code(self, hs_code: str, product_name: str = "") -> Dict[str, Any]:
        """
        HS ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ê²Ÿ ê¸°ê´€ ë° ê²€ìƒ‰ ì „ëµ ë°˜í™˜
        
        ìš°ì„ ìˆœìœ„:
        1. í•˜ë“œì½”ë”© ë§¤í•‘ (ë¹ ë¦„, ì‹ ë¢°ë„ ë†’ìŒ)
        2. ë°±ì—”ë“œ DB ì¡°íšŒ (ìºì‹œëœ AI ë§¤í•‘)
        3. AI ìƒì„± ë§¤í•‘ (ìƒˆë¡œìš´ HS ì½”ë“œ)
        4. ê¸°ë³¸ ë§¤í•‘ (ëª¨ë“  ê¸°ê´€)
        """
        # HS ì½”ë“œì—ì„œ 4ìë¦¬ ì½”ë“œ ì¶”ì¶œ
        hs_4digit = hs_code.split('.')[0] if '.' in hs_code else hs_code[:4]
        
        # 1. í•˜ë“œì½”ë”© ë§¤í•‘ í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
        mapping = self.hs_code_agency_mapping.get(hs_4digit, {})
        
        if mapping:
            print(f"âœ… í•˜ë“œì½”ë”© ë§¤í•‘ ì‚¬ìš© - HS: {hs_code}")
            return {
                **mapping,
                "confidence": 0.9,
                "source": "hardcoded"
            }
        
        # 2. ë°±ì—”ë“œ DBì—ì„œ AI ë§¤í•‘ ì¡°íšŒ ë˜ëŠ” ìƒì„±
        try:
            if self.backend_api:
                ai_mapping = await self._get_or_generate_ai_mapping(hs_code, product_name)
                if ai_mapping and ai_mapping.get("primary_agencies"):
                    print(f"âœ… AI ë§¤í•‘ ì‚¬ìš© - HS: {hs_code}, ì‹ ë¢°ë„: {ai_mapping.get('confidence', 0):.2f}")
                    return ai_mapping
        except Exception as e:
            print(f"âš ï¸ AI ë§¤í•‘ ì¡°íšŒ/ìƒì„± ì‹¤íŒ¨: {e}")
        
        # 3. ê¸°ë³¸ ë§¤í•‘ (HS ì½”ë“œ ì±•í„°ë³„ ì¶”ë¡ )
        hs_chapter = hs_4digit[:2]  # HS ì½”ë“œ ì• 2ìë¦¬ (ì±•í„°)
        
        # HS ì±•í„°ë³„ ê¸°ë³¸ ê¸°ê´€ ì¶”ë¡ 
        if hs_chapter in ["01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24"]:
            # ë†ì‹í’ˆ (01-24ì¥)
            default_agencies = ["FDA", "USDA"]
        elif hs_chapter in ["28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38"]:
            # í™”í•™ì œí’ˆ (28-38ì¥)
            default_agencies = ["FDA", "EPA"]
        elif hs_chapter in ["84", "85", "90"]:
            # ì „ê¸°ì „ì (84, 85, 90ì¥)
            default_agencies = ["FCC", "EPA"]
        elif hs_chapter in ["94", "95"]:
            # ê°€êµ¬, ì™„êµ¬ (94, 95ì¥)
            default_agencies = ["CPSC"]
        else:
            # ê¸°íƒ€ - ìµœì†Œ 3ê°œ ê¸°ê´€
            default_agencies = ["FDA", "EPA", "CBP"]
        
        print(f"âš ï¸ HS ì½”ë“œ {hs_code} ë§¤í•‘ ì—†ìŒ - ì±•í„° {hs_chapter} ê¸°ë°˜ ì¶”ë¡ : {default_agencies}")
        return {
            "primary_agencies": default_agencies,
            "secondary_agencies": [],
            "search_keywords": [],
            "requirements": [],
            "confidence": 0.4,  # ë‚®ì€ ì‹ ë¢°ë„
            "source": "chapter_based_inference"
        }
    
    async def _get_or_generate_ai_mapping(self, hs_code: str, product_name: str) -> Optional[Dict[str, Any]]:
        """ë°±ì—”ë“œì—ì„œ AI ë§¤í•‘ ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
        try:
            import httpx
            
            # ë°±ì—”ë“œ API í˜¸ì¶œ (AI Engineì„ í†µí•´ ìƒì„±í•˜ê³  DBì— ì €ì¥)
            url = f"{self.backend_api.base_url}/api/hs-code-agency-mappings/search"
            params = {"hsCode": hs_code}
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    # DBì— ìˆìœ¼ë©´ ë°˜í™˜
                    if data:
                        return self._parse_backend_mapping(data)
                
                # DBì— ì—†ìœ¼ë©´ AIë¡œ ìƒì„± ìš”ì²­
                print(f"ğŸ¤– AI ë§¤í•‘ ìƒì„± ìš”ì²­ - HS: {hs_code}")
                
                # ë°±ì—”ë“œê°€ AI Engineì„ í˜¸ì¶œí•˜ì—¬ ìƒì„±í•˜ë„ë¡ ìš”ì²­
                generate_url = f"{self.backend_api.base_url}/api/hs-code-agency-mappings/generate"
                generate_data = {
                    "hsCode": hs_code,
                    "productName": product_name,
                    "productCategory": ""
                }
                
                response = await client.post(generate_url, json=generate_data)
                
                if response.status_code in [200, 201]:
                    data = response.json()
                    return self._parse_backend_mapping(data)
                    
        except Exception as e:
            print(f"âš ï¸ ë°±ì—”ë“œ ë§¤í•‘ ì¡°íšŒ/ìƒì„± ì‹¤íŒ¨: {e}")
        
        return None
    
    def _parse_backend_mapping(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°±ì—”ë“œ ë§¤í•‘ ë°ì´í„° íŒŒì‹±"""
        try:
            import json
            
            agencies_json = data.get("recommendedAgencies", "{}")
            if isinstance(agencies_json, str):
                agencies_data = json.loads(agencies_json)
            else:
                agencies_data = agencies_json
            
            return {
                "primary_agencies": agencies_data.get("primary_agencies", []),
                "secondary_agencies": agencies_data.get("secondary_agencies", []),
                "search_keywords": agencies_data.get("search_keywords", []),
                "requirements": agencies_data.get("key_requirements", []),
                "confidence": float(data.get("confidenceScore", 0.5)),
                "source": "ai_generated"
            }
        except Exception as e:
            print(f"âš ï¸ ë°±ì—”ë“œ ë§¤í•‘ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {}

    def _extract_keywords_from_product(self, product_name: str, product_description: str = "") -> List[str]:
        """ìƒí’ˆëª…ê³¼ ì„¤ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ìƒí’ˆëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        name_keywords = [
            "vitamin", "serum", "cream", "extract", "ginseng", "rice", "noodle", 
            "kimchi", "snack", "perfume", "cosmetic", "supplement", "food",
            "electronic", "device", "toy", "clothing", "textile"
        ]
        
        product_text = f"{product_name} {product_description}".lower()
        for keyword in name_keywords:
            if keyword in product_text:
                keywords.append(keyword)
        
        # ìƒí’ˆëª…ì—ì„œ ì§ì ‘ ì¶”ì¶œ
        words = product_name.lower().split()
        for word in words:
            if len(word) > 3 and word not in ["premium", "korean", "instant", "pack"]:
                keywords.append(word)
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°

    def _build_hs_code_based_queries(self, product_name: str, hs_code: str, target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """HS ì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = {}
        
        # ì£¼ìš” ê¸°ê´€ë³„ ê²€ìƒ‰ (HS ì½”ë“œ ê¸°ë°˜)
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ê²€ìƒ‰
            queries[f"{agency}_hs_requirements"] = f"site:{agency_lower}.gov import requirements {product_name} HS {hs_code}"
            
            # ì„¸ë¶€ ê·œì • ê²€ìƒ‰ (ê¸°ê´€ë³„ íŠ¹í™”)
            if agency == "FDA":
                if "cosmetic" in target_agencies.get("search_keywords", []):
                    queries[f"{agency}_hs_cosmetic"] = f"site:{agency_lower}.gov cosmetic regulations HS {hs_code} ingredient safety"
                if "food" in target_agencies.get("search_keywords", []):
                    queries[f"{agency}_hs_food"] = f"site:{agency_lower}.gov food import requirements HS {hs_code} prior notice"
                if "supplement" in target_agencies.get("search_keywords", []):
                    queries[f"{agency}_hs_supplement"] = f"site:{agency_lower}.gov dietary supplement requirements HS {hs_code} DSHEA"
            
            elif agency == "USDA":
                queries[f"{agency}_hs_agricultural"] = f"site:{agency_lower}.gov agricultural import requirements HS {hs_code}"
                queries[f"{agency}_hs_organic"] = f"site:{agency_lower}.gov organic certification HS {hs_code}"
            
            elif agency == "EPA":
                queries[f"{agency}_hs_chemical"] = f"site:{agency_lower}.gov chemical regulations HS {hs_code}"
                queries[f"{agency}_hs_environmental"] = f"site:{agency_lower}.gov environmental standards HS {hs_code}"
            
            elif agency == "FCC":
                queries[f"{agency}_hs_device"] = f"site:{agency_lower}.gov device authorization HS {hs_code}"
                queries[f"{agency}_hs_emc"] = f"site:{agency_lower}.gov EMC electromagnetic compatibility HS {hs_code}"
            
            elif agency == "CPSC":
                queries[f"{agency}_hs_safety"] = f"site:{agency_lower}.gov safety standards HS {hs_code}"
                queries[f"{agency}_hs_recall"] = f"site:{agency_lower}.gov recall information HS {hs_code}"
        
        return queries

    def _build_keyword_based_queries(self, product_name: str, keywords: List[str], target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = {}
        
        # ì£¼ìš” ê¸°ê´€ë³„ í‚¤ì›Œë“œ ê²€ìƒ‰
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                # í‚¤ì›Œë“œë³„ íŠ¹í™” ê²€ìƒ‰
                if keyword in ["vitamin", "serum", "cream", "cosmetic"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov cosmetic regulations {keyword} import requirements"
                elif keyword in ["ginseng", "extract", "supplement"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov dietary supplement {keyword} import requirements"
                elif keyword in ["rice", "noodle", "kimchi", "food"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov food import requirements {keyword}"
                elif keyword in ["electronic", "device"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov device authorization {keyword}"
                elif keyword in ["toy", "clothing", "textile"]:
                    queries[f"{agency}_kw_{keyword}"] = f"site:{agency_lower}.gov safety standards {keyword}"
        
        return queries

    def _build_fullname_queries(self, product_name: str, hs_code: str, target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """ìƒí’ˆëª… ì „ì²´ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (3ë‹¨ê³„)"""
        queries = {}
        
        # ì£¼ìš” ê¸°ê´€ë³„ ìƒí’ˆëª… ì „ì²´ ê²€ìƒ‰
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            # ìƒí’ˆëª… ì „ì²´ë¡œ í¬ê´„ì  ê²€ìƒ‰
            queries[f"{agency}_fullname_import"] = f"site:{agency_lower}.gov \"{product_name}\" import requirements"
            queries[f"{agency}_fullname_regulations"] = f"site:{agency_lower}.gov \"{product_name}\" regulations compliance"
        
        return queries
    
    def _build_phase_specific_queries(self, product_name: str, hs_code: str, target_agencies: Dict[str, Any]) -> Dict[str, str]:
        """Phase 2-4 ì „ìš© ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = {}
        
        for agency in target_agencies.get("primary_agencies", []):
            agency_lower = agency.lower()
            
            # Phase 2: ê²€ì‚¬ ì ˆì°¨ ë° ë°©ë²•
            queries[f"{agency}_phase2_testing"] = f"site:{agency_lower}.gov testing procedures {product_name} HS {hs_code}"
            queries[f"{agency}_phase2_inspection"] = f"site:{agency_lower}.gov inspection methods {product_name} HS {hs_code}"
            queries[f"{agency}_phase2_authorization"] = f"site:{agency_lower}.gov authorization procedures {product_name} HS {hs_code}"
            
            # Phase 3: ì²˜ë²Œ ë° ë²Œê¸ˆ ì •ë³´
            queries[f"{agency}_phase3_penalties"] = f"site:{agency_lower}.gov penalties violations {product_name} HS {hs_code}"
            queries[f"{agency}_phase3_enforcement"] = f"site:{agency_lower}.gov enforcement actions {product_name} HS {hs_code}"
            queries[f"{agency}_phase3_fines"] = f"site:{agency_lower}.gov civil penalties {product_name} HS {hs_code}"
            
            # Phase 4: ìœ íš¨ê¸°ê°„ ë° ê°±ì‹  ì •ë³´
            queries[f"{agency}_phase4_validity"] = f"site:{agency_lower}.gov certificate validity period {product_name} HS {hs_code}"
            queries[f"{agency}_phase4_renewal"] = f"site:{agency_lower}.gov certification renewal {product_name} HS {hs_code}"
            queries[f"{agency}_phase4_duration"] = f"site:{agency_lower}.gov permit duration {product_name} HS {hs_code}"
        
        return queries

    def _init_cbp_collector(self):
        """precedents-analysis/cbp_scraper.pyì˜ CBPDataCollectorë¥¼ ë™ì  ë¡œë“œí•œë‹¤."""
        try:
            base_dir = Path(__file__).resolve().parents[1]  # ai-engine/app
            project_root = base_dir.parent  # ai-engine
            target_path = project_root / "precedents-analysis" / "cbp_scraper.py"
            if not target_path.exists():
                return None
            spec = importlib.util.spec_from_file_location("cbp_scraper", str(target_path))
            if spec is None or spec.loader is None:
                return None
            module = importlib.util.module_from_spec(spec)
            sys.modules["cbp_scraper"] = module
            spec.loader.exec_module(module)
            if hasattr(module, "CBPDataCollector"):
                return module.CBPDataCollector()
        except Exception:
            return None
        return None

        
        # ê¸°ê´€ë³„ ë„ë©”ì¸ ë§¤í•‘
        self.agency_domains = {
            "FDA": "fda.gov",
            "FCC": "fcc.gov", 
            "CBP": "cbp.gov",
            "USDA": "usda.gov",
            "EPA": "epa.gov",
            "CPSC": "cpsc.gov",
            "KCS": "customs.go.kr",  # í•œêµ­ ê´€ì„¸ì²­
            "MFDS": "mfds.go.kr",    # ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜
            "MOTIE": "motie.go.kr"   # ì‚°ì—…í†µìƒìì›ë¶€
        }
    
    async def search_agency_documents(self, agency: str, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (í†µí•©)"""
        print(f"ğŸ”§ [TOOL] {agency} ë¬¸ì„œ ê²€ìƒ‰: {query}")
        
        results = await self.search_provider.search(query, max_results=max_results)
        
        # ê¸°ê´€ë³„ ë„ë©”ì¸ í•„í„°ë§
        agency_domain = self.agency_domains.get(agency, "")
        agency_results = []
        
        for result in results:
            url = result.get("url", "")
            if agency_domain in url:
                agency_results.append(result)
                print(f"  âœ… {agency} ê³µì‹ ë¬¸ì„œ ë°œê²¬: {result.get('title', 'No title')}")
            else:
                print(f"  âŒ {agency} ì™¸ë¶€ ë¬¸ì„œ ì œì™¸: {result.get('title', 'No title')}")
        
        return {
            "agency": agency,
            "query": query,
            "total_results": len(results),
            "agency_results": agency_results,
            "selected_url": agency_results[0]["url"] if agency_results else None,
            "domain": agency_domain
        }
    
    async def search_fda_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """FDA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.search_agency_documents("FDA", query, max_results)
    
    # ë¯¸êµ­ ì •ë¶€ ê¸°ê´€ë“¤
    async def search_fcc_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """FCC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("FCC", query, max_results)
    
    async def search_cbp_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """CBP ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("CBP", query, max_results)
    
    async def search_usda_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """USDA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("USDA", query, max_results)
    
    async def search_epa_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """EPA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("EPA", query, max_results)
    
    async def search_cpsc_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """CPSC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("CPSC", query, max_results)
    
    # í•œêµ­ ì •ë¶€ ê¸°ê´€ë“¤
    async def search_kcs_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """í•œêµ­ ê´€ì„¸ì²­ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("KCS", query, max_results)
    
    async def search_mfds_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("MFDS", query, max_results)
    
    async def search_motie_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ì‚°ì—…í†µìƒìì›ë¶€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("MOTIE", query, max_results)
    
    async def scrape_document(self, agency: str, url: str, hs_code: str) -> Dict[str, Any]:
        """íŠ¹ì • ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ë„êµ¬ (í™•ì¥)"""
        print(f"ğŸ”§ [TOOL] {agency} ë¬¸ì„œ ìŠ¤í¬ë˜í•‘")
        print(f"  URL: {url}")
        print(f"  HSì½”ë“œ: {hs_code}")
        
        # WebScraperê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.web_scraper:
            print(f"  âŒ WebScraperê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return {
                "agency": agency,
                "error": "WebScraper not initialized",
                "certifications": [],
                "documents": [],
                "sources": []
            }
        
        try:
            # ê¸°ê´€ë³„ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œ ë§¤í•‘
            scraper_methods = {
                "FDA": "scrape_fda_requirements",
                "FCC": "scrape_fcc_requirements", 
                "CBP": "scrape_cbp_requirements",
                "USDA": "scrape_usda_requirements",
                "EPA": "scrape_epa_requirements",
                "CPSC": "scrape_cpsc_requirements",
                "KCS": "scrape_kcs_requirements",
                "MFDS": "scrape_mfds_requirements",
                "MOTIE": "scrape_motie_requirements"
            }
            
            method_name = scraper_methods.get(agency)
            if not method_name:
                return {"error": f"Unknown agency: {agency}"}
            
            # ë™ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œ í˜¸ì¶œ
            scraper_method = getattr(self.web_scraper, method_name, None)
            if not scraper_method:
                return {"error": f"Scraper method not implemented for {agency}"}
            
            result = await scraper_method(hs_code, url)
            
            # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            certs = result.get("certifications", [])
            docs = result.get("documents", [])
            sources = result.get("sources", [])
            
            print(f"  âœ… {agency} ìŠ¤í¬ë˜í•‘ ì„±ê³µ:")
            print(f"    ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ")
            for i, cert in enumerate(certs, 1):
                print(f"      {i}. {cert.get('name', 'Unknown')} ({cert.get('agency', 'Unknown')})")
                print(f"         ì„¤ëª…: {cert.get('description', 'No description')}")
            
            print(f"    ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ")
            for i, doc in enumerate(docs, 1):
                print(f"      {i}. {doc.get('name', 'Unknown')}")
                print(f"         ì„¤ëª…: {doc.get('description', 'No description')}")
            
            print(f"    ğŸ“š ì¶œì²˜: {len(sources)}ê°œ")
            for i, source in enumerate(sources, 1):
                print(f"      {i}. {source.get('title', 'Unknown')} ({source.get('type', 'Unknown')})")
            
            return result
            
        except Exception as e:
            print(f"  âŒ {agency} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                "agency": agency,
                "error": str(e),
                "certifications": [],
                "documents": [],
                "sources": []
            }

    async def get_cbp_precedents(self, hs_code: str) -> Dict[str, Any]:
        """CBP íŒë¡€/ê²°ì • ì‚¬ë¡€ ì¡°íšŒ ë„êµ¬."""
        try:
            if not self.precedent_collector:
                return {"hs_code": hs_code, "count": 0, "precedents": [], "error": "cbp_collector_not_available"}
            precedents = await self.precedent_collector.get_precedents_by_hs_code(hs_code)
            return {
                "hs_code": hs_code,
                "count": len(precedents),
                "precedents": precedents
            }
        except Exception as e:
            return {"hs_code": hs_code, "count": 0, "precedents": [], "error": str(e)}

    async def summarize_pdf(self, url: str, max_pages: int = 5) -> Dict[str, Any]:
        """PDF ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì•ë¶€ë¶„ì„ ìš”ì•½(ë°œì·Œ)í•œë‹¤."""
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                resp = await client.get(url)
                resp.raise_for_status()
                data = BytesIO(resp.content)
                reader = PdfReader(data)
                num_pages = min(len(reader.pages), max_pages)
                text_chunks: List[str] = []
                for i in range(num_pages):
                    try:
                        text_chunks.append(reader.pages[i].extract_text() or "")
                    except Exception:
                        continue
                combined = "\n".join([t.strip() for t in text_chunks if t and t.strip()])
                preview = (combined[:1200] + "â€¦") if len(combined) > 1200 else combined
                return {
                    "url": url,
                    "pages_read": num_pages,
                    "excerpt": preview,
                    "char_count": len(preview)
                }
        except Exception as e:
            return {"url": url, "error": str(e)}

    def save_reference_links(self, hs_code: str, product_name: str, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ëœ ì°¸ê³  ë§í¬ë“¤ì„ ë¡œì»¬ JSONì— ì €ì¥/ë³‘í•©í•œë‹¤."""
        try:
            existing: Dict[str, Any] = {}
            if self.references_store_path.exists():
                existing = json.loads(self.references_store_path.read_text(encoding="utf-8"))
            key = f"{hs_code}:{product_name}"
            payload = {
                "hs_code": hs_code,
                "product_name": product_name,
                "saved_at": datetime.utcnow().isoformat() + "Z",
                "agencies": {}
            }
            for k, v in search_results.items():
                agency = v.get("agency") or k
                urls = v.get("urls", [])
                payload["agencies"].setdefault(agency, {"urls": []})
                # ë³‘í•©
                payload["agencies"][agency]["urls"] = list({*payload["agencies"][agency]["urls"], *urls})
            existing[key] = payload
            self.references_store_path.write_text(json.dumps(existing, ensure_ascii=False, indent=2), encoding="utf-8")
            return {"saved": True, "reference_key": key, "agencies": list(payload["agencies"].keys())}
        except Exception as e:
            return {"saved": False, "error": str(e)}
    
    async def analyze_requirements(self, requirements_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë„êµ¬ (í™•ì¥)"""
        print(f"ğŸ”§ [TOOL] ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹œì‘")
        
        certifications = requirements_data.get("certifications", [])
        documents = requirements_data.get("documents", [])
        sources = requirements_data.get("sources", [])
        
        # ê¸°ë³¸ ë¶„ì„ í†µê³„
        total_certs = len(certifications)
        total_docs = len(documents)
        total_sources = len(sources)
        
        # ê¸°ê´€ë³„ í†µê³„
        agency_stats = {}
        for cert in certifications:
            agency = cert.get("agency", "Unknown")
            agency_stats[agency] = agency_stats.get(agency, 0) + 1
        
        # ìš°ì„ ìˆœìœ„ ë¶„ì„
        high_priority = [c for c in certifications if c.get("priority") == "high"]
        required_docs = [d for d in documents if d.get("required", False)]
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        completeness_score = min(1.0, (total_certs + total_docs) / 10)  # 0-1 ìŠ¤ì¼€ì¼
        coverage_ratio = len(agency_stats) / len(self.agency_domains)  # ê¸°ê´€ ì»¤ë²„ë¦¬ì§€
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_factors = []
        if total_certs > 5:
            complexity_factors.append("ë‹¤ì¤‘ ì¸ì¦ ìš”êµ¬")
        if len(agency_stats) > 3:
            complexity_factors.append("ë‹¤ê¸°ê´€ ê·œì œ")
        if any("critical" in str(cert).lower() for cert in certifications):
            complexity_factors.append("ì¤‘ìš” ì¸ì¦ ìš”êµ¬")
        
        compliance_complexity = "simple" if len(complexity_factors) == 0 else "moderate" if len(complexity_factors) <= 2 else "complex"
        
        # ë¹„ìš© ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        estimated_cost_low = total_certs * 100 + total_docs * 50  # USD
        estimated_cost_high = total_certs * 500 + total_docs * 200
        
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        risk_factors = []
        if total_certs == 0:
            risk_factors.append("ì¸ì¦ ìš”êµ¬ì‚¬í•­ ë¶ˆëª…í™•")
        if len(required_docs) > 10:
            risk_factors.append("ì„œë¥˜ ìš”êµ¬ì‚¬í•­ ê³¼ë‹¤")
        if coverage_ratio < 0.3:
            risk_factors.append("ê¸°ê´€ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±")
        
        overall_risk_level = "low" if len(risk_factors) == 0 else "medium" if len(risk_factors) <= 2 else "high"
        
        print(f"  ğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"    ğŸ“‹ ì´ ì¸ì¦ìš”ê±´: {total_certs}ê°œ")
        print(f"    ğŸ“„ ì´ í•„ìš”ì„œë¥˜: {total_docs}ê°œ")
        print(f"    ğŸ“š ì´ ì¶œì²˜: {total_sources}ê°œ")
        print(f"    ğŸ¢ ê¸°ê´€ë³„ ì¸ì¦ìš”ê±´:")
        for agency, count in agency_stats.items():
            print(f"      â€¢ {agency}: {count}ê°œ")
        print(f"    âš ï¸ ê³ ìš°ì„ ìˆœìœ„ ì¸ì¦ìš”ê±´: {len(high_priority)}ê°œ")
        print(f"    ğŸ“‹ í•„ìˆ˜ ì„œë¥˜: {len(required_docs)}ê°œ")
        print(f"    ğŸ“ˆ ì™„ì„±ë„ ì ìˆ˜: {completeness_score:.2f}")
        print(f"    ğŸ¯ ê¸°ê´€ ì»¤ë²„ë¦¬ì§€: {coverage_ratio:.2f}")
        print(f"    âš¡ ë³µì¡ë„: {compliance_complexity}")
        print(f"    ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost_low}-${estimated_cost_high}")
        print(f"    âš ï¸ ë¦¬ìŠ¤í¬ ë ˆë²¨: {overall_risk_level}")
        
        return {
            # ê¸°ë³¸ í†µê³„
            "total_certifications": total_certs,
            "total_documents": total_docs,
            "total_sources": total_sources,
            "agency_stats": agency_stats,
            "high_priority_count": len(high_priority),
            "required_docs_count": len(required_docs),
            
            # í’ˆì§ˆ ì§€í‘œ
            "quality_metrics": {
                "completeness_score": completeness_score,
                "coverage_ratio": coverage_ratio,
                "compliance_complexity": compliance_complexity,
                "complexity_factors": complexity_factors
            },
            
            # ë¹„ìš© ë¶„ì„
            "cost_analysis": {
                "estimated_cost_low": estimated_cost_low,
                "estimated_cost_high": estimated_cost_high,
                "currency": "USD"
            },
            
            # ë¦¬ìŠ¤í¬ ë¶„ì„
            "risk_analysis": {
                "overall_risk_level": overall_risk_level,
                "risk_factors": risk_factors
            },
            
            "analysis_complete": True
        }
    
    async def search_requirements_hybrid(self, hs_code: str, product_name: str, product_description: str = "") -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Backend API (ìš°ì„ ) + Tavily Search (ë³´ì¡°)"""
        print(f"\nğŸš€ [HYBRID] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {product_name}")
        
        results = {
            "hs_code": hs_code,
            "product_name": product_name,
            "search_timestamp": None,
            "api_results": {},
            "web_results": {},
            "combined_results": {},
            "search_methods": [],
            "citations": []  # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        }
        
        # 1. Backend API ê²€ìƒ‰ (ìš°ì„  - ì •ë¶€ API í†µí•© ìˆ˜ì§‘)
        try:
            print(f"\n  ğŸ” 1ë‹¨ê³„: Backend API ê²€ìƒ‰ (ì •ë¶€ API í†µí•©)")
            if not self.backend_api:
                print(f"    âš ï¸ BackendAPIServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ëŒ€ì²´ ë°©ì‹ ì‚¬ìš©")
                # ë°±ì—”ë“œ API ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                if self.data_gov_api:
                    api_results = await self.data_gov_api.search_requirements_by_hs_code(hs_code, product_name)
                    results["search_methods"].append("data_gov_api_fallback")
                else:
                    api_results = {
                        "hs_code": hs_code,
                        "product_name": product_name,
                        "error": "No API service available",
                        "total_requirements": 0,
                        "agencies": {}
                    }
            else:
                # ë°±ì—”ë“œ API í˜¸ì¶œ
                backend_response = await self.backend_api.collect_requirements(
                    product=product_name,
                    hs_code=hs_code,
                    include_raw_data=False
                )
                
                # AI ë¶„ì„ìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜
                api_results = self.backend_api.format_for_ai_analysis(backend_response)
                results["search_methods"].append("backend_api")
                
                # Citations ì¶”ì¶œ
                results["citations"] = backend_response.get("citations", [])
                print(f"    ğŸ“š ì¶œì²˜: {len(results['citations'])}ê°œ")
            
            results["api_results"] = api_results
            total_reqs = api_results.get('requirements_summary', {}).get('total', 0) or api_results.get('total_requirements', 0)
            print(f"    âœ… API ê²€ìƒ‰ ì™„ë£Œ: {total_reqs}ê°œ ìš”êµ¬ì‚¬í•­")
            
        except Exception as e:
            print(f"    âŒ API ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["api_results"] = {"error": str(e)}
        
        # 2. Tavily Search (íƒ€ê²Ÿ ê¸°ê´€ ê¸°ë°˜ ê²€ìƒ‰)
        try:
            print(f"\n  ğŸ” 2ë‹¨ê³„: Tavily Search (íƒ€ê²Ÿ ê¸°ê´€ ê¸°ë°˜)")
            
            # HS ì½”ë“œ ê¸°ë°˜ íƒ€ê²Ÿ ê¸°ê´€ ë¶„ì„ (AI ë§¤í•‘ í¬í•¨)
            target_agencies = await self._get_target_agencies_for_hs_code(hs_code, product_name)
            
            # AI ë§¤í•‘ì´ ìˆìœ¼ë©´ í•´ë‹¹ ê¸°ê´€ë§Œ ê²€ìƒ‰, ì—†ìœ¼ë©´ ì „ì²´ ê²€ìƒ‰
            if target_agencies.get("source") in ["hardcoded", "ai_generated"]:
                print(f"  âœ… íƒ€ê²Ÿ ê¸°ê´€ í™•ì • ({target_agencies.get('source')}): {target_agencies.get('primary_agencies')}")
                print(f"  ğŸ’° Tavily ê²€ìƒ‰ ìµœì í™”: íƒ€ê²Ÿ ê¸°ê´€ë§Œ ê²€ìƒ‰")
            else:
                print(f"  âš ï¸ íƒ€ê²Ÿ ê¸°ê´€ ë¶ˆëª…í™• ({target_agencies.get('source')})")
                print(f"  ğŸ’¸ Tavily ê²€ìƒ‰ í™•ì¥: ëª¨ë“  ê¸°ê´€ ê²€ìƒ‰ (ë¹„ìš© ì¦ê°€)")
            
            # ìƒí’ˆëª…/ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_from_product(product_name, product_description)
            
            # 3ë‹¨ê³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            # 1ë‹¨ê³„: HS ì½”ë“œ ê¸°ë°˜ ê²€ìƒ‰ (ê°€ì¥ ì •í™•)
            hs_queries = self._build_hs_code_based_queries(product_name, hs_code, target_agencies)
            
            # 2ë‹¨ê³„: AI í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ì •í™•ë„ ë†’ìŒ)
            keyword_queries = self._build_keyword_based_queries(product_name, keywords, target_agencies)
            
            # 3ë‹¨ê³„: ìƒí’ˆëª… ì „ì²´ ê²€ìƒ‰ (í¬ê´„ì )
            fullname_queries = self._build_fullname_queries(product_name, hs_code, target_agencies)
            
            # Phase 2-4 ì „ìš© ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            phase_queries = self._build_phase_specific_queries(product_name, hs_code, target_agencies)
            
            # ë³µí•© ê²€ìƒ‰ ì¿¼ë¦¬ ë³‘í•©
            web_queries = {**hs_queries, **keyword_queries, **fullname_queries, **phase_queries}
            
            print(f"  ğŸ“Š 3ë‹¨ê³„ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±:")
            print(f"    1ï¸âƒ£ HS ì½”ë“œ ê¸°ë°˜: {len(hs_queries)}ê°œ")
            print(f"    2ï¸âƒ£ AI í‚¤ì›Œë“œ ê¸°ë°˜: {len(keyword_queries)}ê°œ")
            print(f"    3ï¸âƒ£ ìƒí’ˆëª… ì „ì²´: {len(fullname_queries)}ê°œ")
            print(f"    â• Phase 2-4: {len(phase_queries)}ê°œ")
            
            print(f"  ğŸ¯ íƒ€ê²Ÿ ê¸°ê´€: {', '.join(target_agencies.get('primary_agencies', []))}")
            print(f"  ğŸ“Š ê²€ìƒ‰ ì‹ ë¢°ë„: {target_agencies.get('confidence', 0):.1%}")
            print(f"  ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}")
            print(f"  ğŸ” ì´ ê²€ìƒ‰ ì¿¼ë¦¬: {len(web_queries)}ê°œ (HSì½”ë“œ {len(hs_queries)}ê°œ + í‚¤ì›Œë“œ {len(keyword_queries)}ê°œ + Phase2-4 {len(phase_queries)}ê°œ)")
            
            web_results = {}
            for query_key, query in web_queries.items():
                try:
                    if self.search_provider:
                        search_results = await self.search_provider.search(query, max_results=5)
                    else:
                        print(f"    âš ï¸ ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì—†ìŒ: {query_key} ìŠ¤í‚µë¨")
                        search_results = []
                    # ê²°ê³¼ ë¶„ë¥˜ (HS ì½”ë“œ ê¸°ë°˜ + í‚¤ì›Œë“œ ê¸°ë°˜)
                    category = "basic_requirements"
                    search_type = "hs_code" if "hs_" in query_key else "keyword"
                    
                    # ì¿¼ë¦¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (Phase 1-4)
                    if any(keyword in query_key for keyword in ["cosmetic", "regulations", "standards", "limits", "restrictions", "safety"]):
                        category = "detailed_regulations"
                    elif any(keyword in query_key for keyword in ["testing", "inspection", "procedures", "authorization", "phase2"]):
                        category = "testing_procedures"
                    elif any(keyword in query_key for keyword in ["penalties", "enforcement", "violations", "recall", "phase3"]):
                        category = "penalties_enforcement"
                    elif any(keyword in query_key for keyword in ["validity", "renewal", "duration", "period", "phase4"]):
                        category = "validity_periods"
                    
                    # ê¸°ê´€ ì¶”ì¶œ
                    agency = query_key.split("_")[0].upper()
                    
                    web_results[query_key] = {
                        "query": query,
                        "results": search_results,
                        "urls": [r.get("url") for r in search_results if r.get("url")],
                        "agency": agency,
                        "category": category,
                        "search_type": search_type,
                        "result_count": len(search_results),
                        "target_confidence": target_agencies.get("confidence", 0.5)
                    }
                except Exception as e:
                    web_results[query_key] = {"error": str(e)}
            
            results["web_results"] = web_results
            results["search_methods"].append("tavily_search")
            print(f"    âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(web_results)}ê°œ ì¿¼ë¦¬")
            
        except Exception as e:
            print(f"    âŒ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["web_results"] = {"error": str(e)}
        
        # 3. ê²°ê³¼ í†µí•©
        print(f"\n  ğŸ”„ 3ë‹¨ê³„: ê²°ê³¼ í†µí•©")
        combined_results = self._combine_search_results(hs_code, results["api_results"], results["web_results"])
        combined_results["target_agencies"] = target_agencies  # íƒ€ê²Ÿ ê¸°ê´€ ì •ë³´ ì¶”ê°€
        combined_results["extracted_keywords"] = keywords  # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
        
        # Citationsë¥¼ combined_resultsì—ë„ ì¶”ê°€
        combined_results["citations"] = results["citations"]
        
        results["combined_results"] = combined_results
        
        print(f"\nâœ… [HS ì½”ë“œ + í‚¤ì›Œë“œ ë³µí•© ê²€ìƒ‰] ì™„ë£Œ")
        print(f"  ğŸ” ê²€ìƒ‰ ë°©ë²•: {', '.join(results['search_methods'])}")
        print(f"  ğŸ¯ íƒ€ê²Ÿ ê¸°ê´€: {', '.join(target_agencies.get('primary_agencies', []))}")
        print(f"  ğŸ“Š ê²€ìƒ‰ ì‹ ë¢°ë„: {target_agencies.get('confidence', 0):.1%}")
        print(f"  ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}")
        print(f"  ğŸ“š ì¶œì²˜(Citations): {len(results['citations'])}ê°œ")
        print(f"  ğŸ“‹ ì´ ìš”êµ¬ì‚¬í•­: {combined_results.get('total_requirements', 0)}ê°œ")
        print(f"  ğŸ† ì¸ì¦ìš”ê±´: {combined_results.get('total_certifications', 0)}ê°œ")
        print(f"  ğŸ“„ í•„ìš”ì„œë¥˜: {combined_results.get('total_documents', 0)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì¶œë ¥
        category_stats = combined_results.get('category_stats', {})
        print(f"  ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼:")
        print(f"    ğŸ” ê¸°ë³¸ ìš”êµ¬ì‚¬í•­: {category_stats.get('basic_requirements', 0)}ê°œ")
        print(f"    ğŸ“‹ ì„¸ë¶€ ê·œì •: {category_stats.get('detailed_regulations', 0)}ê°œ")
        print(f"    ğŸ§ª ê²€ì‚¬ ì ˆì°¨: {category_stats.get('testing_procedures', 0)}ê°œ")
        print(f"    âš–ï¸ ì²˜ë²Œ ì •ë³´: {category_stats.get('penalties_enforcement', 0)}ê°œ")
        print(f"    â° ìœ íš¨ê¸°ê°„: {category_stats.get('validity_periods', 0)}ê°œ")
        
        return results
    
    def _extract_requirements_from_web_results(self, web_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        extracted_requirements = {
            "certifications": [],
            "documents": [],
            "sources": [],
            "detailed_regulations": [],
            "testing_procedures": [],
            "penalties_enforcement": [],
            "validity_periods": []
        }
        
        for query_key, result in web_results.items():
            if "error" in result:
                continue
                
            agency = result.get("agency", "Unknown")
            category = result.get("category", "basic_requirements")
            search_results = result.get("results", [])
            
            for search_result in search_results:
                url = search_result.get("url", "")
                title = search_result.get("title", "")
                content = search_result.get("content", "")
                score = search_result.get("score", 0)
                
                # ê³µì‹ ì‚¬ì´íŠ¸ vs ê¸°íƒ€ ì‚¬ì´íŠ¸ êµ¬ë¶„
                is_official = any(domain in url for domain in [".gov", ".fda.gov", ".usda.gov", ".epa.gov", ".fcc.gov", ".cbp.gov", ".cpsc.gov"])
                source_type = "ê³µì‹ ì‚¬ì´íŠ¸" if is_official else "ê¸°íƒ€ ì‚¬ì´íŠ¸"
                
                # ì‹ ë¢°ë„ ê³„ì‚° (ê³µì‹ ì‚¬ì´íŠ¸ëŠ” ë†’ì€ ì ìˆ˜)
                confidence = score * (1.2 if is_official else 0.8)
                
                # ì¹´í…Œê³ ë¦¬ë³„ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
                if category == "basic_requirements":
                    # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­: import, requirements, regulations ë“±ì´ í¬í•¨ëœ ê²½ìš°
                    if any(keyword in content.lower() for keyword in ["import", "requirements", "regulations", "compliance", "standards"]):
                        extracted_requirements["certifications"].append({
                            "name": f"{agency} ìˆ˜ì… ìš”êµ¬ì‚¬í•­ ({title[:50]}...)",
                            "required": True,
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ìˆ˜ì… ìš”êµ¬ì‚¬í•­",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "detailed_regulations":
                    if any(keyword in content.lower() for keyword in ["regulation", "standard", "limit", "restriction"]):
                        extracted_requirements["detailed_regulations"].append({
                            "name": f"{agency} ì„¸ë¶€ ê·œì • ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ì„¸ë¶€ ê·œì •",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "testing_procedures":
                    if any(keyword in content.lower() for keyword in ["test", "inspection", "procedure", "authorization"]):
                        extracted_requirements["testing_procedures"].append({
                            "name": f"{agency} ê²€ì‚¬ ì ˆì°¨ ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ê²€ì‚¬ ì ˆì°¨",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "penalties_enforcement":
                    if any(keyword in content.lower() for keyword in ["penalty", "enforcement", "violation", "fine"]):
                        extracted_requirements["penalties_enforcement"].append({
                            "name": f"{agency} ì²˜ë²Œ ì •ë³´ ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ì²˜ë²Œ ì •ë³´",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                elif category == "validity_periods":
                    if any(keyword in content.lower() for keyword in ["validity", "renewal", "duration", "period"]):
                        extracted_requirements["validity_periods"].append({
                            "name": f"{agency} ìœ íš¨ê¸°ê°„ ({title[:50]}...)",
                            "description": f"{source_type}ì—ì„œ í™•ì¸ëœ {agency} ìœ íš¨ê¸°ê°„ ì •ë³´",
                            "agency": agency,
                            "url": url,
                            "confidence": confidence,
                            "source_type": source_type
                        })
                
                # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                extracted_requirements["sources"].append({
                    "title": title,
                    "url": url,
                    "type": source_type,
                    "relevance": "high" if confidence > 0.7 else "medium" if confidence > 0.5 else "low",
                    "agency": agency,
                    "category": category
                })
        
        return extracted_requirements

    def _combine_search_results(self, hs_code: str, api_results: Dict[str, Any], web_results: Dict[str, Any]) -> Dict[str, Any]:
        """APIì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ í†µí•© + íŒë¡€ ê¸°ë°˜ ê²€ì¦ ì£¼ì…"""
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
        web_requirements = self._extract_requirements_from_web_results(web_results)
        
        combined = {
            "certifications": [],
            "documents": [],
            "sources": [],
            "detailed_regulations": [],
            "testing_procedures": [],
            "penalties_enforcement": [],
            "validity_periods": [],
            "total_requirements": 0,
            "total_certifications": 0,
            "total_documents": 0,
            "agencies_found": [],
            "category_stats": {
                "basic_requirements": 0,
                "detailed_regulations": 0,
                "testing_procedures": 0,
                "penalties_enforcement": 0,
                "validity_periods": 0
            },
            "search_sources": {
                "api_success": "agencies" in api_results and "error" not in api_results,
                "web_success": len(web_results) > 0 and "error" not in web_results
            }
        }
        
        # API ê²°ê³¼ í†µí•©
        if "agencies" in api_results and "error" not in api_results:
            agencies = api_results.get("agencies", {})
            for agency, data in agencies.items():
                if data.get("status") == "success":
                    combined["certifications"].extend(data.get("certifications", []))
                    combined["documents"].extend(data.get("documents", []))
                    combined["sources"].extend(data.get("sources", []))
                    combined["agencies_found"].append(agency)
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ í†µí•© (ìƒˆë¡œìš´ ì¶”ì¶œ ë¡œì§ ì‚¬ìš©)
        combined["certifications"].extend(web_requirements["certifications"])
        combined["documents"].extend(web_requirements["documents"])
        combined["sources"].extend(web_requirements["sources"])
        combined["detailed_regulations"].extend(web_requirements["detailed_regulations"])
        combined["testing_procedures"].extend(web_requirements["testing_procedures"])
        combined["penalties_enforcement"].extend(web_requirements["penalties_enforcement"])
        combined["validity_periods"].extend(web_requirements["validity_periods"])
        
        # ì›¹ ê²€ìƒ‰ì—ì„œ ì°¾ì€ ê¸°ê´€ë“¤ ì¶”ê°€
        web_agencies = set()
        for source in web_requirements["sources"]:
            agency = source.get("agency", "Unknown")
            if agency != "Unknown":
                web_agencies.add(agency)
        
        for agency in web_agencies:
            if agency not in combined["agencies_found"]:
                combined["agencies_found"].append(agency)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ê³„ì‚°
        combined["category_stats"]["basic_requirements"] = len(web_requirements["certifications"])
        combined["category_stats"]["detailed_regulations"] = len(web_requirements["detailed_regulations"])
        combined["category_stats"]["testing_procedures"] = len(web_requirements["testing_procedures"])
        combined["category_stats"]["penalties_enforcement"] = len(web_requirements["penalties_enforcement"])
        combined["category_stats"]["validity_periods"] = len(web_requirements["validity_periods"])
        
        # í†µê³„ ê³„ì‚°
        combined["total_certifications"] = len(combined["certifications"])
        combined["total_documents"] = len(combined["documents"])
        combined["total_requirements"] = combined["total_certifications"] + combined["total_documents"]
        
        # íŒë¡€ ê¸°ë°˜ ê²€ì¦ ë‹¨ê³„ (CBP)
        try:
            precedents_payload = None
            if hasattr(self, 'get_cbp_precedents'):
                precedents_payload = awaitable_result = None
            # ë™ê¸°/ë¹„ë™ê¸° í˜¸í™˜ ì²˜ë¦¬
            try:
                import asyncio
                if asyncio.get_event_loop().is_running():
                    # toolsëŠ” ì¼ë°˜ ë©”ì„œë“œì´ë¯€ë¡œ ë‚´ë¶€ì—ì„œ ë¹„ë™ê¸° í˜¸ì¶œì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ì—†ì„ ìˆ˜ ìˆìŒ
                    # precedentsëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë¹„ë™ê¸°ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ í—¬í¼ ì‚¬ìš©
                    precedents_payload = asyncio.get_event_loop().run_until_complete(
                        self.get_cbp_precedents(hs_code)  # type: ignore
                    )
                else:
                    precedents_payload = asyncio.run(self.get_cbp_precedents(hs_code))  # type: ignore
            except RuntimeError:
                # ì´ë¯¸ ìƒìœ„ê°€ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ê´€ë¦¬ ì¤‘ì¸ ê²½ìš°, best-effortë¡œ ì§ì ‘ await ì‹œë„
                try:
                    precedents_payload = self.get_cbp_precedents(hs_code)  # type: ignore
                    if asyncio.iscoroutine(precedents_payload):
                        precedents_payload = asyncio.get_event_loop().run_until_complete(precedents_payload)
                except Exception:
                    precedents_payload = None

            if isinstance(precedents_payload, dict):
                combined["precedents"] = {
                    "hs_code": hs_code,
                    "count": precedents_payload.get("count", 0)
                }

                precedents_list = precedents_payload.get("precedents", [])

                # ê°„ë‹¨ ê²€ì¦ ë¡œì§: ë™ì¼ ê¸°ê´€ ì–¸ê¸‰ ë˜ëŠ” ê³µì‹ ë„ë©”ì¸ í¬í•¨ ì‹œ verified í‘œì‹œ
                def mark_verified(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
                    marked: List[Dict[str, Any]] = []
                    for it in items:
                        agency = (it.get("agency") or it.get("source") or "").upper()
                        verified = False
                        for case in precedents_list:
                            text_blob = " ".join([
                                str(case.get("title", "")),
                                str(case.get("summary", "")),
                                str(case.get("agency", "")),
                                str(case.get("url", ""))
                            ]).lower()
                            if agency and agency.lower() in text_blob:
                                verified = True
                                break
                        it["verified_by_precedent"] = bool(verified)
                        marked.append(it)
                    return marked

                combined["certifications"] = mark_verified(combined.get("certifications", []))
                combined["documents"] = mark_verified(combined.get("documents", []))

                # ì§‘ê³„: ê²€ì¦ ì¹´ìš´íŠ¸
                combined["precedent_verification"] = {
                    "total_precedents": len(precedents_list),
                    "verified_certifications": sum(1 for c in combined.get("certifications", []) if c.get("verified_by_precedent")),
                    "verified_documents": sum(1 for d in combined.get("documents", []) if d.get("verified_by_precedent"))
                }
        except Exception as e:
            combined["precedent_verification_error"] = str(e)

        return combined
