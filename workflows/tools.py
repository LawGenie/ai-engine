"""
LangGraph Tools for Requirements Analysis
íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë„êµ¬ë“¤
"""

from typing import Dict, Any, List, Optional
import asyncio
import httpx
from pathlib import Path
import json
from pypdf import PdfReader
from io import BytesIO
from datetime import datetime
import importlib.util
import sys
from abc import ABC, abstractmethod
from app.services.requirements.tavily_search import TavilySearchService
from app.services.requirements.web_scraper import WebScraper
from app.services.requirements.data_gov_api import DataGovAPIService


class SearchProvider(ABC):
    """ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì¶”ìƒí™” í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ì‹¤í–‰"""
        pass
    
    @property
    @abstractmethod
    def provider_name(self) -> str:
        """í”„ë¡œë°”ì´ë” ì´ë¦„"""
        pass


class TavilyProvider(SearchProvider):
    """Tavily ê²€ìƒ‰ í”„ë¡œë°”ì´ë”"""
    
    def __init__(self):
        self.service = TavilySearchService()
    
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        try:
            results = await self.service.search(query, **kwargs)
            return results if results else []
        except Exception as e:
            print(f"âŒ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    @property
    def provider_name(self) -> str:
        return "tavily"


class DisabledProvider(SearchProvider):
    """ê²€ìƒ‰ ë¹„í™œì„±í™” í”„ë¡œë°”ì´ë” (Tavily 432 ì—ëŸ¬ ì‹œ ì‚¬ìš©)"""
    
    async def search(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        print(f"ğŸ”‡ ê²€ìƒ‰ ë¹„í™œì„±í™” ëª¨ë“œ: '{query}' ìŠ¤í‚µë¨")
        return []
    
    @property
    def provider_name(self) -> str:
        return "disabled"


class RequirementsTools:
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ìœ„í•œ LangGraph ë„êµ¬ë“¤"""
    
    def __init__(self, search_provider: Optional[SearchProvider] = None):
        # ê²€ìƒ‰ í”„ë¡œë°”ì´ë” ì„¤ì • (ê¸°ë³¸ê°’: Tavily, í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
        import os
        provider_mode = os.getenv("SEARCH_PROVIDER", "tavily").lower()
        
        if provider_mode == "disabled":
            self.search_provider = DisabledProvider()
        else:
            self.search_provider = TavilyProvider()
        
        # ì™¸ë¶€ì—ì„œ ì œê³µëœ í”„ë¡œë°”ì´ë”ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if search_provider:
            self.search_provider = search_provider
            
        self.web_scraper = WebScraper()
        self.data_gov_api = DataGovAPIService()
        self.precedent_collector = self._init_cbp_collector()
        self.references_store_path = Path("reference_links.json")
    def _init_cbp_collector(self):
        """precedents-analysis/cbp_scraper.pyì˜ CBPDataCollectorë¥¼ ë™ì  ë¡œë“œí•œë‹¤."""
        try:
            base_dir = Path(__file__).resolve().parents[1]  # ai-engine/app
            project_root = base_dir.parent  # ai-engine
            target_path = project_root / "precedents-analysis" / "cbp_scraper.py"
            if not target_path.exists():
                return None
            spec = importlib.util.spec_from_file_location("cbp_scraper", str(target_path))
            if spec is None or spec.loader is None:
                return None
            module = importlib.util.module_from_spec(spec)
            sys.modules["cbp_scraper"] = module
            spec.loader.exec_module(module)
            if hasattr(module, "CBPDataCollector"):
                return module.CBPDataCollector()
        except Exception:
            return None
        return None

        
        # ê¸°ê´€ë³„ ë„ë©”ì¸ ë§¤í•‘
        self.agency_domains = {
            "FDA": "fda.gov",
            "FCC": "fcc.gov", 
            "CBP": "cbp.gov",
            "USDA": "usda.gov",
            "EPA": "epa.gov",
            "CPSC": "cpsc.gov",
            "KCS": "customs.go.kr",  # í•œêµ­ ê´€ì„¸ì²­
            "MFDS": "mfds.go.kr",    # ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜
            "MOTIE": "motie.go.kr"   # ì‚°ì—…í†µìƒìì›ë¶€
        }
    
    async def search_agency_documents(self, agency: str, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ê¸°ê´€ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (í†µí•©)"""
        print(f"ğŸ”§ [TOOL] {agency} ë¬¸ì„œ ê²€ìƒ‰: {query}")
        
        results = await self.search_provider.search(query, max_results=max_results)
        
        # ê¸°ê´€ë³„ ë„ë©”ì¸ í•„í„°ë§
        agency_domain = self.agency_domains.get(agency, "")
        agency_results = []
        
        for result in results:
            url = result.get("url", "")
            if agency_domain in url:
                agency_results.append(result)
                print(f"  âœ… {agency} ê³µì‹ ë¬¸ì„œ ë°œê²¬: {result.get('title', 'No title')}")
            else:
                print(f"  âŒ {agency} ì™¸ë¶€ ë¬¸ì„œ ì œì™¸: {result.get('title', 'No title')}")
        
        return {
            "agency": agency,
            "query": query,
            "total_results": len(results),
            "agency_results": agency_results,
            "selected_url": agency_results[0]["url"] if agency_results else None,
            "domain": agency_domain
        }
    
    async def search_fda_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """FDA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.search_agency_documents("FDA", query, max_results)
    
    # ë¯¸êµ­ ì •ë¶€ ê¸°ê´€ë“¤
    async def search_fcc_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """FCC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("FCC", query, max_results)
    
    async def search_cbp_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """CBP ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("CBP", query, max_results)
    
    async def search_usda_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """USDA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("USDA", query, max_results)
    
    async def search_epa_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """EPA ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("EPA", query, max_results)
    
    async def search_cpsc_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """CPSC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("CPSC", query, max_results)
    
    # í•œêµ­ ì •ë¶€ ê¸°ê´€ë“¤
    async def search_kcs_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """í•œêµ­ ê´€ì„¸ì²­ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("KCS", query, max_results)
    
    async def search_mfds_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("MFDS", query, max_results)
    
    async def search_motie_documents(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """ì‚°ì—…í†µìƒìì›ë¶€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬"""
        return await self.search_agency_documents("MOTIE", query, max_results)
    
    async def scrape_document(self, agency: str, url: str, hs_code: str) -> Dict[str, Any]:
        """íŠ¹ì • ë¬¸ì„œ ìŠ¤í¬ë˜í•‘ ë„êµ¬ (í™•ì¥)"""
        print(f"ğŸ”§ [TOOL] {agency} ë¬¸ì„œ ìŠ¤í¬ë˜í•‘")
        print(f"  URL: {url}")
        print(f"  HSì½”ë“œ: {hs_code}")
        
        try:
            # ê¸°ê´€ë³„ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œ ë§¤í•‘
            scraper_methods = {
                "FDA": "scrape_fda_requirements",
                "FCC": "scrape_fcc_requirements", 
                "CBP": "scrape_cbp_requirements",
                "USDA": "scrape_usda_requirements",
                "EPA": "scrape_epa_requirements",
                "CPSC": "scrape_cpsc_requirements",
                "KCS": "scrape_kcs_requirements",
                "MFDS": "scrape_mfds_requirements",
                "MOTIE": "scrape_motie_requirements"
            }
            
            method_name = scraper_methods.get(agency)
            if not method_name:
                return {"error": f"Unknown agency: {agency}"}
            
            # ë™ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œ í˜¸ì¶œ
            scraper_method = getattr(self.web_scraper, method_name, None)
            if not scraper_method:
                return {"error": f"Scraper method not implemented for {agency}"}
            
            result = await scraper_method(hs_code, url)
            
            # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            certs = result.get("certifications", [])
            docs = result.get("documents", [])
            sources = result.get("sources", [])
            
            print(f"  âœ… {agency} ìŠ¤í¬ë˜í•‘ ì„±ê³µ:")
            print(f"    ğŸ“‹ ì¸ì¦ìš”ê±´: {len(certs)}ê°œ")
            for i, cert in enumerate(certs, 1):
                print(f"      {i}. {cert.get('name', 'Unknown')} ({cert.get('agency', 'Unknown')})")
                print(f"         ì„¤ëª…: {cert.get('description', 'No description')}")
            
            print(f"    ğŸ“„ í•„ìš”ì„œë¥˜: {len(docs)}ê°œ")
            for i, doc in enumerate(docs, 1):
                print(f"      {i}. {doc.get('name', 'Unknown')}")
                print(f"         ì„¤ëª…: {doc.get('description', 'No description')}")
            
            print(f"    ğŸ“š ì¶œì²˜: {len(sources)}ê°œ")
            for i, source in enumerate(sources, 1):
                print(f"      {i}. {source.get('title', 'Unknown')} ({source.get('type', 'Unknown')})")
            
            return result
            
        except Exception as e:
            print(f"  âŒ {agency} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return {
                "agency": agency,
                "error": str(e),
                "certifications": [],
                "documents": [],
                "sources": []
            }

    async def get_cbp_precedents(self, hs_code: str) -> Dict[str, Any]:
        """CBP íŒë¡€/ê²°ì • ì‚¬ë¡€ ì¡°íšŒ ë„êµ¬."""
        try:
            if not self.precedent_collector:
                return {"hs_code": hs_code, "count": 0, "precedents": [], "error": "cbp_collector_not_available"}
            precedents = await self.precedent_collector.get_precedents_by_hs_code(hs_code)
            return {
                "hs_code": hs_code,
                "count": len(precedents),
                "precedents": precedents
            }
        except Exception as e:
            return {"hs_code": hs_code, "count": 0, "precedents": [], "error": str(e)}

    async def summarize_pdf(self, url: str, max_pages: int = 5) -> Dict[str, Any]:
        """PDF ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì•ë¶€ë¶„ì„ ìš”ì•½(ë°œì·Œ)í•œë‹¤."""
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                resp = await client.get(url)
                resp.raise_for_status()
                data = BytesIO(resp.content)
                reader = PdfReader(data)
                num_pages = min(len(reader.pages), max_pages)
                text_chunks: List[str] = []
                for i in range(num_pages):
                    try:
                        text_chunks.append(reader.pages[i].extract_text() or "")
                    except Exception:
                        continue
                combined = "\n".join([t.strip() for t in text_chunks if t and t.strip()])
                preview = (combined[:1200] + "â€¦") if len(combined) > 1200 else combined
                return {
                    "url": url,
                    "pages_read": num_pages,
                    "excerpt": preview,
                    "char_count": len(preview)
                }
        except Exception as e:
            return {"url": url, "error": str(e)}

    def save_reference_links(self, hs_code: str, product_name: str, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ëœ ì°¸ê³  ë§í¬ë“¤ì„ ë¡œì»¬ JSONì— ì €ì¥/ë³‘í•©í•œë‹¤."""
        try:
            existing: Dict[str, Any] = {}
            if self.references_store_path.exists():
                existing = json.loads(self.references_store_path.read_text(encoding="utf-8"))
            key = f"{hs_code}:{product_name}"
            payload = {
                "hs_code": hs_code,
                "product_name": product_name,
                "saved_at": datetime.utcnow().isoformat() + "Z",
                "agencies": {}
            }
            for k, v in search_results.items():
                agency = v.get("agency") or k
                urls = v.get("urls", [])
                payload["agencies"].setdefault(agency, {"urls": []})
                # ë³‘í•©
                payload["agencies"][agency]["urls"] = list({*payload["agencies"][agency]["urls"], *urls})
            existing[key] = payload
            self.references_store_path.write_text(json.dumps(existing, ensure_ascii=False, indent=2), encoding="utf-8")
            return {"saved": True, "reference_key": key, "agencies": list(payload["agencies"].keys())}
        except Exception as e:
            return {"saved": False, "error": str(e)}
    
    async def analyze_requirements(self, requirements_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë„êµ¬ (í™•ì¥)"""
        print(f"ğŸ”§ [TOOL] ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹œì‘")
        
        certifications = requirements_data.get("certifications", [])
        documents = requirements_data.get("documents", [])
        sources = requirements_data.get("sources", [])
        
        # ê¸°ë³¸ ë¶„ì„ í†µê³„
        total_certs = len(certifications)
        total_docs = len(documents)
        total_sources = len(sources)
        
        # ê¸°ê´€ë³„ í†µê³„
        agency_stats = {}
        for cert in certifications:
            agency = cert.get("agency", "Unknown")
            agency_stats[agency] = agency_stats.get(agency, 0) + 1
        
        # ìš°ì„ ìˆœìœ„ ë¶„ì„
        high_priority = [c for c in certifications if c.get("priority") == "high"]
        required_docs = [d for d in documents if d.get("required", False)]
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        completeness_score = min(1.0, (total_certs + total_docs) / 10)  # 0-1 ìŠ¤ì¼€ì¼
        coverage_ratio = len(agency_stats) / len(self.agency_domains)  # ê¸°ê´€ ì»¤ë²„ë¦¬ì§€
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_factors = []
        if total_certs > 5:
            complexity_factors.append("ë‹¤ì¤‘ ì¸ì¦ ìš”êµ¬")
        if len(agency_stats) > 3:
            complexity_factors.append("ë‹¤ê¸°ê´€ ê·œì œ")
        if any("critical" in str(cert).lower() for cert in certifications):
            complexity_factors.append("ì¤‘ìš” ì¸ì¦ ìš”êµ¬")
        
        compliance_complexity = "simple" if len(complexity_factors) == 0 else "moderate" if len(complexity_factors) <= 2 else "complex"
        
        # ë¹„ìš© ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        estimated_cost_low = total_certs * 100 + total_docs * 50  # USD
        estimated_cost_high = total_certs * 500 + total_docs * 200
        
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        risk_factors = []
        if total_certs == 0:
            risk_factors.append("ì¸ì¦ ìš”êµ¬ì‚¬í•­ ë¶ˆëª…í™•")
        if len(required_docs) > 10:
            risk_factors.append("ì„œë¥˜ ìš”êµ¬ì‚¬í•­ ê³¼ë‹¤")
        if coverage_ratio < 0.3:
            risk_factors.append("ê¸°ê´€ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±")
        
        overall_risk_level = "low" if len(risk_factors) == 0 else "medium" if len(risk_factors) <= 2 else "high"
        
        print(f"  ğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"    ğŸ“‹ ì´ ì¸ì¦ìš”ê±´: {total_certs}ê°œ")
        print(f"    ğŸ“„ ì´ í•„ìš”ì„œë¥˜: {total_docs}ê°œ")
        print(f"    ğŸ“š ì´ ì¶œì²˜: {total_sources}ê°œ")
        print(f"    ğŸ¢ ê¸°ê´€ë³„ ì¸ì¦ìš”ê±´:")
        for agency, count in agency_stats.items():
            print(f"      â€¢ {agency}: {count}ê°œ")
        print(f"    âš ï¸ ê³ ìš°ì„ ìˆœìœ„ ì¸ì¦ìš”ê±´: {len(high_priority)}ê°œ")
        print(f"    ğŸ“‹ í•„ìˆ˜ ì„œë¥˜: {len(required_docs)}ê°œ")
        print(f"    ğŸ“ˆ ì™„ì„±ë„ ì ìˆ˜: {completeness_score:.2f}")
        print(f"    ğŸ¯ ê¸°ê´€ ì»¤ë²„ë¦¬ì§€: {coverage_ratio:.2f}")
        print(f"    âš¡ ë³µì¡ë„: {compliance_complexity}")
        print(f"    ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost_low}-${estimated_cost_high}")
        print(f"    âš ï¸ ë¦¬ìŠ¤í¬ ë ˆë²¨: {overall_risk_level}")
        
        return {
            # ê¸°ë³¸ í†µê³„
            "total_certifications": total_certs,
            "total_documents": total_docs,
            "total_sources": total_sources,
            "agency_stats": agency_stats,
            "high_priority_count": len(high_priority),
            "required_docs_count": len(required_docs),
            
            # í’ˆì§ˆ ì§€í‘œ
            "quality_metrics": {
                "completeness_score": completeness_score,
                "coverage_ratio": coverage_ratio,
                "compliance_complexity": compliance_complexity,
                "complexity_factors": complexity_factors
            },
            
            # ë¹„ìš© ë¶„ì„
            "cost_analysis": {
                "estimated_cost_low": estimated_cost_low,
                "estimated_cost_high": estimated_cost_high,
                "currency": "USD"
            },
            
            # ë¦¬ìŠ¤í¬ ë¶„ì„
            "risk_analysis": {
                "overall_risk_level": overall_risk_level,
                "risk_factors": risk_factors
            },
            
            "analysis_complete": True
        }
    
    async def search_requirements_hybrid(self, hs_code: str, product_name: str) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Data.gov API + Tavily Search"""
        print(f"\nğŸš€ [HYBRID] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘")
        print(f"  ğŸ“‹ HSì½”ë“œ: {hs_code}")
        print(f"  ğŸ“¦ ìƒí’ˆëª…: {product_name}")
        
        results = {
            "hs_code": hs_code,
            "product_name": product_name,
            "search_timestamp": None,
            "api_results": {},
            "web_results": {},
            "combined_results": {},
            "search_methods": []
        }
        
        # 1. Data.gov API ê²€ìƒ‰ (HSì½”ë“œ ì§ì ‘ ê²€ìƒ‰)
        try:
            print(f"\n  ğŸ” 1ë‹¨ê³„: Data.gov API ê²€ìƒ‰")
            api_results = await self.data_gov_api.search_requirements_by_hs_code(hs_code, product_name)
            results["api_results"] = api_results
            results["search_methods"].append("data_gov_api")
            print(f"    âœ… API ê²€ìƒ‰ ì™„ë£Œ: {api_results.get('total_requirements', 0)}ê°œ ìš”êµ¬ì‚¬í•­")
        except Exception as e:
            print(f"    âŒ API ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["api_results"] = {"error": str(e)}
        
        # 2. Tavily Search (ì›¹ ê²€ìƒ‰)
        try:
            print(f"\n  ğŸ” 2ë‹¨ê³„: Tavily Search ì›¹ ê²€ìƒ‰")
            # 8ìë¦¬ì™€ 6ìë¦¬ HSì½”ë“œë¡œ ê²€ìƒ‰
            hs_code_8digit = hs_code
            hs_code_6digit = ".".join(hs_code.split(".")[:2]) if "." in hs_code else hs_code
            
            web_queries = {
                f"FDA_8digit": f"site:fda.gov import requirements {product_name} HS {hs_code_8digit}",
                f"USDA_8digit": f"site:usda.gov agricultural import requirements {product_name} HS {hs_code_8digit}",
                f"EPA_8digit": f"site:epa.gov environmental regulations {product_name} HS {hs_code_8digit}",
                f"FCC_8digit": f"site:fcc.gov device authorization requirements {product_name} HS {hs_code_8digit}",
                f"CBP_8digit": f"site:cbp.gov import documentation requirements HS {hs_code_8digit} {product_name}",
                f"CPSC_8digit": f"site:cpsc.gov consumer product safety {product_name} HS {hs_code_8digit}",
                f"FDA_6digit": f"site:fda.gov import requirements {product_name} HS {hs_code_6digit}",
                f"USDA_6digit": f"site:usda.gov agricultural import requirements {product_name} HS {hs_code_6digit}",
                f"EPA_6digit": f"site:epa.gov environmental regulations {product_name} HS {hs_code_6digit}",
                f"FCC_6digit": f"site:fcc.gov device authorization requirements {product_name} HS {hs_code_6digit}",
                f"CBP_6digit": f"site:cbp.gov import documentation requirements HS {hs_code_6digit} {product_name}",
                f"CPSC_6digit": f"site:cpsc.gov consumer product safety {product_name} HS {hs_code_6digit}"
            }
            
            web_results = {}
            for query_key, query in web_queries.items():
                try:
                    search_results = await self.search_provider.search(query, max_results=5)
                    web_results[query_key] = {
                        "query": query,
                        "results": search_results,
                        "urls": [r.get("url") for r in search_results if r.get("url")],
                        "hs_code_type": "8digit" if "8digit" in query_key else "6digit",
                        "agency": query_key.split("_")[0]
                    }
                except Exception as e:
                    web_results[query_key] = {"error": str(e)}
            
            results["web_results"] = web_results
            results["search_methods"].append("tavily_search")
            print(f"    âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(web_results)}ê°œ ì¿¼ë¦¬")
            
        except Exception as e:
            print(f"    âŒ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            results["web_results"] = {"error": str(e)}
        
        # 3. ê²°ê³¼ í†µí•©
        print(f"\n  ğŸ”„ 3ë‹¨ê³„: ê²°ê³¼ í†µí•©")
        combined_results = self._combine_search_results(results["api_results"], results["web_results"])
        results["combined_results"] = combined_results
        
        print(f"\nâœ… [HYBRID] ê²€ìƒ‰ ì™„ë£Œ")
        print(f"  ğŸ” ê²€ìƒ‰ ë°©ë²•: {', '.join(results['search_methods'])}")
        print(f"  ğŸ“‹ ì´ ìš”êµ¬ì‚¬í•­: {combined_results.get('total_requirements', 0)}ê°œ")
        print(f"  ğŸ† ì¸ì¦ìš”ê±´: {combined_results.get('total_certifications', 0)}ê°œ")
        print(f"  ğŸ“„ í•„ìš”ì„œë¥˜: {combined_results.get('total_documents', 0)}ê°œ")
        
        return results
    
    def _combine_search_results(self, api_results: Dict[str, Any], web_results: Dict[str, Any]) -> Dict[str, Any]:
        """APIì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        combined = {
            "certifications": [],
            "documents": [],
            "sources": [],
            "total_requirements": 0,
            "total_certifications": 0,
            "total_documents": 0,
            "agencies_found": [],
            "search_sources": {
                "api_success": "api_results" in api_results and "error" not in api_results,
                "web_success": "web_results" in web_results and "error" not in web_results
            }
        }
        
        # API ê²°ê³¼ í†µí•©
        if "agencies" in api_results and "error" not in api_results:
            agencies = api_results.get("agencies", {})
            for agency, data in agencies.items():
                if data.get("status") == "success":
                    combined["certifications"].extend(data.get("certifications", []))
                    combined["documents"].extend(data.get("documents", []))
                    combined["sources"].extend(data.get("sources", []))
                    combined["agencies_found"].append(agency)
        
        # ì›¹ ê²°ê³¼ í†µí•© (ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì¶”ê°€)
        if "web_results" in web_results and "error" not in web_results:
            # ì›¹ ê²€ìƒ‰ì—ì„œ ì°¾ì€ ê¸°ê´€ë“¤ì— ëŒ€í•œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì¶”ê°€
            found_agencies = set()
            for query_key, data in web_results.items():
                if "error" not in data and data.get("urls"):
                    agency = data.get("agency")
                    found_agencies.add(agency)
            
            # ì°¾ì€ ê¸°ê´€ë“¤ì— ëŒ€í•œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì¶”ê°€
            for agency in found_agencies:
                if agency not in combined["agencies_found"]:
                    combined["certifications"].append({
                        "name": f"{agency} ê¸°ë³¸ ë“±ë¡ ìš”êµ¬ì‚¬í•­",
                        "required": True,
                        "description": f"{agency}ì—ì„œ ìš”êµ¬í•˜ëŠ” ê¸°ë³¸ ë“±ë¡ ìš”êµ¬ì‚¬í•­",
                        "agency": agency,
                        "url": f"https://www.{agency.lower()}.gov"
                    })
                    combined["agencies_found"].append(agency)
        
        # í†µê³„ ê³„ì‚°
        combined["total_certifications"] = len(combined["certifications"])
        combined["total_documents"] = len(combined["documents"])
        combined["total_requirements"] = combined["total_certifications"] + combined["total_documents"]
        
        return combined
