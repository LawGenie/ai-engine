from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any
import json
import logging
from cbp_scraper import CBPDataCollector
from ai_analyzer import PrecedentsAnalyzer
from vector_precedents_search import VectorPrecedentsSearch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Precedents Analysis AI Engine", version="1.0.0")

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ëª¨ë¸
class ProductAnalysisRequest(BaseModel):
    product_id: str
    product_name: str
    description: str
    hs_code: str
    origin_country: str
    price: float
    fob_price: float

# íŒë¡€ ìƒì„¸ ì •ë³´ ëª¨ë¸
class PrecedentDetail(BaseModel):
    case_id: str
    title: str
    description: str
    status: str
    link: str
    source: str
    hs_code: str

# íŒë¡€ ë¶„ì„ ì „ìš© ëª¨ë¸ë“¤ë§Œ ìœ ì§€

# ì‘ë‹µ ëª¨ë¸
class PrecedentsAnalysisResponse(BaseModel):
    success_cases: List[str]
    failure_cases: List[str]
    review_cases: List[str]  # ê²€í†  í•„ìš” ì‚¬ë¡€ ì¶”ê°€
    actionable_insights: List[str]
    risk_factors: List[str]
    recommended_action: str
    confidence_score: float
    is_valid: bool
    precedents_data: List[PrecedentDetail]  # íŒë¡€ ì›ë³¸ ë°ì´í„° ì¶”ê°€

# ğŸš€ AI ë¶„ì„ê¸°, CBP ë°ì´í„° ìˆ˜ì§‘ê¸°, ë²¡í„° ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
analyzer = PrecedentsAnalyzer()
cbp_collector = CBPDataCollector()
vector_search = VectorPrecedentsSearch()

# ğŸš€ ìƒí˜¸ ì—°ê²° ì„¤ì •
vector_search.set_cbp_collector(cbp_collector)
cbp_collector.set_vector_search(vector_search)

# AI ë¶„ì„ ê²°ê³¼ ìºì‹±ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
analysis_cache = {}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "service": "precedents-analysis-ai-engine",
        "version": "1.0.0"
    }

# íŒë¡€ ë¶„ì„ ì „ìš© ì—”ë“œí¬ì¸íŠ¸ë§Œ ìœ ì§€

@app.post("/analyze-precedents", response_model=PrecedentsAnalysisResponse)
async def analyze_precedents(request: ProductAnalysisRequest):
    """
    ìƒí’ˆ ì •ë³´ë¥¼ ë°›ì•„ì„œ íŒë¡€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ìºì‹± + ë²¡í„° ê²€ìƒ‰ ì ìš©)
    """
    try:
        logger.info(f"ğŸš€ íŒë¡€ ë¶„ì„ ì‹œì‘: {request.product_id} - {request.product_name}")
        
        # ğŸš€ 1. CBPì—ì„œ ì‹¤ì œ íŒë¡€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹± ì ìš©)
        logger.info(f"CBP ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: HSì½”ë“œ {request.hs_code}")
        cbp_data = await cbp_collector.get_precedents_by_hs_code(request.hs_code)
        logger.info(f"CBP ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(cbp_data)}ê°œ ì‚¬ë¡€ ë°œê²¬")
        
        # ğŸš€ 2. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ íŒë¡€ ì¶”ê°€ ìˆ˜ì§‘
        logger.info("ë²¡í„° ê¸°ë°˜ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ì‹œì‘")
        similar_precedents = await vector_search.find_similar_precedents(
            product_description=request.description,
            product_name=request.product_name,
            top_k=5
        )
        logger.info(f"ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ: {len(similar_precedents)}ê°œ ì¶”ê°€ ì‚¬ë¡€")
        
        # ğŸš€ 3. ê¸°ì¡´ CBP ë°ì´í„° + ìœ ì‚¬ íŒë¡€ ê²°í•©
        combined_data = cbp_data + similar_precedents
        logger.info(f"ì´ íŒë¡€ ë°ì´í„°: {len(combined_data)}ê°œ (CBP: {len(cbp_data)}, ìœ ì‚¬: {len(similar_precedents)})")
        
        # ğŸš€ 4. AI ë¶„ì„ ìˆ˜í–‰ (ìºì‹± ì ìš©)
        cache_key = f"{request.hs_code}_{request.product_name}_{len(combined_data)}"
        
        if cache_key in analysis_cache:
            logger.info("âœ… ìºì‹œì—ì„œ AI ë¶„ì„ ê²°ê³¼ ë°˜í™˜")
            analysis_result = analysis_cache[cache_key]
        else:
            logger.info("AI ë¶„ì„ ì‹œì‘")
            analysis_result = await analyzer.analyze_precedents(request.model_dump(), combined_data)
            analysis_cache[cache_key] = analysis_result  # ê²°ê³¼ ìºì‹±
            logger.info("AI ë¶„ì„ ì™„ë£Œ ë° ìºì‹±")
        
        # 5. ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        success_cases = []
        failure_cases = []
        review_cases = []
        
        for case in analysis_result.get("success_cases", []):
            if isinstance(case, dict):
                success_cases.append(case.get("text", str(case)))
            else:
                success_cases.append(str(case))
        
        for case in analysis_result.get("failure_cases", []):
            if isinstance(case, dict):
                failure_cases.append(case.get("text", str(case)))
            else:
                failure_cases.append(str(case))
        
        for case in analysis_result.get("review_cases", []):
            if isinstance(case, dict):
                review_cases.append(case.get("text", str(case)))
            else:
                review_cases.append(str(case))
        
        # 6. íŒë¡€ ì›ë³¸ ë°ì´í„° í¬ë§·íŒ…
        precedents_data = []
        for precedent in combined_data:
            precedents_data.append(PrecedentDetail(
                case_id=precedent.get("case_id", "N/A"),
                title=precedent.get("title", "N/A"),
                description=precedent.get("description", "N/A"),
                status=precedent.get("status", "UNKNOWN"),
                link=precedent.get("link", ""),
                source=precedent.get("source", "unknown"),
                hs_code=precedent.get("hs_code", request.hs_code)
            ))
        
        # 7. ê²°ê³¼ ë°˜í™˜ (íŒë¡€ ì›ë³¸ ë°ì´í„° í¬í•¨)
        return PrecedentsAnalysisResponse(
            success_cases=success_cases,
            failure_cases=failure_cases,
            review_cases=review_cases,  # ê²€í†  í•„ìš” ì‚¬ë¡€ ì¶”ê°€
            actionable_insights=analysis_result.get("actionable_insights", []),
            risk_factors=analysis_result.get("risk_factors", []),
            recommended_action=analysis_result.get("recommended_action", ""),
            confidence_score=analysis_result.get("confidence_score", 0.0),
            is_valid=analysis_result.get("is_valid", False),
            precedents_data=precedents_data  # íŒë¡€ ì›ë³¸ ë°ì´í„° ì¶”ê°€!
        )
        
    except Exception as e:
        logger.error(f"íŒë¡€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.get("/test-cbp/{hs_code}")
async def test_cbp_data(hs_code: str):
    """
    CBP ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
    """
    try:
        logger.info(f"CBP í…ŒìŠ¤íŠ¸ ì‹œì‘: HSì½”ë“œ {hs_code}")
        cbp_data = await cbp_collector.get_precedents_by_hs_code(hs_code)
        
        return {
            "hs_code": hs_code,
            "data_count": len(cbp_data),
            "sample_data": cbp_data[:3] if cbp_data else [],
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"CBP í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"CBP test failed: {str(e)}")

@app.get("/cache-stats")
async def get_cache_stats():
    """
    ìºì‹œ ë° ë²¡í„° ê²€ìƒ‰ í†µê³„ ì¡°íšŒ
    """
    try:
        cache_stats = cbp_collector.get_cache_stats()
        search_stats = vector_search.get_search_stats()
        
        return {
            "cbp_cache_stats": cache_stats,
            "search_stats": search_stats,
            "ai_analysis_cache_size": len(analysis_cache),
            "ai_analysis_cache_keys": list(analysis_cache.keys()),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Stats retrieval failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
