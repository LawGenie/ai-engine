import asyncio
import aiohttp
import logging
from typing import List, Dict, Any
from tavily import TavilyClient
import time
from datetime import datetime
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logger = logging.getLogger(__name__)

class CBPDataCollector:
    def __init__(self):
        self.session = None
        
        # ğŸš€ ë©”ëª¨ë¦¬ ìºì‹± ì‹œìŠ¤í…œ
        self.memory_cache = {}
        self.cache_ttl = 7 * 24 * 3600  # 7ì¼ (ì´ˆ)
        logger.info("ë©”ëª¨ë¦¬ ìºì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ğŸš€ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ (ì§€ì—° ë¡œë”©)
        self.vector_search = None
        
        # Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.tavily_client = None
        tavily_api_key = os.getenv('TAVILY_API_KEY')
        if tavily_api_key:
            try:
                self.tavily_client = TavilyClient(api_key=tavily_api_key)
                logger.info("âœ… Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            logger.warning("âš ï¸ TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def get_cached_data(self, hs_code: str) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
        if hs_code in self.memory_cache:
            cached_time, data = self.memory_cache[hs_code]
            if time.time() - cached_time < self.cache_ttl:
                logger.info(f"âœ… ìºì‹œì—ì„œ ë°ì´í„° ë°˜í™˜: {hs_code}")
                return data
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self.memory_cache[hs_code]
                logger.info(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {hs_code}")
        
        return None
    
    def cache_data(self, hs_code: str, data: List[Dict[str, Any]]):
        """ë°ì´í„° ìºì‹±"""
        self.memory_cache[hs_code] = (time.time(), data)
        logger.info(f"ğŸ’¾ ë°ì´í„° ìºì‹± ì™„ë£Œ: {hs_code} ({len(data)}ê°œ í•­ëª©)")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        current_time = time.time()
        valid_count = 0
        expired_count = 0
        
        for hs_code, (cached_time, _) in self.memory_cache.items():
            if current_time - cached_time < self.cache_ttl:
                valid_count += 1
            else:
                expired_count += 1
        
        return {
            'total_cached': len(self.memory_cache),
            'valid_cached': valid_count,
            'expired_cached': expired_count,
            'cache_hit_rate': f"{valid_count}/{len(self.memory_cache)}" if self.memory_cache else "0/0"
        }
    
    def set_vector_search(self, vector_search):
        """ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„¤ì •"""
        self.vector_search = vector_search
        logger.info("âœ… ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
    
    def _store_precedents_in_vector_db(self, hs_code: str, precedents: List[Dict[str, Any]]):
        """ìˆ˜ì§‘ëœ íŒë¡€ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥ (ì‹¤ì œ rulingë§Œ)"""
        if not self.vector_search:
            return
        
        try:
            stored_count = 0
            for precedent in precedents:
                # ëª¨ë“  CBP ê´€ë ¨ ë°ì´í„° ì €ì¥ (ëŠìŠ¨í•œ ê¸°ì¤€)
                case_id = precedent.get('case_id', '')
                if not case_id:
                    logger.warning(f"âš ï¸ case_id ì—†ìŒ - ë²¡í„° DB ì €ì¥ ì œì™¸")
                    continue
                
                success = self.vector_search.add_precedent_to_db(
                    precedent_id=case_id,
                    text=precedent.get('description', '') + ' ' + precedent.get('title', ''),
                    hs_code=hs_code,
                    case_type=precedent.get('case_type', 'unknown'),
                    outcome=precedent.get('outcome', 'unknown'),
                    source=precedent.get('source', 'cbp'),
                    additional_metadata={
                        'date': precedent.get('date'),
                        'link': precedent.get('link', ''),
                        'key_factors': precedent.get('key_factors', [])
                    }
                )
                if success:
                    stored_count += 1
                    logger.info(f"âœ… ì‹¤ì œ CBP ruling ë²¡í„° DB ì €ì¥: {case_id}")
            
            logger.info(f"âœ… ë²¡í„° DBì— {stored_count}ê°œ ì‹¤ì œ íŒë¡€ ì €ì¥ ì™„ë£Œ: {hs_code}")
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def get_precedents_by_hs_code(self, hs_code: str) -> List[Dict[str, Any]]:
        """
        HSì½”ë“œì— ë”°ë¥¸ ì‹¤ì œ CBP íŒë¡€ë¥¼ Tavilyë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ìºì‹± ì ìš©)
        """
        logger.info(f"ğŸ” CBP ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: HSì½”ë“œ {hs_code}")
        
        # ğŸš€ 1. ìºì‹œ í™•ì¸
        cached_data = self.get_cached_data(hs_code)
        if cached_data:
            return cached_data
        
        try:
            # ğŸš€ 2. ìºì‹œì— ì—†ìœ¼ë©´ Tavilyë¡œ ê²€ìƒ‰
            logger.info(f"ğŸ“¡ Tavily ê²€ìƒ‰ ì‹œì‘: {hs_code}")
            tavily_data = await self._search_cbp_with_tavily(hs_code)
            
            # ë°ì´í„° ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°
            cleaned_data = self._clean_and_deduplicate_data(tavily_data)
            
            # ğŸš€ 3. ê²°ê³¼ ìºì‹±
            self.cache_data(hs_code, cleaned_data)
            
            # ğŸš€ 4. ë²¡í„° DBì— íŒë¡€ ì €ì¥
            self._store_precedents_in_vector_db(hs_code, cleaned_data)
            
            logger.info(f"âœ… CBP ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(cleaned_data)}ê°œ ì‚¬ë¡€")
            return cleaned_data
            
        except Exception as e:
            logger.error(f"âŒ CBP ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _search_cbp_with_tavily(self, hs_code: str) -> List[Dict[str, Any]]:
        """
        Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ CBP ê³µì‹ rulingë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        if not self.tavily_client:
            logger.error("âŒ Tavily í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. TAVILY_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return []
        
        try:
            # ì‹¤ì œ CBP ruling ê²€ìƒ‰ ì¿¼ë¦¬ (êµ¬ì²´ì ì¸ íŒë¡€ ì°¾ê¸°)
            search_queries = [
                f'site:rulings.cbp.gov "HS {hs_code}" "classification" ruling',
                f'site:rulings.cbp.gov "subheading {hs_code}" "approved" OR "denied"',
                f'site:rulings.cbp.gov "tariff classification {hs_code}" "ruling"',
                f'"HQ" "NY" "HS {hs_code}" "cosmetic" "classification" site:rulings.cbp.gov',
                f'"{hs_code}" "classified in" OR "excluded from" site:rulings.cbp.gov',
            ]
            
            all_results = []
            
            for query in search_queries[:3]:  # ìƒìœ„ 3ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
                try:
                    logger.info(f"ğŸ” Tavily CBP ê²€ìƒ‰: {query}")
                    
                    response = self.tavily_client.search(
                        query=query,
                        search_depth='advanced',
                        max_results=10,
                        include_domains=['rulings.cbp.gov']  # rulings.cbp.govë§Œ!
                    )
                    
                    for result in response.get('results', []):
                        url = result.get('url', '')
                        
                        # rulings.cbp.govë§Œ í—ˆìš© (ì‹¤ì œ ruling í˜ì´ì§€ë§Œ)
                        if 'rulings.cbp.gov' not in url:
                            logger.warning(f"âš ï¸ rulings.cbp.gov ì•„ë‹˜ - ì œì™¸: {url}")
                            continue
                        
                        # ì‹¤ì œ ruling í˜ì´ì§€ë§Œ í—ˆìš©
                        url_lower = url.lower()
                        if '/ruling/' not in url_lower:
                            logger.warning(f"âš ï¸ ruling í˜ì´ì§€ ì•„ë‹˜ - ì œì™¸: {url}")
                            continue
                        
                        # ì œì™¸í•  í˜ì´ì§€ë“¤
                        exclude_pages = [
                            '/search', '/sites/default/files', '/home', '/requirements'
                        ]
                        
                        if any(exclude in url_lower for exclude in exclude_pages):
                            logger.warning(f"âš ï¸ ì¼ë°˜ í˜ì´ì§€ ì œì™¸: {url}")
                            continue
                        
                        cbp_data = self._convert_tavily_result_to_cbp_data(result, hs_code)
                        if cbp_data:
                            all_results.append(cbp_data)
                            logger.info(f"âœ… ì‹¤ì œ CBP ruling ì¶”ê°€: {cbp_data['case_id']}")
                    
                    # ìš”ì²­ ê°„ ì§€ì—° (Rate Limit ë°©ì§€)
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ Tavily ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
                    continue
            
            if not all_results:
                logger.warning(f"âš ï¸ ì‹¤ì œ CBP rulingì„ ì°¾ì§€ ëª»í•¨: HS {hs_code}")
            else:
                logger.info(f"âœ… ì´ {len(all_results)}ê°œ ì‹¤ì œ CBP ruling ë°œê²¬: HS {hs_code}")
            
            return all_results
            
        except Exception as e:
            logger.error(f"âŒ Tavily CBP ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _convert_tavily_result_to_cbp_data(self, result: Dict[str, Any], hs_code: str) -> Dict[str, Any]:
        """
        Tavily ê²€ìƒ‰ ê²°ê³¼ë¥¼ CBP ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ì‹¤ì œ CBP ruling ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        try:
            title = result.get('title', '')
            url = result.get('url', '')
            content = result.get('content', '')
            
            # URLì—ì„œ ì‹¤ì œ CBP ruling ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
            case_id = self._extract_case_id_from_url(url, title)
            
            # case_idê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            if not case_id:
                case_id = f'CBP-{title[:10].replace(" ", "")}'  # ì œëª© ê¸°ë°˜ ID ìƒì„±
                logger.info(f"âœ… ì œëª© ê¸°ë°˜ ID ìƒì„±: {case_id}")
            
            # ë‚´ìš©ì—ì„œ ìŠ¹ì¸/ê±°ë¶€ ìƒíƒœ íŒë‹¨
            status = self._determine_status_from_content(content, title)
            
            # UNKNOWN ìƒíƒœë„ í—ˆìš© (ëŠìŠ¨í•œ ê¸°ì¤€)
            if status == 'UNKNOWN':
                status = 'REVIEW'  # ê²€í†  í•„ìš” ìƒíƒœë¡œ ë³€ê²½
                logger.info(f"âœ… ê²€í†  í•„ìš” ìƒíƒœë¡œ ë¶„ë¥˜: {case_id}")
            
            # HS ì½”ë“œ ì¹´í…Œê³ ë¦¬ ê²°ì •
            category = self._determine_hs_category(hs_code)
            
            logger.info(f"âœ… ì‹¤ì œ CBP ë°ì´í„° ë³€í™˜ ì„±ê³µ: {case_id} ({status})")
            
            return {
                'case_id': case_id,
                'title': title,
                'status': status,
                'date': datetime.now().strftime('%Y-%m-%d'),
                'description': content[:500] if content else f'CBP ruling {case_id} for HS code {hs_code}',
                'key_factors': [category, 'Tavily Search', 'CBP Data'],
                'hs_code': hs_code,
                'source': 'tavily_search',
                'link': url,
                'case_type': 'APPROVAL_CASE' if status == 'APPROVED' else 'DENIAL_CASE',
                'outcome': status,
                'reason': content[:200] if content else f'CBP ruling {case_id} for HS code {hs_code}'
            }
            
        except Exception as e:
            logger.error(f"âŒ Tavily ê²°ê³¼ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_case_id_from_url(self, url: str, title: str) -> str:
        """URLì´ë‚˜ ì œëª©ì—ì„œ CBP ê´€ë ¨ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ëŠìŠ¨í•œ ê¸°ì¤€)"""
        import re
        
        # URLì—ì„œ CBP ê´€ë ¨ ID íŒ¨í„´ ì°¾ê¸° (ë” ëŠìŠ¨í•˜ê²Œ)
        url_patterns = [
            r'ruling/([A-Z]\d{6})',     # ruling/N256328, ruling/W968396
            r'/(HQ[A-Z0-9]{6,})',       # HQ ruling
            r'/(NY[A-Z0-9]{6,})',       # NY ruling
            r'/([A-Z]\d{6})',           # ê¸°íƒ€ ruling
            r'/(R\d{6})',               # R ruling
            r'/([A-Z]{2}\d{6})',        # 2ê¸€ì+6ìˆ«ì
            r'/([A-Z]\d+)',             # ë” ëŠìŠ¨í•œ íŒ¨í„´
            r'term=([A-Z0-9.]+)',       # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ HSì½”ë“œë‚˜ ID
        ]
        
        for pattern in url_patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                ruling_id = match.group(1).upper()
                logger.info(f"âœ… URLì—ì„œ CBP ID ë°œê²¬: {ruling_id}")
                return ruling_id
        
        # ì œëª©ì—ì„œ CBP ê´€ë ¨ ID ì°¾ê¸°
        title_patterns = [
            r'\b(HQ\s*[A-Z0-9]{6,})\b',  # HQ ruling
            r'\b(NY\s*[A-Z0-9]{6,})\b',  # NY ruling
            r'\b([A-Z]{2}\d{6,})\b',     # 2ê¸€ì+6ìë¦¬ ì´ìƒ
            r'\b(R\d{6,})\b',            # R ruling
            r'\b([A-Z]\d{6,})\b',        # ë” ëŠìŠ¨í•œ íŒ¨í„´
        ]
        
        for pattern in title_patterns:
            match = re.search(pattern, title, re.IGNORECASE)
            if match:
                ruling_id = match.group(1).replace(" ", "").upper()
                logger.info(f"âœ… ì œëª©ì—ì„œ CBP ID ë°œê²¬: {ruling_id}")
                return ruling_id
        
        # URL í•´ì‹œë¡œ ê³ ìœ  ID ìƒì„± (ëŠìŠ¨í•œ ê¸°ì¤€ì—ì„œëŠ” í—ˆìš©)
        import hashlib
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        case_id = f'CBP-{url_hash}'
        logger.info(f"âœ… URL í•´ì‹œë¡œ ID ìƒì„±: {case_id}")
        return case_id
    
    def _determine_status_from_content(self, content: str, title: str) -> str:
        """ë‚´ìš©ì—ì„œ ìŠ¹ì¸/ê±°ë¶€ ìƒíƒœë¥¼ ì •í™•í•˜ê²Œ íŒë‹¨í•©ë‹ˆë‹¤."""
        content_lower = content.lower()
        title_lower = title.lower()
        combined = f"{title_lower} {content_lower}"
        
        # ê±°ë¶€/ì œì™¸ ê´€ë ¨ í‚¤ì›Œë“œì™€ íŒ¨í„´ (ë” ì •í™•í•˜ê²Œ)
        denial_patterns = [
            'denied', 'rejected', 'refused', 'prohibited', 'banned',
            'excluded from', 'not classified in', 'not classifiable',
            'not eligible', 'revoked', 'withdrawn', 'violation',
            'does not qualify', 'cannot be classified', 'improperly classified',
            'incorrectly classified', 'misclassified'
        ]
        
        # ìŠ¹ì¸ ê´€ë ¨ í‚¤ì›Œë“œì™€ íŒ¨í„´ (ë” ì •í™•í•˜ê²Œ)
        approval_patterns = [
            'approved for', 'classified in', 'classifiable in', 
            'properly classified', 'correctly classified',
            'meets the requirements', 'qualifies for',
            'authorized for import', 'permitted entry',
            'granted classification', 'ruling issued'
        ]
        
        # ê±°ë¶€/ì œì™¸ íŒ¨í„´ ìš°ì„  í™•ì¸ (ë” ì¤‘ìš”í•¨)
        denial_count = sum(1 for pattern in denial_patterns if pattern in combined)
        approval_count = sum(1 for pattern in approval_patterns if pattern in combined)
        
        # ëª…í™•í•œ íŒë‹¨ ê¸°ì¤€
        if denial_count > 0 and denial_count > approval_count:
            return 'DENIED'
        elif approval_count > 0 and approval_count > denial_count:
            return 'APPROVED'
        elif denial_count == approval_count and denial_count > 0:
            # ë™ì ì´ë©´ ë” êµ¬ì²´ì ì¸ ë¬¸êµ¬ í™•ì¸
            if 'excluded from classification' in combined or 'not classifiable' in combined:
                return 'DENIED'
            elif 'classified in subheading' in combined or 'properly classified' in combined:
                return 'APPROVED'
        
        # íŒë‹¨ ë¶ˆê°€
        return 'UNKNOWN'
    
    def _determine_hs_category(self, hs_code: str) -> str:
        """HS ì½”ë“œì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        hs_category = hs_code.split('.')[0] if '.' in hs_code else hs_code[:2]
        
        category_map = {
            '33': 'Cosmetic',
            '3304': 'Cosmetic',
            '21': 'Food',
            '2106': 'Food',
            '85': 'Electronics',
            '8517': 'Electronics',
            '84': 'Machinery',
            '90': 'Medical',
            '9018': 'Medical',
            '94': 'Furniture'
        }
        
        return category_map.get(hs_category, 'General')
    
    def _clean_and_deduplicate_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""
        # ì¤‘ë³µ ì œê±° (case_id ê¸°ì¤€)
        seen_ids = set()
        cleaned_data = []
        
        for item in data:
            case_id = item.get('case_id', '')
            if case_id and case_id not in seen_ids:
                seen_ids.add(case_id)
                cleaned_data.append(item)
        
        return cleaned_data
