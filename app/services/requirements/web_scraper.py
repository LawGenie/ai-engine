import httpx
import asyncio
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
import re
import json
from datetime import datetime

class WebScraper:
    """ì‹¤ì œ ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.timeout = 120.0  # 2ë¶„ìœ¼ë¡œ ì¦ê°€  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
        
        # HSì½”ë“œë³„ í‚¤ì›Œë“œ ë§¤í•‘ (í™•ì¥)
        self.hs_keywords = {
            "8471": ["computer", "data processing", "electronic", "equipment", "laptop", "notebook", "desktop"],
            "0901": ["coffee", "roasted", "ground", "instant", "coffee beans", "coffee products"],
            "3004": ["pharmaceutical", "medicine", "drug", "medical", "pharmaceutical products", "medicinal"],
            "8517": ["telecommunication", "radio", "wireless", "communication", "telephone", "mobile phone"],
            "2208": ["alcohol", "spirits", "liquor", "beverage", "alcoholic beverages", "distilled spirits"],
            "3304": ["cosmetics", "beauty", "makeup", "skincare", "facial", "serum", "cream"],
            "6404": ["footwear", "shoes", "sneakers", "boots", "sandals", "footwear products"],
            "6204": ["clothing", "garments", "apparel", "textile", "fashion", "clothes"]
        }
        
        # HSì½”ë“œë³„ ê·œì œê¸°ê´€ ë§¤í•‘
        self.hs_regulatory_mapping = {
            "8471": ["FCC", "CBP", "EPA"],  # ì „ìì œí’ˆ
            "0901": ["FDA", "USDA", "CBP"],  # ì»¤í”¼
            "3004": ["FDA", "CBP"],  # ì˜ì•½í’ˆ
            "8517": ["FCC", "CBP", "EPA"],  # í†µì‹ ê¸°ê¸°
            "2208": ["FDA", "CBP", "EPA"],  # ì£¼ë¥˜
            "3304": ["FDA", "CBP"],  # í™”ì¥í’ˆ
            "6404": ["CPSC", "CBP"],  # ì‹ ë°œ
            "6204": ["CPSC", "CBP"]  # ì˜ë¥˜
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    async def scrape_fda_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """FDA ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” FDA ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0] if '.' in hs_code else hs_code[:4]
        keywords = self.hs_keywords.get(hs_prefix, [])
        print(f"  ğŸ” HSì½”ë“œ {hs_prefix} í‚¤ì›Œë“œ: {keywords}")
        
        # URL ì„ íƒ ë¡œì§
        if url_override:
            print(f"  ğŸ¯ Tavilyì—ì„œ ì°¾ì€ FDA URL ì‚¬ìš©: {url_override}")
            urls_to_try = [url_override]
        else:
            print(f"  ğŸ”„ ê¸°ë³¸ FDA URL ì‚¬ìš©")
            urls_to_try = ["https://www.fda.gov/food/importing-food-products-imported-food"]
        
        for i, url in enumerate(urls_to_try, 1):
            print(f"  ğŸ“¡ FDA URL ì‹œë„ {i}/{len(urls_to_try)}: {url}")
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers, follow_redirects=True) as client:
                    response = await client.get(url)
                    
                    print(f"  ğŸ“Š FDA ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    print(f"  ğŸ“Š FDA ìµœì¢… URL: {response.url}")
                    print(f"  ğŸ“Š FDA ì½˜í…ì¸  ê¸¸ì´: {len(response.text)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"  ğŸ“„ FDA í˜ì´ì§€ ì œëª©: {title.text if title else 'No title'}")
                        
                        # FDA ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ (HSì½”ë“œ ê¸°ë°˜)
                        requirements = self._extract_fda_requirements(soup, hs_code, keywords)
                        print(f"  âœ… FDA ìŠ¤í¬ë˜í•‘ ì„±ê³µ: ì¸ì¦ {len(requirements.get('certifications', []))}ê°œ, ì„œë¥˜ {len(requirements.get('documents', []))}ê°œ")
                        
                        # ì›ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ
                        page_content = soup.get_text()[:2000]  # ì²˜ìŒ 2000ìë§Œ
                        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
                        if main_content:
                            main_text = main_content.get_text()[:1500]  # ë©”ì¸ ì½˜í…ì¸  1500ì
                        else:
                            main_text = page_content[:1500]
                        
                        return {
                            "agency": "FDA",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [
                                {
                                    "title": title.text if title else "FDA Food Import Guide",
                                    "url": str(response.url),
                                    "type": "ê³µì‹ ê°€ì´ë“œ",
                                    "relevance": "high",
                                    "raw_content": {
                                        "page_title": title.text if title else "No title",
                                        "main_content": main_text,
                                        "full_content_preview": page_content,
                                        "content_length": len(response.text),
                                        "scraped_at": datetime.now().isoformat()
                                    }
                                }
                            ],
                            "hs_code_matched": True,
                            "hs_code_used": hs_code,
                            "keywords_used": keywords,
                            "raw_page_data": {
                                "url": str(response.url),
                                "status_code": response.status_code,
                                "content_length": len(response.text),
                                "title": title.text if title else "No title",
                                "main_content": main_text
                            }
                        }
                    else:
                        print(f"  âŒ FDA ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                        if i < len(urls_to_try):
                            print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                            continue
                        else:
                            raise Exception(f"HTTP {response.status_code}")
            except Exception as e:
                print(f"  âŒ FDA URL {i} ì‹¤íŒ¨: {e}")
                if i < len(urls_to_try):
                    print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print(f"âŒ FDA ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {e}")
                    return {
                        "agency": "FDA",
                        "certifications": [],
                        "documents": [],
                        "sources": [],
                        "error": str(e)
                    }
    
    async def scrape_fcc_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """FCC ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” FCC ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # URL ì„ íƒ ë¡œì§
        if url_override:
            print(f"  ğŸ¯ Tavilyì—ì„œ ì°¾ì€ FCC URL ì‚¬ìš©: {url_override}")
            urls_to_try = [url_override]
        else:
            print(f"  ğŸ”„ ê¸°ë³¸ FCC URL ì‚¬ìš©")
            urls_to_try = ["https://www.fcc.gov/device-authorization"]
        
        for i, url in enumerate(urls_to_try, 1):
            print(f"  ğŸ“¡ FCC URL ì‹œë„ {i}/{len(urls_to_try)}: {url}")
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers, follow_redirects=True) as client:
                    response = await client.get(url)
                    
                    print(f"  ğŸ“Š FCC ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    print(f"  ğŸ“Š FCC ìµœì¢… URL: {response.url}")
                    print(f"  ğŸ“Š FCC ì½˜í…ì¸  ê¸¸ì´: {len(response.text)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"  ğŸ“„ FCC í˜ì´ì§€ ì œëª©: {title.text if title else 'No title'}")
                        
                        # FCC ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ
                        requirements = self._extract_fcc_requirements(soup, hs_code)
                        print(f"  âœ… FCC ìŠ¤í¬ë˜í•‘ ì„±ê³µ: ì¸ì¦ {len(requirements.get('certifications', []))}ê°œ, ì„œë¥˜ {len(requirements.get('documents', []))}ê°œ")
                        
                        return {
                            "agency": "FCC",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [
                                {
                                    "title": title.text if title else "FCC Device Authorization Guide",
                                    "url": str(response.url),
                                    "type": "ê³µì‹ ê°€ì´ë“œ",
                                    "relevance": "high"
                                }
                            ]
                        }
                    else:
                        print(f"  âŒ FCC ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                        if i < len(urls_to_try):
                            print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                            continue
                        else:
                            raise Exception(f"HTTP {response.status_code}")
            except Exception as e:
                print(f"  âŒ FCC URL {i} ì‹¤íŒ¨: {e}")
                if i < len(urls_to_try):
                    print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print(f"âŒ FCC ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {e}")
                    return {
                        "agency": "FCC",
                        "certifications": [],
                        "documents": [],
                        "sources": [],
                        "error": str(e)
                    }
    
    async def scrape_cbp_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """CBP ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” CBP ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # URL ì„ íƒ ë¡œì§
        if url_override:
            print(f"  ğŸ¯ Tavilyì—ì„œ ì°¾ì€ CBP URL ì‚¬ìš©: {url_override}")
            urls_to_try = [url_override]
        else:
            print(f"  ğŸ”„ ê¸°ë³¸ CBP URL ì‚¬ìš©")
            urls_to_try = ["https://www.cbp.gov/trade/basic-import-export"]
        
        for i, url in enumerate(urls_to_try, 1):
            print(f"  ğŸ“¡ CBP URL ì‹œë„ {i}/{len(urls_to_try)}: {url}")
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers, follow_redirects=True) as client:
                    response = await client.get(url)
                    
                    print(f"  ğŸ“Š CBP ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    print(f"  ğŸ“Š CBP ìµœì¢… URL: {response.url}")
                    print(f"  ğŸ“Š CBP ì½˜í…ì¸  ê¸¸ì´: {len(response.text)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"  ğŸ“„ CBP í˜ì´ì§€ ì œëª©: {title.text if title else 'No title'}")
                        
                        # CBP ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ
                        requirements = self._extract_cbp_requirements(soup, hs_code)
                        print(f"  âœ… CBP ìŠ¤í¬ë˜í•‘ ì„±ê³µ: ì¸ì¦ {len(requirements.get('certifications', []))}ê°œ, ì„œë¥˜ {len(requirements.get('documents', []))}ê°œ")
                        
                        return {
                            "agency": "CBP",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [
                                {
                                    "title": title.text if title else "CBP Entry Summary Guide",
                                    "url": str(response.url),
                                    "type": "ê³µì‹ ê°€ì´ë“œ",
                                    "relevance": "high"
                                }
                            ]
                        }
                    else:
                        print(f"  âŒ CBP ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                        if i < len(urls_to_try):
                            print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                            continue
                        else:
                            raise Exception(f"HTTP {response.status_code}")
            except Exception as e:
                print(f"  âŒ CBP URL {i} ì‹¤íŒ¨: {e}")
                if i < len(urls_to_try):
                    print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print(f"âŒ CBP ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {e}")
                    return {
                        "agency": "CBP",
                        "certifications": [],
                        "documents": [],
                        "sources": [],
                        "error": str(e)
                    }
    
    def _extract_fda_requirements(self, soup: BeautifulSoup, hs_code: str, keywords: List[str] = None) -> Dict:
        """FDA í˜ì´ì§€ì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ (HSì½”ë“œ ê¸°ë°˜)"""
        certifications = []
        documents = []
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0] if '.' in hs_code else hs_code[:4]
        if keywords is None:
            keywords = self.hs_keywords.get(hs_prefix, [])
        
        print(f"  ğŸ” FDA í‚¤ì›Œë“œ ë§¤ì¹­: {keywords} (HSì½”ë“œ: {hs_code})")
        
        # ì‹¤ì œ ì›¹ ì½˜í…ì¸ ì—ì„œ FDA ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # FDA ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            fda_sections = soup.find_all(['div', 'section'], class_=re.compile(r'fda|food|import', re.I))
            print(f"  ğŸ“„ FDA ê´€ë ¨ ì„¹ì…˜ ë°œê²¬: {len(fda_sections)}ê°œ")
            
            # ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ ì¸ì¦ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            for section in fda_sections:
                text = section.get_text().lower()
                if any(keyword in text for keyword in keywords):
                    print(f"  âœ… FDA í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {[k for k in keywords if k in text]}")
                    # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©
                    break
            else:
                print(f"  âŒ FDA í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {keywords}")
                
        except Exception as e:
            print(f"  âŒ FDA ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # í•˜ë“œì½”ë”©ëœ fallback ìš”êµ¬ì‚¬í•­ ì œê±°ë¨
        # ì‹¤ì œ ì›¹ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë§Œ ë°˜í™˜
        print(f"  ğŸ“ í•˜ë“œì½”ë”©ëœ fallback ìš”êµ¬ì‚¬í•­ ì œê±°ë¨ - ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë§Œ ë°˜í™˜")
        
        return {
            "certifications": certifications,
            "documents": documents
        }
    
    def _extract_fcc_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """FCC í˜ì´ì§€ì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ"""
        certifications = []
        documents = []
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0]
        keywords = self.hs_keywords.get(hs_prefix, [])
        
        print(f"  ğŸ” FCC í‚¤ì›Œë“œ ë§¤ì¹­: {keywords}")
        
        # ì‹¤ì œ ì›¹ ì½˜í…ì¸ ì—ì„œ FCC ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # FCC ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            fcc_sections = soup.find_all(['div', 'section'], class_=re.compile(r'fcc|device|authorization', re.I))
            print(f"  ğŸ“„ FCC ê´€ë ¨ ì„¹ì…˜ ë°œê²¬: {len(fcc_sections)}ê°œ")
            
            # ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ ì¸ì¦ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            for section in fcc_sections:
                text = section.get_text().lower()
                if any(keyword in text for keyword in keywords):
                    print(f"  âœ… FCC í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {[k for k in keywords if k in text]}")
                    # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©
                    break
            else:
                print(f"  âŒ FCC í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {keywords}")
                
        except Exception as e:
            print(f"  âŒ FCC ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # HSì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (í´ë°±)
        try:
            # HSì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ
            if hs_code.startswith("84") or hs_code.startswith("85"):  # ì „ìê¸°ê¸°
                certifications.append({
                    "name": "FCC ì¸ì¦",
                    "required": True,
                    "description": "ì „ìê¸°ê¸°ë¡œ ë¶„ë¥˜ë˜ëŠ” ìƒí’ˆì˜ ê²½ìš° FCC ì¸ì¦ í•„ìš”",
                    "agency": "FCC",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                documents.append({
                    "name": "FCC ì¸ì¦ì„œ",
                    "required": True,
                    "description": "FCCì—ì„œ ë°œê¸‰í•œ ê¸°ê¸° ì¸ì¦ì„œ",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                documents.append({
                    "name": "ê¸°ìˆ  ë¬¸ì„œ",
                    "required": True,
                    "description": "ê¸°ê¸°ì˜ ê¸°ìˆ ì  ì‚¬ì–‘ì´ í¬í•¨ëœ ë¬¸ì„œ",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                print(f"  âœ… FCC ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
            
            else:
                # ì¼ë°˜ì ì¸ FCC ìš”êµ¬ì‚¬í•­
                documents.append({
                    "name": "ê¸°ê¸° ì‚¬ì–‘ì„œ",
                    "required": True,
                    "description": "ê¸°ê¸°ì˜ ê¸°ìˆ ì  ì‚¬ì–‘ì´ í¬í•¨ëœ ë¬¸ì„œ",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                print(f"  âœ… FCC ì¼ë°˜ ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
                
        except Exception as e:
            print(f"  âŒ FCC ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "certifications": certifications,
            "documents": documents
        }
    
    def _extract_cbp_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """CBP í˜ì´ì§€ì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ"""
        certifications = []
        documents = []
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0]
        keywords = self.hs_keywords.get(hs_prefix, [])
        
        print(f"  ğŸ” CBP í‚¤ì›Œë“œ ë§¤ì¹­: {keywords}")
        
        # ì‹¤ì œ ì›¹ ì½˜í…ì¸ ì—ì„œ CBP ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # CBP ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            cbp_sections = soup.find_all(['div', 'section'], class_=re.compile(r'cbp|import|export|customs', re.I))
            print(f"  ğŸ“„ CBP ê´€ë ¨ ì„¹ì…˜ ë°œê²¬: {len(cbp_sections)}ê°œ")
            
            # ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ ì¸ì¦ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            for section in cbp_sections:
                text = section.get_text().lower()
                if any(keyword in text for keyword in keywords):
                    print(f"  âœ… CBP í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {[k for k in keywords if k in text]}")
                    # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©
                    break
            else:
                print(f"  âŒ CBP í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {keywords}")
                
        except Exception as e:
            print(f"  âŒ CBP ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (ê°•í™”ëœ í´ë°±)
        print(f"  ğŸ”„ CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ í´ë°±)")
        
        # CBPëŠ” ëª¨ë“  ìƒí’ˆì— ëŒ€í•´ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ
        documents.append({
            "name": "ğŸ“‹ [ê¸°ë³¸] ìƒì—…ì  ì†¡ì¥",
            "required": True,
            "description": "ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ê°€ í¬í•¨ëœ ìƒì—…ì  ì†¡ì¥ (CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
            "url": "https://www.cbp.gov/trade/basic-import-export"
        })
        
        documents.append({
            "name": "ğŸ“‹ [ê¸°ë³¸] í¬ì¥ ëª…ì„¸ì„œ",
            "required": True,
            "description": "í¬ì¥ ë‚´ìš©ê³¼ ìˆ˜ëŸ‰ì„ ëª…ì‹œí•œ ëª…ì„¸ì„œ (CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
            "url": "https://www.cbp.gov/trade/basic-import-export"
        })
        
        documents.append({
            "name": "ğŸ“‹ [ê¸°ë³¸] ì›ì‚°ì§€ ì¦ëª…ì„œ",
            "required": True,
            "description": "ìƒí’ˆì˜ ì›ì‚°ì§€ë¥¼ ì¦ëª…í•˜ëŠ” ì„œë¥˜ (CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
            "url": "https://www.cbp.gov/trade/basic-import-export"
        })
        
        # íŠ¹ì • HSì½”ë“œì— ëŒ€í•œ ì¶”ê°€ ìš”êµ¬ì‚¬í•­
        if hs_code.startswith("30"):  # ì˜ë£Œìš©í’ˆ
            documents.append({
                "name": "ğŸ“‹ [ê¸°ë³¸] ì˜ë£Œê¸°ê¸° ë“±ë¡ì¦",
                "required": True,
                "description": "FDA ë“±ë¡ëœ ì˜ë£Œê¸°ê¸°ì„ì„ ì¦ëª…í•˜ëŠ” ì„œë¥˜ (ì˜ë£Œìš©í’ˆ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
                "url": "https://www.cbp.gov/trade/basic-import-export"
            })
        
        print(f"  âœ… CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ {len(documents)}ê°œ ì œê³µ ì™„ë£Œ")
        
        return {
            "certifications": certifications,
            "documents": documents
        }
    
    # ë¯¸êµ­ ì •ë¶€ ê¸°ê´€ ì¶”ê°€ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œë“¤
    async def scrape_usda_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """USDA ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë†ì‚°ë¬¼ ìˆ˜ì…ìš”ê±´ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” USDA ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # USDA ê¸°ë³¸ URL ë˜ëŠ” ì˜¤ë²„ë¼ì´ë“œ URL ì‚¬ìš©
        urls_to_try = [url_override] if url_override else ["https://www.usda.gov/topics/trade"]
        
        for i, url in enumerate(urls_to_try, 1):
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(url)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # USDA ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
                        requirements = self._extract_usda_requirements(soup, hs_code)
                        
                        return {
                            "agency": "USDA",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [{
                                "title": "USDA Trade Information",
                                "url": str(response.url),
                                "type": "ê³µì‹ ê°€ì´ë“œ",
                                "relevance": "high"
                            }]
                        }
            except Exception as e:
                print(f"  âŒ USDA URL {i} ì‹¤íŒ¨: {e}")
                continue
        
        # í´ë°±: HSì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­
        return self._get_usda_fallback_requirements(hs_code)
    
    async def scrape_epa_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """EPA ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™˜ê²½ê·œì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” EPA ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        urls_to_try = [url_override] if url_override else ["https://www.epa.gov/import-export"]
        
        for i, url in enumerate(urls_to_try, 1):
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(url)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        requirements = self._extract_epa_requirements(soup, hs_code)
                        
                        return {
                            "agency": "EPA",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [{
                                "title": "EPA Import Export Guide",
                                "url": str(response.url),
                                "type": "ê³µì‹ ê°€ì´ë“œ",
                                "relevance": "high"
                            }]
                        }
            except Exception as e:
                print(f"  âŒ EPA URL {i} ì‹¤íŒ¨: {e}")
                continue
        
        return self._get_epa_fallback_requirements(hs_code)
    
    async def scrape_cpsc_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """CPSC ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì†Œë¹„ìì œí’ˆ ì•ˆì „ìš”ê±´ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” CPSC ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        urls_to_try = [url_override] if url_override else ["https://www.cpsc.gov/Business--Manufacturing"]
        
        for i, url in enumerate(urls_to_try, 1):
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(url)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        requirements = self._extract_cpsc_requirements(soup, hs_code)
                        
                        return {
                            "agency": "CPSC",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [{
                                "title": "CPSC Business Manufacturing",
                                "url": str(response.url),
                                "type": "ê³µì‹ ê°€ì´ë“œ",
                                "relevance": "high"
                            }]
                        }
            except Exception as e:
                print(f"  âŒ CPSC URL {i} ì‹¤íŒ¨: {e}")
                continue
        
        return self._get_cpsc_fallback_requirements(hs_code)
    
    # í•œêµ­ ì •ë¶€ ê¸°ê´€ ìŠ¤í¬ë˜í•‘ ë©”ì„œë“œë“¤
    async def scrape_kcs_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """í•œêµ­ ê´€ì„¸ì²­ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìˆ˜ì…ìš”ê±´ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” í•œêµ­ ê´€ì„¸ì²­ ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        urls_to_try = [url_override] if url_override else ["https://www.customs.go.kr/kcshome/main/content/ContentC.menu?contentId=CONTENT_000001000004"]
        
        for i, url in enumerate(urls_to_try, 1):
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(url)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        requirements = self._extract_kcs_requirements(soup, hs_code)
                        
                        return {
                            "agency": "KCS",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [{
                                "title": "í•œêµ­ ê´€ì„¸ì²­ ìˆ˜ì…ìš”ê±´",
                                "url": str(response.url),
                                "type": "ê³µì‹ ê°€ì´ë“œ",
                                "relevance": "high"
                            }]
                        }
            except Exception as e:
                print(f"  âŒ KCS URL {i} ì‹¤íŒ¨: {e}")
                continue
        
        return self._get_kcs_fallback_requirements(hs_code)
    
    async def scrape_mfds_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìˆ˜ì…ìš”ê±´ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        urls_to_try = [url_override] if url_override else ["https://www.mfds.go.kr/brd/m_99/list.do"]
        
        for i, url in enumerate(urls_to_try, 1):
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(url)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        requirements = self._extract_mfds_requirements(soup, hs_code)
                        
                        return {
                            "agency": "MFDS",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [{
                                "title": "ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ìˆ˜ì…ìš”ê±´",
                                "url": str(response.url),
                                "type": "ê³µì‹ ê°€ì´ë“œ",
                                "relevance": "high"
                            }]
                        }
            except Exception as e:
                print(f"  âŒ MFDS URL {i} ì‹¤íŒ¨: {e}")
                continue
        
        return self._get_mfds_fallback_requirements(hs_code)
    
    async def scrape_motie_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """ì‚°ì—…í†µìƒìì›ë¶€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìˆ˜ì…ìš”ê±´ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” ì‚°ì—…í†µìƒìì›ë¶€ ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        urls_to_try = [url_override] if url_override else ["https://www.motie.go.kr/motie/ne/policy/policyview.do?bbs=bbs&bbs_cd_n=81&seq=162895"]
        
        for i, url in enumerate(urls_to_try, 1):
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers) as client:
                    response = await client.get(url)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        requirements = self._extract_motie_requirements(soup, hs_code)
                        
                        return {
                            "agency": "MOTIE",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [{
                                "title": "ì‚°ì—…í†µìƒìì›ë¶€ ìˆ˜ì…ìš”ê±´",
                                "url": str(response.url),
                                "type": "ê³µì‹ ê°€ì´ë“œ",
                                "relevance": "high"
                            }]
                        }
            except Exception as e:
                print(f"  âŒ MOTIE URL {i} ì‹¤íŒ¨: {e}")
                continue
        
        return self._get_motie_fallback_requirements(hs_code)
    
    # í´ë°± ë©”ì„œë“œë“¤ (ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ)
    def _get_usda_fallback_requirements(self, hs_code: str) -> Dict:
        """USDA í´ë°± ìš”êµ¬ì‚¬í•­"""
        certifications = []
        documents = []
        
        if hs_code.startswith("01") or hs_code.startswith("02"):  # ë†ì‚°ë¬¼
            certifications.append({
                "name": "USDA ë†ì‚°ë¬¼ ê²€ì—­",
                "required": True,
                "description": "ë†ì‚°ë¬¼ ìˆ˜ì… ì‹œ USDA ê²€ì—­ í•„ìˆ˜",
                "agency": "USDA",
                "url": "https://www.usda.gov/topics/trade"
            })
            
            documents.append({
                "name": "ì‹ë¬¼ê²€ì—­ì¦ëª…ì„œ",
                "required": True,
                "description": "ì›ì‚°ì§€ì—ì„œ ë°œê¸‰í•œ ì‹ë¬¼ê²€ì—­ì¦ëª…ì„œ",
                "url": "https://www.usda.gov/topics/trade"
            })
        
        return {
            "agency": "USDA",
            "certifications": certifications,
            "documents": documents,
            "sources": []
        }
    
    def _get_epa_fallback_requirements(self, hs_code: str) -> Dict:
        """EPA í´ë°± ìš”êµ¬ì‚¬í•­"""
        certifications = []
        documents = []
        
        if hs_code.startswith("28") or hs_code.startswith("29"):  # í™”í•™ë¬¼ì§ˆ
            certifications.append({
                "name": "EPA í™”í•™ë¬¼ì§ˆ ë“±ë¡",
                "required": True,
                "description": "í™”í•™ë¬¼ì§ˆ ìˆ˜ì… ì‹œ EPA ë“±ë¡ í•„ìš”",
                "agency": "EPA",
                "url": "https://www.epa.gov/import-export"
            })
        
        return {
            "agency": "EPA",
            "certifications": certifications,
            "documents": documents,
            "sources": []
        }
    
    def _get_cpsc_fallback_requirements(self, hs_code: str) -> Dict:
        """CPSC í´ë°± ìš”êµ¬ì‚¬í•­"""
        certifications = []
        documents = []
        
        if hs_code.startswith("95") or hs_code.startswith("96"):  # ì†Œë¹„ìì œí’ˆ
            certifications.append({
                "name": "CPSC ì•ˆì „ ì¸ì¦",
                "required": True,
                "description": "ì†Œë¹„ìì œí’ˆ ì•ˆì „ê¸°ì¤€ ì¤€ìˆ˜ ì¸ì¦",
                "agency": "CPSC",
                "url": "https://www.cpsc.gov/Business--Manufacturing"
            })
        
        return {
            "agency": "CPSC",
            "certifications": certifications,
            "documents": documents,
            "sources": []
        }
    
    def _get_kcs_fallback_requirements(self, hs_code: str) -> Dict:
        """í•œêµ­ ê´€ì„¸ì²­ í´ë°± ìš”êµ¬ì‚¬í•­"""
        certifications = []
        documents = []
        
        documents.append({
            "name": "ìˆ˜ì…ì‹ ê³ ì„œ",
            "required": True,
            "description": "í•œêµ­ ê´€ì„¸ì²­ ìˆ˜ì…ì‹ ê³  í•„ìˆ˜",
            "url": "https://www.customs.go.kr"
        })
        
        documents.append({
            "name": "ìƒì—…ì†¡ì¥",
            "required": True,
            "description": "ìˆ˜ì¶œìê°€ ë°œê¸‰í•œ ìƒì—…ì†¡ì¥",
            "url": "https://www.customs.go.kr"
        })
        
        return {
            "agency": "KCS",
            "certifications": certifications,
            "documents": documents,
            "sources": []
        }
    
    def _get_mfds_fallback_requirements(self, hs_code: str) -> Dict:
        """ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ í´ë°± ìš”êµ¬ì‚¬í•­"""
        certifications = []
        documents = []
        
        if hs_code.startswith("30"):  # ì˜ì•½í’ˆ
            certifications.append({
                "name": "ì‹ì•½ì²˜ ì˜ì•½í’ˆ í—ˆê°€",
                "required": True,
                "description": "ì˜ì•½í’ˆ ìˆ˜ì… ì‹œ ì‹ì•½ì²˜ í—ˆê°€ í•„ìš”",
                "agency": "MFDS",
                "url": "https://www.mfds.go.kr"
            })
        
        return {
            "agency": "MFDS",
            "certifications": certifications,
            "documents": documents,
            "sources": []
        }
    
    def _get_motie_fallback_requirements(self, hs_code: str) -> Dict:
        """ì‚°ì—…í†µìƒìì›ë¶€ í´ë°± ìš”êµ¬ì‚¬í•­"""
        certifications = []
        documents = []
        
        # ì¼ë°˜ì ì¸ ìˆ˜ì…ìš”ê±´
        documents.append({
            "name": "ìˆ˜ì…ì‹ ê³ ì„œ",
            "required": True,
            "description": "ì‚°ì—…í†µìƒìì›ë¶€ ìˆ˜ì…ì‹ ê³ ",
            "url": "https://www.motie.go.kr"
        })
        
        return {
            "agency": "MOTIE",
            "certifications": certifications,
            "documents": documents,
            "sources": []
        }
    
    # ì¶”ì¶œ ë©”ì„œë“œë“¤ (ì‹¤ì œ êµ¬í˜„ì€ ê°„ë‹¨í•œ ë²„ì „)
    def _extract_usda_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """USDA ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        return {"certifications": [], "documents": []}
    
    def _extract_epa_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """EPA ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        return {"certifications": [], "documents": []}
    
    def _extract_cpsc_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """CPSC ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        return {"certifications": [], "documents": []}
    
    def _extract_kcs_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """í•œêµ­ ê´€ì„¸ì²­ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        return {"certifications": [], "documents": []}
    
    def _extract_mfds_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        return {"certifications": [], "documents": []}
    
    def _extract_motie_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """ì‚°ì—…í†µìƒìì›ë¶€ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""
        return {"certifications": [], "documents": []}