import httpx
import asyncio
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
import re
import json

class WebScraper:
    """ì‹¤ì œ ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.timeout = 120.0  # 2ë¶„ìœ¼ë¡œ ì¦ê°€  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
        
        # HSì½”ë“œë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.hs_keywords = {
            "8471": ["computer", "data processing", "electronic", "equipment"],
            "0901": ["coffee", "roasted", "ground", "instant"],
            "3004": ["pharmaceutical", "medicine", "drug", "medical"],
            "8517": ["telecommunication", "radio", "wireless", "communication"],
            "2208": ["alcohol", "spirits", "liquor", "beverage"]
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    async def scrape_fda_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """FDA ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” FDA ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # URL ì„ íƒ ë¡œì§
        if url_override:
            print(f"  ğŸ¯ Tavilyì—ì„œ ì°¾ì€ FDA URL ì‚¬ìš©: {url_override}")
            urls_to_try = [url_override]
        else:
            print(f"  ğŸ”„ ê¸°ë³¸ FDA URL ì‚¬ìš©")
            urls_to_try = ["https://www.fda.gov/food/importing-food-products-imported-food"]
        
        for i, url in enumerate(urls_to_try, 1):
            print(f"  ğŸ“¡ FDA URL ì‹œë„ {i}/{len(urls_to_try)}: {url}")
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers, follow_redirects=True) as client:
                    response = await client.get(url)
                    
                    print(f"  ğŸ“Š FDA ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    print(f"  ğŸ“Š FDA ìµœì¢… URL: {response.url}")
                    print(f"  ğŸ“Š FDA ì½˜í…ì¸  ê¸¸ì´: {len(response.text)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"  ğŸ“„ FDA í˜ì´ì§€ ì œëª©: {title.text if title else 'No title'}")
                        
                        # FDA ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ
                        requirements = self._extract_fda_requirements(soup, hs_code)
                        print(f"  âœ… FDA ìŠ¤í¬ë˜í•‘ ì„±ê³µ: ì¸ì¦ {len(requirements.get('certifications', []))}ê°œ, ì„œë¥˜ {len(requirements.get('documents', []))}ê°œ")
                        
                        return {
                            "agency": "FDA",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [
                                {
                                    "title": title.text if title else "FDA Food Import Guide",
                                    "url": str(response.url),
                                    "type": "ê³µì‹ ê°€ì´ë“œ",
                                    "relevance": "high"
                                }
                            ]
                        }
                    else:
                        print(f"  âŒ FDA ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                        if i < len(urls_to_try):
                            print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                            continue
                        else:
                            raise Exception(f"HTTP {response.status_code}")
            except Exception as e:
                print(f"  âŒ FDA URL {i} ì‹¤íŒ¨: {e}")
                if i < len(urls_to_try):
                    print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print(f"âŒ FDA ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {e}")
                    return {
                        "agency": "FDA",
                        "certifications": [],
                        "documents": [],
                        "sources": [],
                        "error": str(e)
                    }
    
    async def scrape_fcc_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """FCC ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” FCC ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # URL ì„ íƒ ë¡œì§
        if url_override:
            print(f"  ğŸ¯ Tavilyì—ì„œ ì°¾ì€ FCC URL ì‚¬ìš©: {url_override}")
            urls_to_try = [url_override]
        else:
            print(f"  ğŸ”„ ê¸°ë³¸ FCC URL ì‚¬ìš©")
            urls_to_try = ["https://www.fcc.gov/device-authorization"]
        
        for i, url in enumerate(urls_to_try, 1):
            print(f"  ğŸ“¡ FCC URL ì‹œë„ {i}/{len(urls_to_try)}: {url}")
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers, follow_redirects=True) as client:
                    response = await client.get(url)
                    
                    print(f"  ğŸ“Š FCC ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    print(f"  ğŸ“Š FCC ìµœì¢… URL: {response.url}")
                    print(f"  ğŸ“Š FCC ì½˜í…ì¸  ê¸¸ì´: {len(response.text)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"  ğŸ“„ FCC í˜ì´ì§€ ì œëª©: {title.text if title else 'No title'}")
                        
                        # FCC ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ
                        requirements = self._extract_fcc_requirements(soup, hs_code)
                        print(f"  âœ… FCC ìŠ¤í¬ë˜í•‘ ì„±ê³µ: ì¸ì¦ {len(requirements.get('certifications', []))}ê°œ, ì„œë¥˜ {len(requirements.get('documents', []))}ê°œ")
                        
                        return {
                            "agency": "FCC",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [
                                {
                                    "title": title.text if title else "FCC Device Authorization Guide",
                                    "url": str(response.url),
                                    "type": "ê³µì‹ ê°€ì´ë“œ",
                                    "relevance": "high"
                                }
                            ]
                        }
                    else:
                        print(f"  âŒ FCC ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                        if i < len(urls_to_try):
                            print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                            continue
                        else:
                            raise Exception(f"HTTP {response.status_code}")
            except Exception as e:
                print(f"  âŒ FCC URL {i} ì‹¤íŒ¨: {e}")
                if i < len(urls_to_try):
                    print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print(f"âŒ FCC ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {e}")
                    return {
                        "agency": "FCC",
                        "certifications": [],
                        "documents": [],
                        "sources": [],
                        "error": str(e)
                    }
    
    async def scrape_cbp_requirements(self, hs_code: str, url_override: Optional[str] = None) -> Dict:
        """CBP ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ” CBP ìŠ¤í¬ë˜í•‘ ì‹œì‘ - HSì½”ë“œ: {hs_code}")
        
        # URL ì„ íƒ ë¡œì§
        if url_override:
            print(f"  ğŸ¯ Tavilyì—ì„œ ì°¾ì€ CBP URL ì‚¬ìš©: {url_override}")
            urls_to_try = [url_override]
        else:
            print(f"  ğŸ”„ ê¸°ë³¸ CBP URL ì‚¬ìš©")
            urls_to_try = ["https://www.cbp.gov/trade/basic-import-export"]
        
        for i, url in enumerate(urls_to_try, 1):
            print(f"  ğŸ“¡ CBP URL ì‹œë„ {i}/{len(urls_to_try)}: {url}")
            try:
                async with httpx.AsyncClient(timeout=self.timeout, headers=self.headers, follow_redirects=True) as client:
                    response = await client.get(url)
                    
                    print(f"  ğŸ“Š CBP ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    print(f"  ğŸ“Š CBP ìµœì¢… URL: {response.url}")
                    print(f"  ğŸ“Š CBP ì½˜í…ì¸  ê¸¸ì´: {len(response.text)}")
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        title = soup.find('title')
                        print(f"  ğŸ“„ CBP í˜ì´ì§€ ì œëª©: {title.text if title else 'No title'}")
                        
                        # CBP ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ
                        requirements = self._extract_cbp_requirements(soup, hs_code)
                        print(f"  âœ… CBP ìŠ¤í¬ë˜í•‘ ì„±ê³µ: ì¸ì¦ {len(requirements.get('certifications', []))}ê°œ, ì„œë¥˜ {len(requirements.get('documents', []))}ê°œ")
                        
                        return {
                            "agency": "CBP",
                            "certifications": requirements.get("certifications", []),
                            "documents": requirements.get("documents", []),
                            "sources": [
                                {
                                    "title": title.text if title else "CBP Entry Summary Guide",
                                    "url": str(response.url),
                                    "type": "ê³µì‹ ê°€ì´ë“œ",
                                    "relevance": "high"
                                }
                            ]
                        }
                    else:
                        print(f"  âŒ CBP ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                        if i < len(urls_to_try):
                            print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                            continue
                        else:
                            raise Exception(f"HTTP {response.status_code}")
            except Exception as e:
                print(f"  âŒ CBP URL {i} ì‹¤íŒ¨: {e}")
                if i < len(urls_to_try):
                    print(f"  ğŸ”„ ë‹¤ìŒ URLë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print(f"âŒ CBP ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {e}")
                    return {
                        "agency": "CBP",
                        "certifications": [],
                        "documents": [],
                        "sources": [],
                        "error": str(e)
                    }
    
    def _extract_fda_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """FDA í˜ì´ì§€ì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ"""
        certifications = []
        documents = []
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0]
        keywords = self.hs_keywords.get(hs_prefix, [])
        
        print(f"  ğŸ” FDA í‚¤ì›Œë“œ ë§¤ì¹­: {keywords}")
        
        # ì‹¤ì œ ì›¹ ì½˜í…ì¸ ì—ì„œ FDA ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # FDA ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            fda_sections = soup.find_all(['div', 'section'], class_=re.compile(r'fda|food|import', re.I))
            print(f"  ğŸ“„ FDA ê´€ë ¨ ì„¹ì…˜ ë°œê²¬: {len(fda_sections)}ê°œ")
            
            # ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ ì¸ì¦ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            for section in fda_sections:
                text = section.get_text().lower()
                if any(keyword in text for keyword in keywords):
                    print(f"  âœ… FDA í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {[k for k in keywords if k in text]}")
                    # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©
                    break
            else:
                print(f"  âŒ FDA í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {keywords}")
                
        except Exception as e:
            print(f"  âŒ FDA ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # HSì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (í´ë°±)
        try:
            # HSì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ
            if hs_code.startswith("09"):  # ì»¤í”¼, ì°¨ ë“±
                certifications.append({
                    "name": "FDA ì‹í’ˆ ë“±ë¡",
                    "required": True,
                    "description": "ì‹í’ˆìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ìƒí’ˆì˜ ê²½ìš° FDA ë“±ë¡ í•„ìš”",
                    "agency": "FDA",
                    "url": "https://www.fda.gov/food/importing-food-products-imported-food"
                })
                
                documents.append({
                    "name": "ìƒì—…ì  ì†¡ì¥",
                    "required": True,
                    "description": "ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ê°€ í¬í•¨ëœ ìƒì—…ì  ì†¡ì¥",
                    "url": "https://www.fda.gov/food/importing-food-products-imported-food"
                })
                
                documents.append({
                    "name": "ì›ì‚°ì§€ ì¦ëª…ì„œ",
                    "required": True,
                    "description": "ìƒí’ˆì˜ ì›ì‚°ì§€ë¥¼ ì¦ëª…í•˜ëŠ” ì„œë¥˜",
                    "url": "https://www.fda.gov/food/importing-food-products-imported-food"
                })
                
                print(f"  âœ… FDA ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
            
            elif hs_code.startswith("30"):  # ì˜ë£Œìš©í’ˆ
                certifications.append({
                    "name": "FDA ì˜ë£Œê¸°ê¸° ìŠ¹ì¸",
                    "required": True,
                    "description": "ì˜ë£Œê¸°ê¸°ë¡œ ë¶„ë¥˜ë˜ëŠ” ìƒí’ˆì˜ ê²½ìš° FDA ìŠ¹ì¸ í•„ìš”",
                    "agency": "FDA",
                    "url": "https://www.fda.gov/medical-devices/device-registration-and-listing"
                })
                
                documents.append({
                    "name": "ì˜ë£Œê¸°ê¸° ë“±ë¡ì¦",
                    "required": True,
                    "description": "FDAì— ë“±ë¡ëœ ì˜ë£Œê¸°ê¸°ì„ì„ ì¦ëª…í•˜ëŠ” ì„œë¥˜",
                    "url": "https://www.fda.gov/medical-devices/device-registration-and-listing"
                })
                
                print(f"  âœ… FDA ì˜ë£Œê¸°ê¸° ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
            
            else:
                # ì¼ë°˜ì ì¸ FDA ìš”êµ¬ì‚¬í•­
                documents.append({
                    "name": "ìƒì—…ì  ì†¡ì¥",
                    "required": True,
                    "description": "ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ê°€ í¬í•¨ëœ ìƒì—…ì  ì†¡ì¥",
                    "url": "https://www.fda.gov/food/importing-food-products-imported-food"
                })
                
                print(f"  âœ… FDA ì¼ë°˜ ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
                
        except Exception as e:
            print(f"  âŒ FDA ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "certifications": certifications,
            "documents": documents
        }
    
    def _extract_fcc_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """FCC í˜ì´ì§€ì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ"""
        certifications = []
        documents = []
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0]
        keywords = self.hs_keywords.get(hs_prefix, [])
        
        print(f"  ğŸ” FCC í‚¤ì›Œë“œ ë§¤ì¹­: {keywords}")
        
        # ì‹¤ì œ ì›¹ ì½˜í…ì¸ ì—ì„œ FCC ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # FCC ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            fcc_sections = soup.find_all(['div', 'section'], class_=re.compile(r'fcc|device|authorization', re.I))
            print(f"  ğŸ“„ FCC ê´€ë ¨ ì„¹ì…˜ ë°œê²¬: {len(fcc_sections)}ê°œ")
            
            # ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ ì¸ì¦ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            for section in fcc_sections:
                text = section.get_text().lower()
                if any(keyword in text for keyword in keywords):
                    print(f"  âœ… FCC í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {[k for k in keywords if k in text]}")
                    # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©
                    break
            else:
                print(f"  âŒ FCC í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {keywords}")
                
        except Exception as e:
            print(f"  âŒ FCC ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # HSì½”ë“œ ê¸°ë°˜ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (í´ë°±)
        try:
            # HSì½”ë“œ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ
            if hs_code.startswith("84") or hs_code.startswith("85"):  # ì „ìê¸°ê¸°
                certifications.append({
                    "name": "FCC ì¸ì¦",
                    "required": True,
                    "description": "ì „ìê¸°ê¸°ë¡œ ë¶„ë¥˜ë˜ëŠ” ìƒí’ˆì˜ ê²½ìš° FCC ì¸ì¦ í•„ìš”",
                    "agency": "FCC",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                documents.append({
                    "name": "FCC ì¸ì¦ì„œ",
                    "required": True,
                    "description": "FCCì—ì„œ ë°œê¸‰í•œ ê¸°ê¸° ì¸ì¦ì„œ",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                documents.append({
                    "name": "ê¸°ìˆ  ë¬¸ì„œ",
                    "required": True,
                    "description": "ê¸°ê¸°ì˜ ê¸°ìˆ ì  ì‚¬ì–‘ì´ í¬í•¨ëœ ë¬¸ì„œ",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                print(f"  âœ… FCC ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
            
            else:
                # ì¼ë°˜ì ì¸ FCC ìš”êµ¬ì‚¬í•­
                documents.append({
                    "name": "ê¸°ê¸° ì‚¬ì–‘ì„œ",
                    "required": True,
                    "description": "ê¸°ê¸°ì˜ ê¸°ìˆ ì  ì‚¬ì–‘ì´ í¬í•¨ëœ ë¬¸ì„œ",
                    "url": "https://www.fcc.gov/device-authorization"
                })
                
                print(f"  âœ… FCC ì¼ë°˜ ìš”êµ¬ì‚¬í•­ ì œê³µ (HSì½”ë“œ: {hs_code})")
                
        except Exception as e:
            print(f"  âŒ FCC ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "certifications": certifications,
            "documents": documents
        }
    
    def _extract_cbp_requirements(self, soup: BeautifulSoup, hs_code: str) -> Dict:
        """CBP í˜ì´ì§€ì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ ì¶”ì¶œ"""
        certifications = []
        documents = []
        
        # HSì½”ë“œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        hs_prefix = hs_code.split('.')[0]
        keywords = self.hs_keywords.get(hs_prefix, [])
        
        print(f"  ğŸ” CBP í‚¤ì›Œë“œ ë§¤ì¹­: {keywords}")
        
        # ì‹¤ì œ ì›¹ ì½˜í…ì¸ ì—ì„œ CBP ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # CBP ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            cbp_sections = soup.find_all(['div', 'section'], class_=re.compile(r'cbp|import|export|customs', re.I))
            print(f"  ğŸ“„ CBP ê´€ë ¨ ì„¹ì…˜ ë°œê²¬: {len(cbp_sections)}ê°œ")
            
            # ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ ì¸ì¦ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            for section in cbp_sections:
                text = section.get_text().lower()
                if any(keyword in text for keyword in keywords):
                    print(f"  âœ… CBP í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: {[k for k in keywords if k in text]}")
                    # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©
                    break
            else:
                print(f"  âŒ CBP í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: {keywords}")
                
        except Exception as e:
            print(f"  âŒ CBP ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (ê°•í™”ëœ í´ë°±)
        print(f"  ğŸ”„ CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ (ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ í´ë°±)")
        
        # CBPëŠ” ëª¨ë“  ìƒí’ˆì— ëŒ€í•´ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ì œê³µ
        documents.append({
            "name": "ğŸ“‹ [ê¸°ë³¸] ìƒì—…ì  ì†¡ì¥",
            "required": True,
            "description": "ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ê°€ í¬í•¨ëœ ìƒì—…ì  ì†¡ì¥ (CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
            "url": "https://www.cbp.gov/trade/basic-import-export"
        })
        
        documents.append({
            "name": "ğŸ“‹ [ê¸°ë³¸] í¬ì¥ ëª…ì„¸ì„œ",
            "required": True,
            "description": "í¬ì¥ ë‚´ìš©ê³¼ ìˆ˜ëŸ‰ì„ ëª…ì‹œí•œ ëª…ì„¸ì„œ (CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
            "url": "https://www.cbp.gov/trade/basic-import-export"
        })
        
        documents.append({
            "name": "ğŸ“‹ [ê¸°ë³¸] ì›ì‚°ì§€ ì¦ëª…ì„œ",
            "required": True,
            "description": "ìƒí’ˆì˜ ì›ì‚°ì§€ë¥¼ ì¦ëª…í•˜ëŠ” ì„œë¥˜ (CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
            "url": "https://www.cbp.gov/trade/basic-import-export"
        })
        
        # íŠ¹ì • HSì½”ë“œì— ëŒ€í•œ ì¶”ê°€ ìš”êµ¬ì‚¬í•­
        if hs_code.startswith("30"):  # ì˜ë£Œìš©í’ˆ
            documents.append({
                "name": "ğŸ“‹ [ê¸°ë³¸] ì˜ë£Œê¸°ê¸° ë“±ë¡ì¦",
                "required": True,
                "description": "FDA ë“±ë¡ëœ ì˜ë£Œê¸°ê¸°ì„ì„ ì¦ëª…í•˜ëŠ” ì„œë¥˜ (ì˜ë£Œìš©í’ˆ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­)",
                "url": "https://www.cbp.gov/trade/basic-import-export"
            })
        
        print(f"  âœ… CBP ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ {len(documents)}ê°œ ì œê³µ ì™„ë£Œ")
        
        return {
            "certifications": certifications,
            "documents": documents
        }
