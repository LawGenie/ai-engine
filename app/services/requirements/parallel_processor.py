"""
ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ì„œë¹„ìŠ¤
ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬, ë°°ì¹˜ ì²˜ë¦¬, ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ë“±ì„ í¬í•¨
"""

import asyncio
import time
from typing import Dict, List, Any, Optional, Callable, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import logging
from enum import Enum

class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ"""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    BATCH = "batch"
    STREAM = "stream"

@dataclass
class ProcessingTask:
    """ì²˜ë¦¬ ì‘ì—…"""
    id: str
    func: Callable
    args: tuple = ()
    kwargs: dict = None
    priority: int = 0
    timeout: float = None
    retry_count: int = 0
    max_retries: int = 3
    
    def __post_init__(self):
        if self.kwargs is None:
            self.kwargs = {}

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    task_id: str
    success: bool
    result: Any = None
    error: str = None
    processing_time: float = 0
    retry_count: int = 0

class ParallelProcessor:
    """ë³‘ë ¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(
        self,
        max_workers: int = 10,
        max_concurrent_tasks: int = 20,
        default_timeout: float = 30.0,
        enable_metrics: bool = True
    ):
        self.logger = logging.getLogger(__name__)
        
        # ì²˜ë¦¬ ì„¤ì •
        self.max_workers = max_workers
        self.max_concurrent_tasks = max_concurrent_tasks
        self.default_timeout = default_timeout
        self.enable_metrics = enable_metrics
        
        # ì‹¤í–‰ê¸°ë“¤
        self.thread_executor = ThreadPoolExecutor(max_workers=max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=max_workers)
        
        # ë™ì‹œì„± ì œì–´
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        
        # ë©”íŠ¸ë¦­
        self.metrics = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'total_processing_time': 0.0,
            'average_processing_time': 0.0,
            'concurrent_peak': 0,
            'current_concurrent': 0
        }
        
        # ì‘ì—… í
        self.task_queue = asyncio.Queue()
        self.results = {}
        
        self.logger.info(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   ìµœëŒ€ ì›Œì»¤: {max_workers}")
        self.logger.info(f"   ìµœëŒ€ ë™ì‹œ ì‘ì—…: {max_concurrent_tasks}")
        self.logger.info(f"   ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ: {default_timeout}ì´ˆ")
    
    async def process_parallel(
        self,
        tasks: List[ProcessingTask],
        mode: ProcessingMode = ProcessingMode.PARALLEL,
        timeout: float = None
    ) -> List[ProcessingResult]:
        """ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰"""
        
        timeout = timeout or self.default_timeout
        self.logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ - ëª¨ë“œ: {mode.value}, ì‘ì—… ìˆ˜: {len(tasks)}")
        
        start_time = time.time()
        
        try:
            if mode == ProcessingMode.SEQUENTIAL:
                results = await self._process_sequential(tasks, timeout)
            elif mode == ProcessingMode.PARALLEL:
                results = await self._process_parallel(tasks, timeout)
            elif mode == ProcessingMode.BATCH:
                results = await self._process_batch(tasks, timeout)
            elif mode == ProcessingMode.STREAM:
                results = await self._process_stream(tasks, timeout)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì²˜ë¦¬ ëª¨ë“œ: {mode}")
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_metrics(results, processing_time)
            
            self.logger.info(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    async def _process_sequential(self, tasks: List[ProcessingTask], timeout: float) -> List[ProcessingResult]:
        """ìˆœì°¨ ì²˜ë¦¬"""
        results = []
        
        for task in tasks:
            try:
                result = await self._execute_task(task, timeout)
                results.append(result)
            except Exception as e:
                self.logger.error(f"âŒ ì‘ì—… ì‹¤íŒ¨: {task.id}, ì—ëŸ¬: {e}")
                results.append(ProcessingResult(
                    task_id=task.id,
                    success=False,
                    error=str(e)
                ))
        
        return results
    
    async def _process_parallel(self, tasks: List[ProcessingTask], timeout: float) -> List[ProcessingResult]:
        """ë³‘ë ¬ ì²˜ë¦¬"""
        # ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
        coroutines = []
        for task in tasks:
            coro = self._execute_task_with_semaphore(task, timeout)
            coroutines.append(coro)
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*coroutines, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(ProcessingResult(
                    task_id=tasks[i].id,
                    success=False,
                    error=str(result)
                ))
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_batch(self, tasks: List[ProcessingTask], timeout: float) -> List[ProcessingResult]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        batch_size = min(len(tasks), self.max_concurrent_tasks)
        results = []
        
        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i + batch_size]
            batch_results = await self._process_parallel(batch, timeout)
            results.extend(batch_results)
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (ë¦¬ì†ŒìŠ¤ ë¶€í•˜ ë°©ì§€)
            if i + batch_size < len(tasks):
                await asyncio.sleep(0.1)
        
        return results
    
    async def _process_stream(self, tasks: List[ProcessingTask], timeout: float) -> List[ProcessingResult]:
        """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜)"""
        results = []
        
        # ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹œì‘í•˜ë˜, ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
        pending_tasks = {}
        
        for task in tasks:
            future = asyncio.create_task(
                self._execute_task_with_semaphore(task, timeout)
            )
            pending_tasks[future] = task
        
        # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ì²˜ë¦¬
        while pending_tasks:
            done, pending = await asyncio.wait(
                pending_tasks.keys(),
                return_when=asyncio.FIRST_COMPLETED
            )
            
            for future in done:
                task = pending_tasks[future]
                try:
                    result = await future
                    results.append(result)
                except Exception as e:
                    results.append(ProcessingResult(
                        task_id=task.id,
                        success=False,
                        error=str(e)
                    ))
                
                del pending_tasks[future]
        
        return results
    
    async def _execute_task_with_semaphore(self, task: ProcessingTask, timeout: float) -> ProcessingResult:
        """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•œ ì‘ì—… ì‹¤í–‰"""
        async with self.semaphore:
            return await self._execute_task(task, timeout)
    
    async def _execute_task(self, task: ProcessingTask, timeout: float) -> ProcessingResult:
        """ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        task_id = task.id
        
        self.logger.debug(f"ğŸ”„ ì‘ì—… ì‹œì‘: {task_id}")
        
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            task_timeout = task.timeout or timeout
            
            # í•¨ìˆ˜ ì‹¤í–‰
            if asyncio.iscoroutinefunction(task.func):
                # ë¹„ë™ê¸° í•¨ìˆ˜
                result = await asyncio.wait_for(
                    task.func(*task.args, **task.kwargs),
                    timeout=task_timeout
                )
            else:
                # ë™ê¸° í•¨ìˆ˜ (ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰)
                result = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(
                        self.thread_executor,
                        lambda: task.func(*task.args, **task.kwargs)
                    ),
                    timeout=task_timeout
                )
            
            processing_time = time.time() - start_time
            
            self.logger.debug(f"âœ… ì‘ì—… ì™„ë£Œ: {task_id}, ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            return ProcessingResult(
                task_id=task_id,
                success=True,
                result=result,
                processing_time=processing_time,
                retry_count=task.retry_count
            )
            
        except asyncio.TimeoutError:
            processing_time = time.time() - start_time
            error_msg = f"ì‘ì—… íƒ€ì„ì•„ì›ƒ: {task_id} ({task_timeout}ì´ˆ ì´ˆê³¼)"
            
            self.logger.warning(f"â° {error_msg}")
            
            return ProcessingResult(
                task_id=task_id,
                success=False,
                error=error_msg,
                processing_time=processing_time,
                retry_count=task.retry_count
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {task_id}, ì—ëŸ¬: {str(e)}"
            
            self.logger.error(f"âŒ {error_msg}")
            
            return ProcessingResult(
                task_id=task_id,
                success=False,
                error=error_msg,
                processing_time=processing_time,
                retry_count=task.retry_count
            )
    
    async def process_with_retry(
        self,
        tasks: List[ProcessingTask],
        max_retries: int = 3,
        retry_delay: float = 1.0,
        backoff_factor: float = 2.0
    ) -> List[ProcessingResult]:
        """ì¬ì‹œë„ê°€ í¬í•¨ëœ ì²˜ë¦¬"""
        
        self.logger.info(f"ğŸ”„ ì¬ì‹œë„ ì²˜ë¦¬ ì‹œì‘ - ìµœëŒ€ ì¬ì‹œë„: {max_retries}")
        
        all_results = []
        retry_tasks = []
        
        # ì²« ë²ˆì§¸ ì‹œë„
        results = await self.process_parallel(tasks)
        
        for result in results:
            if result.success:
                all_results.append(result)
            else:
                # ì‹¤íŒ¨í•œ ì‘ì—…ì„ ì¬ì‹œë„ íì— ì¶”ê°€
                task = next((t for t in tasks if t.id == result.task_id), None)
                if task and task.retry_count < max_retries:
                    task.retry_count += 1
                    retry_tasks.append(task)
        
        # ì¬ì‹œë„ ì‹¤í–‰
        current_delay = retry_delay
        for attempt in range(max_retries):
            if not retry_tasks:
                break
            
            self.logger.info(f"ğŸ”„ ì¬ì‹œë„ {attempt + 1}/{max_retries} - {len(retry_tasks)}ê°œ ì‘ì—…")
            
            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            if attempt > 0:
                await asyncio.sleep(current_delay)
                current_delay *= backoff_factor
            
            # ì¬ì‹œë„ ì‹¤í–‰
            retry_results = await self.process_parallel(retry_tasks)
            
            # ì¬ì‹œë„ í ì—…ë°ì´íŠ¸
            next_retry_tasks = []
            for result in retry_results:
                if result.success:
                    all_results.append(result)
                else:
                    task = next((t for t in retry_tasks if t.id == result.task_id), None)
                    if task and task.retry_count < max_retries:
                        task.retry_count += 1
                        next_retry_tasks.append(task)
            
            retry_tasks = next_retry_tasks
        
        # ìµœì¢… ì‹¤íŒ¨í•œ ì‘ì—…ë“¤ ì¶”ê°€
        for task in retry_tasks:
            all_results.append(ProcessingResult(
                task_id=task.id,
                success=False,
                error=f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {max_retries}",
                retry_count=task.retry_count
            ))
        
        self.logger.info(f"âœ… ì¬ì‹œë„ ì²˜ë¦¬ ì™„ë£Œ - ì„±ê³µ: {len([r for r in all_results if r.success])}/{len(all_results)}")
        
        return all_results
    
    def _update_metrics(self, results: List[ProcessingResult], total_time: float):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        if not self.enable_metrics:
            return
        
        self.metrics['total_tasks'] += len(results)
        self.metrics['completed_tasks'] += len([r for r in results if r.success])
        self.metrics['failed_tasks'] += len([r for r in results if not r.success])
        self.metrics['total_processing_time'] += total_time
        
        if self.metrics['total_tasks'] > 0:
            self.metrics['average_processing_time'] = (
                self.metrics['total_processing_time'] / self.metrics['total_tasks']
            )
        
        # ë™ì‹œì„± ë©”íŠ¸ë¦­
        current_concurrent = len([r for r in results if r.success])
        self.metrics['current_concurrent'] = current_concurrent
        if current_concurrent > self.metrics['concurrent_peak']:
            self.metrics['concurrent_peak'] = current_concurrent
    
    def get_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ë°˜í™˜"""
        if not self.enable_metrics:
            return {}
        
        return {
            'total_tasks': self.metrics['total_tasks'],
            'completed_tasks': self.metrics['completed_tasks'],
            'failed_tasks': self.metrics['failed_tasks'],
            'success_rate': (
                self.metrics['completed_tasks'] / self.metrics['total_tasks'] * 100
                if self.metrics['total_tasks'] > 0 else 0
            ),
            'average_processing_time': round(self.metrics['average_processing_time'], 3),
            'total_processing_time': round(self.metrics['total_processing_time'], 3),
            'concurrent_peak': self.metrics['concurrent_peak'],
            'current_concurrent': self.metrics['current_concurrent'],
            'max_workers': self.max_workers,
            'max_concurrent_tasks': self.max_concurrent_tasks,
            'timestamp': datetime.now().isoformat()
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ë³‘ë ¬ ì²˜ë¦¬ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        
        # ì‹¤í–‰ê¸° ì¢…ë£Œ
        self.thread_executor.shutdown(wait=True)
        self.process_executor.shutdown(wait=True)
        
        self.logger.info("âœ… ë³‘ë ¬ ì²˜ë¦¬ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ë³‘ë ¬ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤
parallel_processor = ParallelProcessor()
