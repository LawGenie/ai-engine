"""
LLM ìš”ì•½ ì„œë¹„ìŠ¤
GPTë¥¼ ì‚¬ìš©í•œ ê·œì • ë¬¸ì„œ ìš”ì•½ ë° êµ¬ì¡°í™”
"""

import asyncio
import json
import hashlib
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import aiohttp
from openai import AsyncOpenAI

@dataclass
class SummaryResult:
    """ìš”ì•½ ê²°ê³¼"""
    hs_code: str
    product_name: str
    critical_requirements: List[str]
    required_documents: List[str]
    compliance_steps: List[str]
    estimated_costs: Dict[str, str]
    timeline: str
    risk_factors: List[str]
    recommendations: List[str]
    model_used: str
    tokens_used: int
    cost: float
    confidence_score: float

class LlmSummaryService:
    """LLM ìš”ì•½ ì„œë¹„ìŠ¤"""
    
    def __init__(self, backend_api_url: str = "http://localhost:8081"):
        self.backend_api_url = backend_api_url
        self.openai_client = AsyncOpenAI()
        self.cache_ttl = 86400  # 24ì‹œê°„
        
        # GPT í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.summary_prompt_template = """
HSì½”ë“œ {hs_code}ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ "{product_name}"ì˜ ë¯¸êµ­ ìˆ˜ì… ê·œì •ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¶„ì„:
{documents}

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "critical_requirements": ["í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ 1", "í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ 2"],
    "required_documents": ["í•„ìˆ˜ ì„œë¥˜ 1", "í•„ìˆ˜ ì„œë¥˜ 2"],
    "compliance_steps": ["1ë‹¨ê³„: ...", "2ë‹¨ê³„: ..."],
    "estimated_costs": {{
        "certification": "ì˜ˆìƒ ë¹„ìš©",
        "testing": "ì˜ˆìƒ ë¹„ìš©",
        "legal_review": "ì˜ˆìƒ ë¹„ìš©",
        "total": "ì´ ì˜ˆìƒ ë¹„ìš©"
    }},
    "timeline": "ì˜ˆìƒ ì†Œìš” ì‹œê°„",
    "risk_factors": ["ìœ„í—˜ ìš”ì†Œ 1", "ìœ„í—˜ ìš”ì†Œ 2"],
    "recommendations": ["ê¶Œê³ ì‚¬í•­ 1", "ê¶Œê³ ì‚¬í•­ 2"],
    "confidence_score": 0.85
}}

ì¤‘ìš” ì‚¬í•­:
- critical_requirements: ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•˜ëŠ” ìš”êµ¬ì‚¬í•­ (ìµœëŒ€ 5ê°œ)
- required_documents: ì œì¶œí•´ì•¼ í•˜ëŠ” ì„œë¥˜ (ìµœëŒ€ 8ê°œ)
- compliance_steps: ë‹¨ê³„ë³„ ì¤€ìˆ˜ ì ˆì°¨ (ìµœëŒ€ 6ë‹¨ê³„)
- estimated_costs: êµ¬ì²´ì ì¸ ë¹„ìš© ë²”ìœ„ ì œì‹œ (ì˜ˆ: "$500-1,000")
- timeline: ì‹¤ì œì ì¸ ì†Œìš” ì‹œê°„ (ì˜ˆ: "4-6ì£¼")
- risk_factors: ìˆ˜ì… ì‹¤íŒ¨ ìœ„í—˜ ìš”ì†Œ
- recommendations: ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì‚¬í•­
- confidence_score: 0.0-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""
    
    async def summarize_regulations(
        self, 
        hs_code: str, 
        product_name: str,
        raw_documents: List[Dict[str, Any]]
    ) -> SummaryResult:
        """ê·œì • ë¬¸ì„œ ìš”ì•½"""
        
        print(f"ğŸ¤– LLM ìš”ì•½ ì‹œì‘ - HSì½”ë“œ: {hs_code}, ìƒí’ˆ: {product_name}")
        
        # ë¬¸ì„œ í•´ì‹œ ìƒì„± (ìºì‹œ í‚¤ìš©)
        documents_hash = self._generate_documents_hash(raw_documents)
        
        # ìºì‹œ í™•ì¸
        cached_result = await self._get_from_cache(hs_code, product_name, documents_hash)
        if cached_result:
            print(f"âœ… LLM ìºì‹œì—ì„œ ì¡°íšŒ")
            return cached_result
        
        # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ë° ì •ë¦¬
        document_texts = self._extract_document_texts(raw_documents)
        
        if not document_texts:
            print(f"âš ï¸ ìš”ì•½í•  ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìŒ")
            return self._create_empty_summary(hs_code, product_name)
        
        # GPT ìš”ì•½ ì‹¤í–‰
        summary_data = await self._call_gpt_summary(hs_code, product_name, document_texts)
        
        if not summary_data:
            print(f"âŒ GPT ìš”ì•½ ì‹¤íŒ¨")
            return self._create_empty_summary(hs_code, product_name)
        
        # ê²°ê³¼ ê°ì²´ ìƒì„±
        result = SummaryResult(
            hs_code=hs_code,
            product_name=product_name,
            critical_requirements=summary_data.get("critical_requirements", []),
            required_documents=summary_data.get("required_documents", []),
            compliance_steps=summary_data.get("compliance_steps", []),
            estimated_costs=summary_data.get("estimated_costs", {}),
            timeline=summary_data.get("timeline", "ì •ë³´ ì—†ìŒ"),
            risk_factors=summary_data.get("risk_factors", []),
            recommendations=summary_data.get("recommendations", []),
            model_used="gpt-4o-mini",
            tokens_used=summary_data.get("tokens_used", 0),
            cost=summary_data.get("cost", 0.0),
            confidence_score=summary_data.get("confidence_score", 0.0)
        )
        
        # ìºì‹œì— ì €ì¥
        await self._save_to_cache(result, documents_hash)
        
        print(f"âœ… LLM ìš”ì•½ ì™„ë£Œ - ì‹ ë¢°ë„: {result.confidence_score:.2f}")
        return result
    
    def _extract_document_texts(self, raw_documents: List[Dict[str, Any]]) -> List[str]:
        """ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        
        for doc in raw_documents:
            # ë‹¤ì–‘í•œ í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_fields = ["content", "summary", "description", "text", "body"]
            
            for field in text_fields:
                if field in doc and doc[field]:
                    text = str(doc[field])
                    if len(text) > 50:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                        texts.append(text[:2000])  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
                        break
            
            # ì œëª©ë„ í¬í•¨
            if "title" in doc and doc["title"]:
                texts.append(f"ì œëª©: {doc['title']}")
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_texts = list(set(texts))
        return unique_texts[:10]  # ìµœëŒ€ 10ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬
    
    async def _call_gpt_summary(
        self, 
        hs_code: str, 
        product_name: str, 
        document_texts: List[str]
    ) -> Optional[Dict[str, Any]]:
        """GPT ìš”ì•½ í˜¸ì¶œ"""
        try:
            # ë¬¸ì„œ ë‚´ìš© ê²°í•©
            combined_text = "\n\n".join(document_texts)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.summary_prompt_template.format(
                hs_code=hs_code,
                product_name=product_name,
                documents=combined_text
            )
            
            # í† í° ìˆ˜ ì¶”ì •
            estimated_tokens = len(prompt.split()) * 1.3  # ëŒ€ëµì ì¸ ì¶”ì •
            
            # GPT í˜¸ì¶œ
            start_time = datetime.now()
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"},
                max_tokens=2000
            )
            
            response_time = (datetime.now() - start_time).total_seconds()
            
            # ì‘ë‹µ íŒŒì‹±
            result = json.loads(response.choices[0].message.content)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result["tokens_used"] = response.usage.total_tokens
            result["cost"] = self._calculate_cost(response.usage.total_tokens)
            result["response_time"] = response_time
            
            print(f"âœ… GPT ìš”ì•½ ì™„ë£Œ - í† í°: {result['tokens_used']}, ë¹„ìš©: ${result['cost']:.4f}")
            
            return result
            
        except Exception as e:
            print(f"âŒ GPT ìš”ì•½ ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_cost(self, tokens: int) -> float:
        """í† í° ë¹„ìš© ê³„ì‚° (GPT-4o-mini ê¸°ì¤€)"""
        # GPT-4o-mini ë¹„ìš©: $0.00015/1K input tokens, $0.0006/1K output tokens
        # ëŒ€ëµì ì¸ ê³„ì‚° (ì…ë ¥:ì¶œë ¥ = 3:1 ë¹„ìœ¨ ê°€ì •)
        input_tokens = int(tokens * 0.75)
        output_tokens = int(tokens * 0.25)
        
        input_cost = (input_tokens / 1000) * 0.00015
        output_cost = (output_tokens / 1000) * 0.0006
        
        return input_cost + output_cost
    
    def _generate_documents_hash(self, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œ í•´ì‹œ ìƒì„±"""
        # ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        doc_strings = []
        for doc in documents:
            doc_str = f"{doc.get('title', '')}_{doc.get('content', '')}_{doc.get('summary', '')}"
            doc_strings.append(doc_str)
        
        # í•´ì‹œ ìƒì„±
        combined = "|".join(doc_strings)
        return hashlib.md5(combined.encode()).hexdigest()
    
    async def _get_from_cache(
        self, 
        hs_code: str, 
        product_name: str, 
        documents_hash: str
    ) -> Optional[SummaryResult]:
        """ìºì‹œì—ì„œ ìš”ì•½ ê²°ê³¼ ì¡°íšŒ"""
        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.backend_api_url}/api/llm-summary-cache/search"
                params = {
                    "hs_code": hs_code,
                    "product_name": product_name,
                    "documents_hash": documents_hash
                }
                
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data:
                            return self._parse_cached_result(data)
        except Exception as e:
            print(f"âš ï¸ LLM ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _save_to_cache(self, result: SummaryResult, documents_hash: str):
        """ìš”ì•½ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.backend_api_url}/api/llm-summary-cache"
                data = {
                    "hsCode": result.hs_code,
                    "productName": result.product_name,
                    "rawDocumentsHash": documents_hash,
                    "summaryResult": json.dumps({
                        "critical_requirements": result.critical_requirements,
                        "required_documents": result.required_documents,
                        "compliance_steps": result.compliance_steps,
                        "estimated_costs": result.estimated_costs,
                        "timeline": result.timeline,
                        "risk_factors": result.risk_factors,
                        "recommendations": result.recommendations,
                        "confidence_score": result.confidence_score
                    }),
                    "modelUsed": result.model_used,
                    "tokensUsed": result.tokens_used,
                    "cost": result.cost,
                    "expiresAt": (datetime.now() + timedelta(seconds=self.cache_ttl)).isoformat()
                }
                
                async with session.post(url, json=data) as response:
                    if response.status in [200, 201]:
                        print(f"âœ… LLM ìºì‹œ ì €ì¥ ì™„ë£Œ")
                    else:
                        print(f"âŒ LLM ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {response.status}")
                        
        except Exception as e:
            print(f"âŒ LLM ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _parse_cached_result(self, data: Dict[str, Any]) -> SummaryResult:
        """ìºì‹œëœ ê²°ê³¼ íŒŒì‹±"""
        summary_data = json.loads(data["summaryResult"])
        
        return SummaryResult(
            hs_code=data["hsCode"],
            product_name=data["productName"],
            critical_requirements=summary_data.get("critical_requirements", []),
            required_documents=summary_data.get("required_documents", []),
            compliance_steps=summary_data.get("compliance_steps", []),
            estimated_costs=summary_data.get("estimated_costs", {}),
            timeline=summary_data.get("timeline", "ì •ë³´ ì—†ìŒ"),
            risk_factors=summary_data.get("risk_factors", []),
            recommendations=summary_data.get("recommendations", []),
            model_used=data["modelUsed"],
            tokens_used=data["tokensUsed"],
            cost=float(data["cost"]),
            confidence_score=summary_data.get("confidence_score", 0.0)
        )
    
    def _create_empty_summary(self, hs_code: str, product_name: str) -> SummaryResult:
        """ë¹ˆ ìš”ì•½ ê²°ê³¼ ìƒì„±"""
        return SummaryResult(
            hs_code=hs_code,
            product_name=product_name,
            critical_requirements=["ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨ - ìˆ˜ë™ ê²€í†  í•„ìš”"],
            required_documents=["ê¸°ë³¸ ìˆ˜ì… ì„œë¥˜ í™•ì¸ í•„ìš”"],
            compliance_steps=["1ë‹¨ê³„: ê´€ë ¨ ê¸°ê´€ ë¬¸ì˜", "2ë‹¨ê³„: ìš”êµ¬ì‚¬í•­ í™•ì¸"],
            estimated_costs={"total": "ë¹„ìš© ì‚°ì • ë¶ˆê°€"},
            timeline="ì†Œìš” ì‹œê°„ ì‚°ì • ë¶ˆê°€",
            risk_factors=["ìš”êµ¬ì‚¬í•­ ë¶ˆëª…í™•"],
            recommendations=["ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥"],
            model_used="none",
            tokens_used=0,
            cost=0.0,
            confidence_score=0.0
        )
    
    async def get_summary_statistics(self) -> Dict[str, Any]:
        """ìš”ì•½ í†µê³„ ì¡°íšŒ"""
        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.backend_api_url}/api/llm-summary-cache/statistics"
                
                async with session.get(url) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        return {"error": f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {response.status}"}
                        
        except Exception as e:
            return {"error": f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}"}
    
    def format_summary_result(self, result: SummaryResult) -> Dict[str, Any]:
        """ìš”ì•½ ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            "llm_summary": {
                "hs_code": result.hs_code,
                "product_name": result.product_name,
                "critical_requirements": result.critical_requirements,
                "required_documents": result.required_documents,
                "compliance_steps": result.compliance_steps,
                "estimated_costs": result.estimated_costs,
                "timeline": result.timeline,
                "risk_factors": result.risk_factors,
                "recommendations": result.recommendations,
                "model_used": result.model_used,
                "tokens_used": result.tokens_used,
                "cost": result.cost,
                "confidence_score": result.confidence_score,
                "status": "completed"
            }
        }
