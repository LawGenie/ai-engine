"""
íŒë¡€ ë¶„ì„ ë¼ìš°í„° (í†µí•© ë ˆì´ì–´)

Original Implementation:
- Location: precedents-analysis/
- Files: main.py, cbp_scraper.py, ai_analyzer.py, vector_precedents_search.py, faiss_precedents_db.py
- Author: hwangseojin223
- Purpose: CBP íŒë¡€ ë°ì´í„° ìˆ˜ì§‘, FAISS ë²¡í„° ê²€ìƒ‰, AI ê¸°ë°˜ íŒë¡€ ë¶„ì„

Integration Work:
- Date: 2025-10-12
- Purpose: ë©”ì¸ AI Engineì— íŒë¡€ ë¶„ì„ ê¸°ëŠ¥ í†µí•©
- Changes: FastAPI ë¼ìš°í„°ë¡œ ë³€í™˜, lifespan ì´ˆê¸°í™”, ë‹¨ì¼ ì„œë²„ í†µí•©

Note: 
ì›ë³¸ ì½”ë“œëŠ” precedents-analysis/ ë””ë ‰í† ë¦¬ì— ë³´ì¡´ë˜ì–´ ìˆìœ¼ë©°,
ì´ íŒŒì¼ì€ í•´ë‹¹ ëª¨ë“ˆë“¤ì„ importí•˜ì—¬ FastAPI ì—”ë“œí¬ì¸íŠ¸ë¡œ ì œê³µí•˜ëŠ” í†µí•© ë ˆì´ì–´ì…ë‹ˆë‹¤.
ì›ì‘ì(hwangseojin223)ì˜ ê¸°ì—¬ ì´ë ¥ì€ precedents-analysis/ ë””ë ‰í† ë¦¬ì˜ Git ì´ë ¥ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
import logging
import sys
from pathlib import Path

# precedents-analysis ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).resolve().parents[2]
precedents_path = project_root / "precedents-analysis"
if str(precedents_path) not in sys.path:
    sys.path.insert(0, str(precedents_path))

# precedents-analysis ëª¨ë“ˆ import (íŒ€ì› ì›ë³¸ ì½”ë“œ)
try:
    from cbp_scraper import CBPDataCollector
    from ai_analyzer import PrecedentsAnalyzer
    from vector_precedents_search import VectorPrecedentsSearch
    print("âœ… precedents-analysis ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ precedents-analysis ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    CBPDataCollector = None
    PrecedentsAnalyzer = None
    VectorPrecedentsSearch = None

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/precedents",
    tags=["precedents"],
    responses={404: {"description": "Not found"}},
)

# ìš”ì²­ ëª¨ë¸
class ProductAnalysisRequest(BaseModel):
    product_id: str
    product_name: str
    description: str
    hs_code: str
    origin_country: str = "US"
    price: float = 0.0
    fob_price: float = 0.0

# íŒë¡€ ìƒì„¸ ì •ë³´ ëª¨ë¸
class PrecedentDetail(BaseModel):
    case_id: str
    title: str
    description: str
    status: str
    link: str
    source: str
    hs_code: str

# ì‘ë‹µ ëª¨ë¸
class PrecedentsAnalysisResponse(BaseModel):
    success_cases: List[str]
    failure_cases: List[str]
    review_cases: List[str]
    actionable_insights: List[str]
    risk_factors: List[str]
    recommended_action: str
    confidence_score: float
    is_valid: bool
    precedents_data: List[PrecedentDetail]

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (lifespanì—ì„œ ì´ˆê¸°í™”)
analyzer = None
cbp_collector = None
vector_search = None
analysis_cache = {}

def initialize_precedents_services():
    """íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
    global analyzer, cbp_collector, vector_search
    
    if CBPDataCollector is None or PrecedentsAnalyzer is None or VectorPrecedentsSearch is None:
        logger.warning("âš ï¸ precedents-analysis ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒë¡€ ë¶„ì„ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        return False
    
    try:
        logger.info("ğŸš€ íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        analyzer = PrecedentsAnalyzer()
        cbp_collector = CBPDataCollector()
        vector_search = VectorPrecedentsSearch()
        
        # ìƒí˜¸ ì—°ê²° ì„¤ì •
        vector_search.set_cbp_collector(cbp_collector)
        cbp_collector.set_vector_search(vector_search)
        
        logger.info("âœ… íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

@router.get("/health")
async def precedents_health_check():
    """íŒë¡€ ë¶„ì„ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"""
    is_ready = analyzer is not None and cbp_collector is not None and vector_search is not None
    
    return {
        "status": "healthy" if is_ready else "unavailable",
        "service": "precedents-analysis",
        "version": "1.0.0",
        "cache_size": len(analysis_cache),
        "ready": is_ready
    }

@router.post("/analyze", response_model=PrecedentsAnalysisResponse)
async def analyze_precedents(request: ProductAnalysisRequest):
    """
    ìƒí’ˆ ì •ë³´ë¥¼ ë°›ì•„ì„œ íŒë¡€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    CBP ë°ì´í„° ìˆ˜ì§‘ + ë²¡í„° ê²€ìƒ‰ + AI ë¶„ì„
    """
    # ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—ëŸ¬
    if analyzer is None or cbp_collector is None or vector_search is None:
        raise HTTPException(
            status_code=503, 
            detail="Precedents analysis service is not available. Please check server logs."
        )
    
    try:
        logger.info(f"ğŸš€ íŒë¡€ ë¶„ì„ ì‹œì‘: {request.product_id} - {request.product_name}")
        
        # 1. CBPì—ì„œ ì‹¤ì œ íŒë¡€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹± ì ìš©)
        logger.info(f"ğŸ“‹ CBP ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: HSì½”ë“œ {request.hs_code}")
        cbp_data = await cbp_collector.get_precedents_by_hs_code(request.hs_code)
        logger.info(f"âœ… CBP ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(cbp_data)}ê°œ ì‚¬ë¡€")
        
        # 2. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ íŒë¡€ ì¶”ê°€ ìˆ˜ì§‘
        logger.info("ğŸ” ë²¡í„° ê¸°ë°˜ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ì‹œì‘")
        similar_precedents = await vector_search.find_similar_precedents(
            product_description=request.description,
            product_name=request.product_name,
            top_k=5
        )
        logger.info(f"âœ… ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ: {len(similar_precedents)}ê°œ")
        
        # 3. ê¸°ì¡´ CBP ë°ì´í„° + ìœ ì‚¬ íŒë¡€ ê²°í•©
        combined_data = cbp_data + similar_precedents
        logger.info(f"ğŸ“Š ì´ íŒë¡€ ë°ì´í„°: {len(combined_data)}ê°œ (CBP: {len(cbp_data)}, ìœ ì‚¬: {len(similar_precedents)})")
        
        # 4. AI ë¶„ì„ ìˆ˜í–‰ (ìºì‹± ì ìš©)
        cache_key = f"{request.hs_code}_{request.product_name}_{len(combined_data)}"
        
        if cache_key in analysis_cache:
            logger.info("âœ… ìºì‹œì—ì„œ AI ë¶„ì„ ê²°ê³¼ ë°˜í™˜")
            analysis_result = analysis_cache[cache_key]
        else:
            logger.info("ğŸ¤– AI ë¶„ì„ ì‹œì‘")
            analysis_result = await analyzer.analyze_precedents(request.model_dump(), combined_data)
            analysis_cache[cache_key] = analysis_result
            logger.info("âœ… AI ë¶„ì„ ì™„ë£Œ ë° ìºì‹±")
        
        # 5. ê²°ê³¼ í¬ë§·íŒ…
        success_cases = []
        failure_cases = []
        review_cases = []
        
        for case in analysis_result.get("success_cases", []):
            if isinstance(case, dict):
                success_cases.append(case.get("text", str(case)))
            else:
                success_cases.append(str(case))
        
        for case in analysis_result.get("failure_cases", []):
            if isinstance(case, dict):
                failure_cases.append(case.get("text", str(case)))
            else:
                failure_cases.append(str(case))
        
        for case in analysis_result.get("review_cases", []):
            if isinstance(case, dict):
                review_cases.append(case.get("text", str(case)))
            else:
                review_cases.append(str(case))
        
        # 6. íŒë¡€ ì›ë³¸ ë°ì´í„° í¬ë§·íŒ…
        precedents_data = []
        for precedent in combined_data:
            precedents_data.append(PrecedentDetail(
                case_id=precedent.get("case_id", "N/A"),
                title=precedent.get("title", "N/A"),
                description=precedent.get("description", "N/A"),
                status=precedent.get("status", "UNKNOWN"),
                link=precedent.get("link", ""),
                source=precedent.get("source", "unknown"),
                hs_code=precedent.get("hs_code", request.hs_code)
            ))
        
        # 7. ê²°ê³¼ ë°˜í™˜
        return PrecedentsAnalysisResponse(
            success_cases=success_cases,
            failure_cases=failure_cases,
            review_cases=review_cases,
            actionable_insights=analysis_result.get("actionable_insights", []),
            risk_factors=analysis_result.get("risk_factors", []),
            recommended_action=analysis_result.get("recommended_action", ""),
            confidence_score=analysis_result.get("confidence_score", 0.0),
            is_valid=analysis_result.get("is_valid", False),
            precedents_data=precedents_data
        )
        
    except Exception as e:
        logger.error(f"âŒ íŒë¡€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@router.get("/test-cbp/{hs_code}")
async def test_cbp_data(hs_code: str):
    """CBP ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    if cbp_collector is None:
        raise HTTPException(status_code=503, detail="CBP collector not available")
    
    try:
        logger.info(f"ğŸ§ª CBP í…ŒìŠ¤íŠ¸ ì‹œì‘: HSì½”ë“œ {hs_code}")
        cbp_data = await cbp_collector.get_precedents_by_hs_code(hs_code)
        
        return {
            "hs_code": hs_code,
            "data_count": len(cbp_data),
            "sample_data": cbp_data[:3] if cbp_data else [],
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"âŒ CBP í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"CBP test failed: {str(e)}")

@router.get("/cache-stats")
async def get_cache_stats():
    """ìºì‹œ ë° ë²¡í„° ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    if cbp_collector is None or vector_search is None:
        raise HTTPException(status_code=503, detail="Services not available")
    
    try:
        cache_stats = cbp_collector.get_cache_stats()
        search_stats = vector_search.get_search_stats()
        
        return {
            "cbp_cache_stats": cache_stats,
            "search_stats": search_stats,
            "ai_analysis_cache_size": len(analysis_cache),
            "ai_analysis_cache_keys": list(analysis_cache.keys()),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Stats retrieval failed: {str(e)}")

